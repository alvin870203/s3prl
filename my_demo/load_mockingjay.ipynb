{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 18:21:38 | INFO | s3prl.file_utils | PyTorch version 1.11.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "import s3prl.hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/chihyuan/.cache/torch/hub/s3prl_cache/a2b432be9adba2cb59f5cf89a4cf84d5fff8ec3c9fe248ad53349694565ef8c9\n",
      "for https://www.dropbox.com/s/zwsfa6w2iy2cc68/states-500000.ckpt?dl=0\n",
      "[UpstreamExpert] - Using the default upstream expert config\n",
      "UpstreamExpert(\n",
      "  (transformer): PretrainedTransformer(\n",
      "    (extracter): OnlinePreprocessor(\n",
      "      (_melscale): MelScale()\n",
      "      (_mfcc_trans): MFCC(\n",
      "        (amplitude_to_DB): AmplitudeToDB()\n",
      "        (MelSpectrogram): MelSpectrogram(\n",
      "          (spectrogram): Spectrogram()\n",
      "          (mel_scale): MelScale()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (model): TransformerModel(\n",
      "      (shared_parameters): ModuleDict()\n",
      "      (invertible_adapters): ModuleDict()\n",
      "      (input_representations): TransformerInputRepresentations(\n",
      "        (spec_transform): Linear(in_features=80, out_features=768, bias=True)\n",
      "        (LayerNorm): TransformerLayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (1): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (2): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (3): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (4): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (5): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (6): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (7): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (8): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (9): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (10): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (11): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (prefix_tuning): PrefixTuningPool(\n",
      "        (prefix_tunings): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = getattr(hub, \"mockingjay\")()  # build the Mockingjay model with pre-trained weights\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                                                 Param #\n",
       "===============================================================================================\n",
       "UpstreamExpert                                                         --\n",
       "├─PretrainedTransformer: 1-1                                           --\n",
       "│    └─OnlinePreprocessor: 2-1                                         --\n",
       "│    │    └─MelScale: 3-1                                              --\n",
       "│    │    └─MFCC: 3-2                                                  --\n",
       "│    └─TransformerModel: 2-2                                           --\n",
       "│    │    └─ModuleDict: 3-3                                            --\n",
       "│    │    └─ModuleDict: 3-4                                            --\n",
       "│    │    └─TransformerInputRepresentations: 3-5                       63,744\n",
       "│    │    └─TransformerEncoder: 3-6                                    85,054,464\n",
       "│    │    └─PrefixTuningPool: 3-7                                      --\n",
       "===============================================================================================\n",
       "Total params: 85,118,208\n",
       "Trainable params: 85,118,208\n",
       "Non-trainable params: 0\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3prl.upstream.mockingjay.model.TransformerModel"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.transformer.model)  # the model with adpaters' functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerConfig {\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {\n",
       "      \"acu\": \"pfeiffer\"\n",
       "    },\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"transformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pre_layer_norm\": false,\n",
       "  \"share_layer\": false,\n",
       "  \"transformers_version\": \"4.17.0\"\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 18:13:11 | INFO | s3prl.adapters.configuration | Adding adapter 'acu'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                                                 Param #\n",
       "===============================================================================================\n",
       "UpstreamExpert                                                         --\n",
       "├─PretrainedTransformer: 1-1                                           --\n",
       "│    └─OnlinePreprocessor: 2-1                                         --\n",
       "│    │    └─MelScale: 3-1                                              --\n",
       "│    │    └─MFCC: 3-2                                                  --\n",
       "│    └─TransformerModel: 2-2                                           --\n",
       "│    │    └─ModuleDict: 3-3                                            --\n",
       "│    │    └─ModuleDict: 3-4                                            --\n",
       "│    │    └─TransformerInputRepresentations: 3-5                       (63,744)\n",
       "│    │    └─TransformerEncoder: 3-6                                    (85,054,464)\n",
       "│    │    └─PrefixTuningPool: 3-7                                      --\n",
       "===============================================================================================\n",
       "Total params: 85,118,208\n",
       "Trainable params: 0\n",
       "Non-trainable params: 85,118,208\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.add_adapter(\"acu\", \"pfeiffer\")\n",
    "model.transformer.model.train_adapter(\"acu\")\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'adapters': {}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n",
      "None\n",
      "{'adapters': {}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n",
      "None\n",
      "{'adapters': {}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n"
     ]
    }
   ],
   "source": [
    "from s3prl.adapters.layer import AdapterLayer, AdapterLayerBase\n",
    "from s3prl.adapters.configuration import AdapterConfig\n",
    "for (i, layer) in model.transformer.model.iter_layers():\n",
    "    for module in layer.modules():\n",
    "        if (isinstance(module, AdapterLayerBase)):\n",
    "            print(module.config.adapters.match(\n",
    "                \"acu\",\n",
    "                config_type=AdapterConfig,\n",
    "                layer_idx=i,\n",
    "                location_key=\"self\",\n",
    "            ))\n",
    "            print(module.config.adapters.to_dict())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 18:49:10 | INFO | s3prl.adapters.configuration | Adding adapter 'acu'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adapters': {'acu': 'pfeiffer'}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n",
      "UpstreamExpert(\n",
      "  (transformer): PretrainedTransformer(\n",
      "    (extracter): OnlinePreprocessor(\n",
      "      (_melscale): MelScale()\n",
      "      (_mfcc_trans): MFCC(\n",
      "        (amplitude_to_DB): AmplitudeToDB()\n",
      "        (MelSpectrogram): MelSpectrogram(\n",
      "          (spectrogram): Spectrogram()\n",
      "          (mel_scale): MelScale()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (model): TransformerModel(\n",
      "      (shared_parameters): ModuleDict()\n",
      "      (invertible_adapters): ModuleDict()\n",
      "      (input_representations): TransformerInputRepresentations(\n",
      "        (spec_transform): Linear(in_features=80, out_features=768, bias=True)\n",
      "        (LayerNorm): TransformerLayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (1): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (2): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (3): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (4): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (5): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (6): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (7): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (8): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (9): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (10): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (11): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (prefix_tuning): PrefixTuningPool(\n",
      "        (prefix_tunings): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.transformer.model.add_adapter(\"acu\")#, overwrite_ok=True)\n",
    "print(model.transformer.model.config.adapters.to_dict())\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59c55fd4b3978c0def058b870785763f0b3768b417004cdd166463757ca86d4c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Multimodal_Schizo_adapter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
