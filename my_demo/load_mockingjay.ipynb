{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 00:16:32 | INFO | s3prl.file_utils | PyTorch version 1.11.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "import s3prl.hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/chihyuan/.cache/torch/hub/s3prl_cache/a2b432be9adba2cb59f5cf89a4cf84d5fff8ec3c9fe248ad53349694565ef8c9\n",
      "for https://www.dropbox.com/s/zwsfa6w2iy2cc68/states-500000.ckpt?dl=0\n",
      "[UpstreamExpert] - Using the default upstream expert config\n",
      "UpstreamExpert(\n",
      "  (transformer): PretrainedTransformer(\n",
      "    (extracter): OnlinePreprocessor(\n",
      "      (_melscale): MelScale()\n",
      "      (_mfcc_trans): MFCC(\n",
      "        (amplitude_to_DB): AmplitudeToDB()\n",
      "        (MelSpectrogram): MelSpectrogram(\n",
      "          (spectrogram): Spectrogram()\n",
      "          (mel_scale): MelScale()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (model): TransformerModel(\n",
      "      (shared_parameters): ModuleDict()\n",
      "      (invertible_adapters): ModuleDict()\n",
      "      (input_representations): TransformerInputRepresentations(\n",
      "        (spec_transform): Linear(in_features=80, out_features=768, bias=True)\n",
      "        (LayerNorm): TransformerLayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (1): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (2): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (3): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (4): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (5): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (6): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (7): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (8): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (9): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (10): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (11): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (prefix_tuning): PrefixTuningPool(\n",
      "        (prefix_tunings): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = getattr(hub, \"mockingjay\")()  # build the Mockingjay model with pre-trained weights\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                                                 Param #\n",
       "===============================================================================================\n",
       "UpstreamExpert                                                         --\n",
       "├─PretrainedTransformer: 1-1                                           --\n",
       "│    └─OnlinePreprocessor: 2-1                                         --\n",
       "│    │    └─MelScale: 3-1                                              --\n",
       "│    │    └─MFCC: 3-2                                                  --\n",
       "│    └─TransformerModel: 2-2                                           --\n",
       "│    │    └─ModuleDict: 3-3                                            --\n",
       "│    │    └─ModuleDict: 3-4                                            --\n",
       "│    │    └─TransformerInputRepresentations: 3-5                       63,744\n",
       "│    │    └─TransformerEncoder: 3-6                                    85,054,464\n",
       "│    │    └─PrefixTuningPool: 3-7                                      --\n",
       "===============================================================================================\n",
       "Total params: 85,118,208\n",
       "Trainable params: 85,118,208\n",
       "Non-trainable params: 0\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3prl.upstream.mockingjay.model.TransformerModel"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.transformer.model)  # the model with adpaters' functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerConfig {\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"transformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pre_layer_norm\": false,\n",
       "  \"share_layer\": false,\n",
       "  \"transformers_version\": \"4.17.0\"\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 00:20:06 | INFO | s3prl.adapters.configuration | Adding adapter 'acu'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UpstreamExpert(\n",
      "  (transformer): PretrainedTransformer(\n",
      "    (extracter): OnlinePreprocessor(\n",
      "      (_melscale): MelScale()\n",
      "      (_mfcc_trans): MFCC(\n",
      "        (amplitude_to_DB): AmplitudeToDB()\n",
      "        (MelSpectrogram): MelSpectrogram(\n",
      "          (spectrogram): Spectrogram()\n",
      "          (mel_scale): MelScale()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (model): TransformerModel(\n",
      "      (shared_parameters): ModuleDict()\n",
      "      (invertible_adapters): ModuleDict()\n",
      "      (input_representations): TransformerInputRepresentations(\n",
      "        (spec_transform): Linear(in_features=80, out_features=768, bias=True)\n",
      "        (LayerNorm): TransformerLayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (1): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (2): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (3): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (4): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (5): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (6): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (7): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (8): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (9): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (10): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (11): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (prefix_tuning): PrefixTuningPool(\n",
      "        (prefix_tunings): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.transformer.model.add_adapter(\"acu\", \"pfeiffer\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                                                Param #\n",
       "==============================================================================================================\n",
       "UpstreamExpert                                                                        --\n",
       "├─PretrainedTransformer: 1-1                                                          --\n",
       "│    └─OnlinePreprocessor: 2-1                                                        --\n",
       "│    │    └─MelScale: 3-1                                                             --\n",
       "│    │    └─MFCC: 3-2                                                                 --\n",
       "│    └─TransformerModel: 2-2                                                          --\n",
       "│    │    └─ModuleDict: 3-3                                                           --\n",
       "│    │    └─ModuleDict: 3-4                                                           --\n",
       "│    │    └─TransformerInputRepresentations: 3-5                                      (63,744)\n",
       "│    │    └─TransformerEncoder: 3-6                                                   85,948,992\n",
       "│    │    └─PrefixTuningPool: 3-7                                                     --\n",
       "==============================================================================================================\n",
       "Total params: 86,012,736\n",
       "Trainable params: 894,528\n",
       "Non-trainable params: 85,118,208\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.train_adapter(\"acu\")\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name):depth-idx)                                                               Param #\n",
       "========================================================================================================================\n",
       "UpstreamExpert                                                                                  --\n",
       "├─PretrainedTransformer (transformer): 1-1                                                      --\n",
       "│    └─OnlinePreprocessor (extracter): 2-1                                                      --\n",
       "│    │    └─MelScale (_melscale): 3-1                                                           --\n",
       "│    │    └─MFCC (_mfcc_trans): 3-2                                                             --\n",
       "│    │    │    └─AmplitudeToDB (amplitude_to_DB): 4-1                                           --\n",
       "│    │    │    └─MelSpectrogram (MelSpectrogram): 4-2                                           --\n",
       "│    │    │    │    └─Spectrogram (spectrogram): 5-1                                            --\n",
       "│    │    │    │    └─MelScale (mel_scale): 5-2                                                 --\n",
       "│    └─TransformerModel (model): 2-2                                                            --\n",
       "│    │    └─ModuleDict (shared_parameters): 3-3                                                 --\n",
       "│    │    └─ModuleDict (invertible_adapters): 3-4                                               --\n",
       "│    │    └─TransformerInputRepresentations (input_representations): 3-5                        --\n",
       "│    │    │    └─Linear (spec_transform): 4-3                                                   (62,208)\n",
       "│    │    │    └─TransformerLayerNorm (LayerNorm): 4-4                                          (1,536)\n",
       "│    │    │    └─Dropout (dropout): 4-5                                                         --\n",
       "│    │    └─TransformerEncoder (encoder): 3-6                                                   --\n",
       "│    │    │    └─ModuleList (layer): 4-6                                                        --\n",
       "│    │    │    │    └─TransformerLayer (0): 5-3                                                 --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-1                                --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-1                            --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-1                                        (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-2                                          (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-3                                        (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-4                                     --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-5                      --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-1                          --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-1                --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-2                             --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-6                                        (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-7                                     --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-8                      (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-9                                 --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-10                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-2                          --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-3                                             (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-3                                      --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-4                                             (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-5                                          --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-6                           (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-7                                      --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-11                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-2        --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-2                                   --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-3                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-3                                 36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-4              --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-1                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-4                              37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-8                          --\n",
       "│    │    │    │    └─TransformerLayer (1): 5-4                                                 --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-4                                --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-9                            --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-12                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-13                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-14                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-15                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-16                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-5                          --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-5                --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-10                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-17                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-18                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-19                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-20                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-21                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-5                          --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-11                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-6                                      --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-12                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-13                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-14                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-15                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-22                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-6        --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-6                                   --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-7                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-7                                 36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-8              --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-2                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-8                              37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-16                         --\n",
       "│    │    │    │    └─TransformerLayer (2): 5-5                                                 --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-7                                --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-17                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-23                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-24                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-25                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-26                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-27                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-9                          --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-9                --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-18                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-28                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-29                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-30                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-31                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-32                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-8                          --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-19                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-9                                      --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-20                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-21                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-22                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-23                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-33                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-10       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-10                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-11                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-11                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-12             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-3                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-12                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-24                         --\n",
       "│    │    │    │    └─TransformerLayer (3): 5-6                                                 --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-10                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-25                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-34                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-35                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-36                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-37                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-38                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-13                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-13               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-26                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-39                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-40                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-41                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-42                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-43                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-11                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-27                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-12                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-28                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-29                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-30                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-31                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-44                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-14       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-14                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-15                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-15                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-16             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-4                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-16                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-32                         --\n",
       "│    │    │    │    └─TransformerLayer (4): 5-7                                                 --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-13                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-33                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-45                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-46                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-47                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-48                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-49                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-17                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-17               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-34                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-50                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-51                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-52                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-53                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-54                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-14                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-35                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-15                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-36                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-37                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-38                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-39                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-55                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-18       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-18                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-19                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-19                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-20             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-5                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-20                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-40                         --\n",
       "│    │    │    │    └─TransformerLayer (5): 5-8                                                 --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-16                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-41                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-56                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-57                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-58                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-59                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-60                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-21                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-21               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-42                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-61                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-62                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-63                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-64                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-65                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-17                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-43                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-18                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-44                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-45                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-46                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-47                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-66                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-22       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-22                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-23                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-23                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-24             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-6                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-24                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-48                         --\n",
       "│    │    │    │    └─TransformerLayer (6): 5-9                                                 --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-19                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-49                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-67                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-68                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-69                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-70                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-71                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-25                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-25               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-50                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-72                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-73                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-74                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-75                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-76                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-20                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-51                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-21                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-52                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-53                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-54                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-55                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-77                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-26       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-26                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-27                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-27                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-28             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-7                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-28                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-56                         --\n",
       "│    │    │    │    └─TransformerLayer (7): 5-10                                                --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-22                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-57                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-78                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-79                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-80                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-81                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-82                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-29                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-29               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-58                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-83                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-84                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-85                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-86                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-87                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-23                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-59                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-24                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-60                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-61                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-62                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-63                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-88                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-30       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-30                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-31                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-31                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-32             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-8                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-32                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-64                         --\n",
       "│    │    │    │    └─TransformerLayer (8): 5-11                                                --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-25                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-65                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-89                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-90                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-91                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-92                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-93                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-33                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-33               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-66                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-94                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-95                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-96                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-97                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-98                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-26                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-67                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-27                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-68                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-69                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-70                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-71                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-99                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-34       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-34                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-35                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-35                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-36             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-9                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-36                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-72                         --\n",
       "│    │    │    │    └─TransformerLayer (9): 5-12                                                --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-28                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-73                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-100                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-101                                        (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-102                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-103                                   --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-104                    --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-37                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-37               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-74                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-105                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-106                                   --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-107                    (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-108                               --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-109                   --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-29                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-75                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-30                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-76                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-77                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-78                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-79                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-110                                       --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-38       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-38                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-39                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-39                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-40             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-10                             --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-40                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-80                         --\n",
       "│    │    │    │    └─TransformerLayer (10): 5-13                                               --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-31                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-81                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-111                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-112                                        (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-113                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-114                                   --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-115                    --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-41                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-41               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-82                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-116                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-117                                   --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-118                    (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-119                               --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-120                   --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-32                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-83                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-33                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-84                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-85                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-86                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-87                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-121                                       --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-42       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-42                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-43                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-43                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-44             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-11                             --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-44                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-88                         --\n",
       "│    │    │    │    └─TransformerLayer (11): 5-14                                               --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-34                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-89                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-122                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-123                                        (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-124                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-125                                   --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-126                    --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-45                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-45               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-90                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-127                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-128                                   --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-129                    (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-130                               --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-131                   --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-35                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-91                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-36                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-92                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-93                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-94                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-95                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-132                                       --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-46       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-46                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-47                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-47                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-48             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-12                             --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-48                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-96                         --\n",
       "│    │    └─PrefixTuningPool (prefix_tuning): 3-7                                               --\n",
       "│    │    │    └─ModuleDict (prefix_tunings): 4-7                                               --\n",
       "========================================================================================================================\n",
       "Total params: 86,012,736\n",
       "Trainable params: 894,528\n",
       "Non-trainable params: 85,118,208\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, depth=20, row_settings=[\"depth\", \"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'adapters': {}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n",
      "None\n",
      "{'adapters': {}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n",
      "None\n",
      "{'adapters': {}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n"
     ]
    }
   ],
   "source": [
    "from s3prl.adapters.layer import AdapterLayer, AdapterLayerBase\n",
    "from s3prl.adapters.configuration import AdapterConfig\n",
    "for (i, layer) in model.transformer.model.iter_layers():\n",
    "    for module in layer.modules():\n",
    "        if (isinstance(module, AdapterLayerBase)):\n",
    "            print(module.config.adapters.match(\n",
    "                \"acu\",\n",
    "                config_type=AdapterConfig,\n",
    "                layer_idx=i,\n",
    "                location_key=\"self\",\n",
    "            ))\n",
    "            print(module.config.adapters.to_dict())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerConfig {\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hahaha\": \"hahaha\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"transformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pre_layer_norm\": false,\n",
       "  \"share_layer\": false,\n",
       "  \"transformers_version\": \"4.17.0\"\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.config.hahaha = \"hahaha\"\n",
    "model.transformer.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerConfig {\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hahaha\": \"hahaha\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"transformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pre_layer_norm\": false,\n",
       "  \"share_layer\": false,\n",
       "  \"transformers_version\": \"4.17.0\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.encoder.layer[0].output.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerConfig {\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"transformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pre_layer_norm\": false,\n",
       "  \"share_layer\": false,\n",
       "  \"transformers_version\": \"4.17.0\"\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.base_model.encoder.layer[0].output.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 19:53:17 | INFO | s3prl.adapters.configuration | Adding adapter 'acu'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adapters': {'acu': 'pfeiffer'}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n",
      "UpstreamExpert(\n",
      "  (transformer): PretrainedTransformer(\n",
      "    (extracter): OnlinePreprocessor(\n",
      "      (_melscale): MelScale()\n",
      "      (_mfcc_trans): MFCC(\n",
      "        (amplitude_to_DB): AmplitudeToDB()\n",
      "        (MelSpectrogram): MelSpectrogram(\n",
      "          (spectrogram): Spectrogram()\n",
      "          (mel_scale): MelScale()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (model): TransformerModel(\n",
      "      (shared_parameters): ModuleDict()\n",
      "      (invertible_adapters): ModuleDict()\n",
      "      (input_representations): TransformerInputRepresentations(\n",
      "        (spec_transform): Linear(in_features=80, out_features=768, bias=True)\n",
      "        (LayerNorm): TransformerLayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (1): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (2): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (3): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (4): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (5): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (6): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (7): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (8): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (9): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (10): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (11): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (prefix_tuning): PrefixTuningPool(\n",
      "        (prefix_tunings): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.transformer.model.add_adapter(\"acu\")\n",
    "print(model.transformer.model.config.adapters.to_dict())\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59c55fd4b3978c0def058b870785763f0b3768b417004cdd166463757ca86d4c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Multimodal_Schizo_adapter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
