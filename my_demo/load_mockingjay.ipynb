{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 01:26:02 | INFO | s3prl.file_utils | PyTorch version 1.11.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "import s3prl.hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/chihyuan/.cache/torch/hub/s3prl_cache/a2b432be9adba2cb59f5cf89a4cf84d5fff8ec3c9fe248ad53349694565ef8c9\n",
      "for https://www.dropbox.com/s/zwsfa6w2iy2cc68/states-500000.ckpt?dl=0\n",
      "[UpstreamExpert] - Using the default upstream expert config\n",
      "UpstreamExpert(\n",
      "  (transformer): PretrainedTransformer(\n",
      "    (extracter): OnlinePreprocessor(\n",
      "      (_melscale): MelScale()\n",
      "      (_mfcc_trans): MFCC(\n",
      "        (amplitude_to_DB): AmplitudeToDB()\n",
      "        (MelSpectrogram): MelSpectrogram(\n",
      "          (spectrogram): Spectrogram()\n",
      "          (mel_scale): MelScale()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (model): TransformerModel(\n",
      "      (shared_parameters): ModuleDict()\n",
      "      (invertible_adapters): ModuleDict()\n",
      "      (input_representations): TransformerInputRepresentations(\n",
      "        (spec_transform): Linear(in_features=80, out_features=768, bias=True)\n",
      "        (LayerNorm): TransformerLayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (1): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (2): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (3): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (4): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (5): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (6): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (7): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (8): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (9): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (10): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (11): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (prefix_tuning): PrefixTuningPool(\n",
      "        (prefix_tunings): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = getattr(hub, \"mockingjay\")()  # build the Mockingjay model with pre-trained weights\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                                                 Param #\n",
       "===============================================================================================\n",
       "UpstreamExpert                                                         --\n",
       "├─PretrainedTransformer: 1-1                                           --\n",
       "│    └─OnlinePreprocessor: 2-1                                         --\n",
       "│    │    └─MelScale: 3-1                                              --\n",
       "│    │    └─MFCC: 3-2                                                  --\n",
       "│    └─TransformerModel: 2-2                                           --\n",
       "│    │    └─ModuleDict: 3-3                                            --\n",
       "│    │    └─ModuleDict: 3-4                                            --\n",
       "│    │    └─TransformerInputRepresentations: 3-5                       63,744\n",
       "│    │    └─TransformerEncoder: 3-6                                    85,054,464\n",
       "│    │    └─PrefixTuningPool: 3-7                                      --\n",
       "===============================================================================================\n",
       "Total params: 85,118,208\n",
       "Trainable params: 85,118,208\n",
       "Non-trainable params: 0\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3prl.upstream.mockingjay.model.TransformerModel"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.transformer.model)  # the model with adpaters' functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerConfig {\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"transformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pre_layer_norm\": false,\n",
       "  \"share_layer\": false,\n",
       "  \"transformers_version\": \"4.17.0\"\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 01:26:50 | INFO | s3prl.adapters.configuration | Adding adapter 'acu'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UpstreamExpert(\n",
      "  (transformer): PretrainedTransformer(\n",
      "    (extracter): OnlinePreprocessor(\n",
      "      (_melscale): MelScale()\n",
      "      (_mfcc_trans): MFCC(\n",
      "        (amplitude_to_DB): AmplitudeToDB()\n",
      "        (MelSpectrogram): MelSpectrogram(\n",
      "          (spectrogram): Spectrogram()\n",
      "          (mel_scale): MelScale()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (model): TransformerModel(\n",
      "      (shared_parameters): ModuleDict()\n",
      "      (invertible_adapters): ModuleDict()\n",
      "      (input_representations): TransformerInputRepresentations(\n",
      "        (spec_transform): Linear(in_features=80, out_features=768, bias=True)\n",
      "        (LayerNorm): TransformerLayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (1): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (2): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (3): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (4): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (5): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (6): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (7): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (8): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (9): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (10): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (11): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (prefix_tuning): PrefixTuningPool(\n",
      "        (prefix_tunings): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.transformer.model.add_adapter(\"acu\", \"pfeiffer\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                                                Param #\n",
       "==============================================================================================================\n",
       "UpstreamExpert                                                                        --\n",
       "├─PretrainedTransformer: 1-1                                                          --\n",
       "│    └─OnlinePreprocessor: 2-1                                                        --\n",
       "│    │    └─MelScale: 3-1                                                             --\n",
       "│    │    └─MFCC: 3-2                                                                 --\n",
       "│    └─TransformerModel: 2-2                                                          --\n",
       "│    │    └─ModuleDict: 3-3                                                           --\n",
       "│    │    └─ModuleDict: 3-4                                                           --\n",
       "│    │    └─TransformerInputRepresentations: 3-5                                      (63,744)\n",
       "│    │    └─TransformerEncoder: 3-6                                                   85,948,992\n",
       "│    │    └─PrefixTuningPool: 3-7                                                     --\n",
       "==============================================================================================================\n",
       "Total params: 86,012,736\n",
       "Trainable params: 894,528\n",
       "Non-trainable params: 85,118,208\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.train_adapter(\"acu\")\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name):depth-idx)                                                               Param #\n",
       "========================================================================================================================\n",
       "UpstreamExpert                                                                                  --\n",
       "├─PretrainedTransformer (transformer): 1-1                                                      --\n",
       "│    └─OnlinePreprocessor (extracter): 2-1                                                      --\n",
       "│    │    └─MelScale (_melscale): 3-1                                                           --\n",
       "│    │    └─MFCC (_mfcc_trans): 3-2                                                             --\n",
       "│    │    │    └─AmplitudeToDB (amplitude_to_DB): 4-1                                           --\n",
       "│    │    │    └─MelSpectrogram (MelSpectrogram): 4-2                                           --\n",
       "│    │    │    │    └─Spectrogram (spectrogram): 5-1                                            --\n",
       "│    │    │    │    └─MelScale (mel_scale): 5-2                                                 --\n",
       "│    └─TransformerModel (model): 2-2                                                            --\n",
       "│    │    └─ModuleDict (shared_parameters): 3-3                                                 --\n",
       "│    │    └─ModuleDict (invertible_adapters): 3-4                                               --\n",
       "│    │    └─TransformerInputRepresentations (input_representations): 3-5                        --\n",
       "│    │    │    └─Linear (spec_transform): 4-3                                                   (62,208)\n",
       "│    │    │    └─TransformerLayerNorm (LayerNorm): 4-4                                          (1,536)\n",
       "│    │    │    └─Dropout (dropout): 4-5                                                         --\n",
       "│    │    └─TransformerEncoder (encoder): 3-6                                                   --\n",
       "│    │    │    └─ModuleList (layer): 4-6                                                        --\n",
       "│    │    │    │    └─TransformerLayer (0): 5-3                                                 --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-1                                --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-1                            --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-1                                        (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-2                                          (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-3                                        (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-4                                     --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-5                      --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-1                          --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-1                --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-2                             --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-6                                        (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-7                                     --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-8                      (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-9                                 --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-10                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-2                          --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-3                                             (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-3                                      --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-4                                             (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-5                                          --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-6                           (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-7                                      --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-11                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-2        --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-2                                   --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-3                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-3                                 36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-4              --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-1                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-4                              37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-8                          --\n",
       "│    │    │    │    └─TransformerLayer (1): 5-4                                                 --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-4                                --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-9                            --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-12                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-13                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-14                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-15                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-16                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-5                          --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-5                --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-10                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-17                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-18                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-19                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-20                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-21                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-5                          --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-11                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-6                                      --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-12                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-13                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-14                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-15                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-22                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-6        --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-6                                   --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-7                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-7                                 36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-8              --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-2                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-8                              37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-16                         --\n",
       "│    │    │    │    └─TransformerLayer (2): 5-5                                                 --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-7                                --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-17                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-23                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-24                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-25                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-26                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-27                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-9                          --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-9                --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-18                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-28                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-29                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-30                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-31                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-32                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-8                          --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-19                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-9                                      --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-20                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-21                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-22                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-23                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-33                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-10       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-10                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-11                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-11                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-12             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-3                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-12                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-24                         --\n",
       "│    │    │    │    └─TransformerLayer (3): 5-6                                                 --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-10                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-25                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-34                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-35                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-36                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-37                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-38                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-13                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-13               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-26                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-39                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-40                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-41                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-42                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-43                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-11                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-27                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-12                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-28                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-29                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-30                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-31                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-44                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-14       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-14                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-15                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-15                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-16             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-4                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-16                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-32                         --\n",
       "│    │    │    │    └─TransformerLayer (4): 5-7                                                 --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-13                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-33                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-45                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-46                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-47                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-48                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-49                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-17                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-17               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-34                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-50                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-51                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-52                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-53                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-54                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-14                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-35                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-15                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-36                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-37                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-38                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-39                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-55                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-18       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-18                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-19                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-19                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-20             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-5                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-20                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-40                         --\n",
       "│    │    │    │    └─TransformerLayer (5): 5-8                                                 --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-16                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-41                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-56                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-57                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-58                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-59                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-60                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-21                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-21               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-42                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-61                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-62                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-63                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-64                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-65                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-17                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-43                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-18                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-44                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-45                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-46                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-47                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-66                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-22       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-22                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-23                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-23                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-24             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-6                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-24                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-48                         --\n",
       "│    │    │    │    └─TransformerLayer (6): 5-9                                                 --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-19                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-49                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-67                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-68                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-69                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-70                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-71                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-25                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-25               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-50                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-72                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-73                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-74                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-75                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-76                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-20                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-51                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-21                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-52                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-53                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-54                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-55                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-77                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-26       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-26                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-27                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-27                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-28             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-7                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-28                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-56                         --\n",
       "│    │    │    │    └─TransformerLayer (7): 5-10                                                --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-22                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-57                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-78                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-79                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-80                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-81                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-82                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-29                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-29               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-58                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-83                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-84                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-85                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-86                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-87                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-23                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-59                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-24                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-60                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-61                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-62                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-63                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-88                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-30       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-30                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-31                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-31                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-32             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-8                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-32                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-64                         --\n",
       "│    │    │    │    └─TransformerLayer (8): 5-11                                                --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-25                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-65                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-89                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-90                                         (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-91                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-92                                    --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-93                     --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-33                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-33               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-66                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-94                                       (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-95                                    --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-96                     (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-97                                --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-98                    --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-26                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-67                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-27                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-68                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-69                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-70                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-71                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-99                                        --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-34       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-34                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-35                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-35                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-36             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-9                              --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-36                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-72                         --\n",
       "│    │    │    │    └─TransformerLayer (9): 5-12                                                --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-28                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-73                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-100                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-101                                        (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-102                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-103                                   --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-104                    --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-37                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-37               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-74                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-105                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-106                                   --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-107                    (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-108                               --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-109                   --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-29                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-75                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-30                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-76                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-77                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-78                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-79                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-110                                       --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-38       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-38                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-39                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-39                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-40             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-10                             --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-40                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-80                         --\n",
       "│    │    │    │    └─TransformerLayer (10): 5-13                                               --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-31                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-81                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-111                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-112                                        (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-113                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-114                                   --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-115                    --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-41                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-41               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-82                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-116                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-117                                   --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-118                    (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-119                               --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-120                   --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-32                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-83                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-33                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-84                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-85                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-86                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-87                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-121                                       --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-42       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-42                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-43                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-43                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-44             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-11                             --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-44                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-88                         --\n",
       "│    │    │    │    └─TransformerLayer (11): 5-14                                               --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-34                               --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-89                           --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-122                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-123                                        (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-124                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-125                                   --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-126                    --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-45                         --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-45               --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-90                            --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-127                                      (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-128                                   --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-129                    (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-130                               --\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-131                   --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-35                         --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-91                                            (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-36                                     --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-92                                            (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-93                                         --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-94                          (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-95                                     --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-132                                       --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-46       --\n",
       "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-46                                  --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-47                       --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-47                                36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-48             --\n",
       "│    │    │    │    │    │    │    │    │    │    └─ReLU (f): 11-12                             --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-48                             37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-96                         --\n",
       "│    │    └─PrefixTuningPool (prefix_tuning): 3-7                                               --\n",
       "│    │    │    └─ModuleDict (prefix_tunings): 4-7                                               --\n",
       "========================================================================================================================\n",
       "Total params: 86,012,736\n",
       "Trainable params: 894,528\n",
       "Non-trainable params: 85,118,208\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, depth=20, row_settings=[\"depth\", \"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Psuedo Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stack[acu]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.active_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'last_hidden_state': tensor([[[ 0.0730, -0.3754,  0.7391,  ...,  0.4291,  0.5099, -0.0267],\n",
      "         [ 0.2851, -0.4773, -0.0198,  ...,  0.5198,  0.3499, -0.4636],\n",
      "         [ 0.0768, -0.1505,  0.6044,  ...,  0.1544,  0.2474, -0.2487],\n",
      "         ...,\n",
      "         [-0.0637, -0.2434,  0.4420,  ...,  0.3178,  0.2866,  0.0657],\n",
      "         [ 0.3156, -0.3128,  0.7295,  ...,  0.6103,  0.3452, -0.2428],\n",
      "         [ 0.0121, -0.1662,  0.4801,  ...,  0.3415,  0.3224, -0.5108]],\n",
      "\n",
      "        [[ 0.1949, -0.4917,  0.4972,  ...,  0.2012, -0.0181, -0.4319],\n",
      "         [ 0.1193, -0.1686,  0.6645,  ...,  0.3248,  0.3163, -0.4534],\n",
      "         [ 0.0673, -0.3221,  0.7631,  ...,  0.2616,  0.3109, -0.1358],\n",
      "         ...,\n",
      "         [ 0.1371, -0.4481,  0.6512,  ...,  0.4214,  0.3136, -0.2292],\n",
      "         [ 0.3459, -0.3516,  0.4059,  ...,  0.1353,  0.2391, -0.4858],\n",
      "         [ 0.1675, -0.3057,  0.8408,  ...,  0.1693,  0.3888, -0.2933]],\n",
      "\n",
      "        [[ 0.1736, -0.4204,  0.4361,  ...,  0.2670,  0.2595, -0.4360],\n",
      "         [ 0.1782, -0.4079,  0.4511,  ...,  0.1565,  0.1590, -0.3787],\n",
      "         [ 0.1329, -0.4358,  0.6178,  ...,  0.2370,  0.3518, -0.3120],\n",
      "         ...,\n",
      "         [ 0.1548, -0.3917,  0.6133,  ...,  0.2043,  0.3498, -0.4552],\n",
      "         [ 0.2055, -0.4647,  0.5612,  ...,  0.3849,  0.3018, -0.3433],\n",
      "         [ 0.0843, -0.1828,  0.5461,  ...,  0.0047,  0.5104, -0.1818]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2160, -0.4088,  0.1457,  ...,  0.2594,  0.2377, -0.3692],\n",
      "         [ 0.1159, -0.3680,  0.3314,  ...,  0.1951,  0.3245, -0.3499],\n",
      "         [ 0.0344, -0.4078,  0.5862,  ...,  0.2055,  0.2750, -0.4847],\n",
      "         ...,\n",
      "         [ 0.1365, -0.3709,  0.2199,  ...,  0.3176,  0.3782, -0.1776],\n",
      "         [ 0.1038, -0.4417,  0.7010,  ...,  0.2546,  0.3015,  0.1831],\n",
      "         [ 0.2247, -0.3604,  0.6230,  ...,  0.2175,  0.3459, -0.3338]],\n",
      "\n",
      "        [[ 0.1296, -0.3556,  0.6575,  ...,  0.5009,  0.1574, -0.2006],\n",
      "         [ 0.0468, -0.3575,  0.1465,  ...,  0.2193,  0.3117, -0.3751],\n",
      "         [ 0.1396, -0.4701,  0.5472,  ...,  0.3645,  0.4282, -0.2658],\n",
      "         ...,\n",
      "         [ 0.1643, -0.3805,  0.6022,  ...,  0.2796,  0.3838, -0.4392],\n",
      "         [ 0.1584, -0.4609,  0.6939,  ...,  0.1608,  0.4409, -0.4028],\n",
      "         [ 0.0457, -0.4468,  0.7762,  ...,  0.1375,  0.4140, -0.2773]],\n",
      "\n",
      "        [[ 0.0981, -0.4405,  0.5983,  ...,  0.3046,  0.2601, -0.4962],\n",
      "         [ 0.1515, -0.2592,  0.6903,  ...,  0.2949,  0.5358, -0.3125],\n",
      "         [ 0.1805, -0.4900,  0.5675,  ...,  0.3115,  0.4129, -0.3871],\n",
      "         ...,\n",
      "         [ 0.1298, -0.4634,  0.5571,  ...,  0.1704,  0.2852, -0.3617],\n",
      "         [ 0.2568, -0.4395,  0.3408,  ...,  0.3096,  0.2919,  0.2499],\n",
      "         [ 0.2304, -0.4107,  0.7310,  ...,  0.3533,  0.3237, -0.3294]]]), 'hidden_states': (tensor([[[ 0.1565, -0.1321,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1321,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         ...,\n",
      "         [ 0.0000, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1321,  0.1802,  ...,  2.5051, -0.5862, -0.0782]],\n",
      "\n",
      "        [[ 0.1565, -0.1321,  0.1803,  ...,  0.0000, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1321,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         ...,\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1321,  0.1802,  ...,  2.5051, -0.5862, -0.0782]],\n",
      "\n",
      "        [[ 0.1565, -0.1321,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.0000,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         ...,\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  0.0000, -0.0000, -0.0782],\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.0000, -0.1321,  0.1802,  ...,  2.5051, -0.5862, -0.0782]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1565, -0.1321,  0.0000,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.0000,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.0000, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         ...,\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0000],\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1321,  0.1802,  ...,  0.0000, -0.5862, -0.0782]],\n",
      "\n",
      "        [[ 0.1565, -0.0000,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1321,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1322,  0.1803,  ...,  2.5051, -0.0000, -0.0783],\n",
      "         ...,\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1321,  0.1802,  ...,  2.5051, -0.5862, -0.0782]],\n",
      "\n",
      "        [[ 0.1565, -0.1321,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1321,  0.0000,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         ...,\n",
      "         [ 0.1565, -0.0000,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1322,  0.0000,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1321,  0.1802,  ...,  0.0000, -0.5862, -0.0782]]]), tensor([[[ 1.2029e-01, -1.8673e-01,  2.6704e-02,  ...,  1.5739e+00,\n",
      "           1.3692e+00, -9.9077e-01],\n",
      "         [-2.5921e-01,  2.4400e-01, -6.2205e-02,  ...,  1.5758e+00,\n",
      "           1.2931e+00, -6.7482e-01],\n",
      "         [-3.0052e-01, -9.2136e-02, -1.1805e-02,  ...,  1.3493e+00,\n",
      "           1.7105e+00, -8.5392e-01],\n",
      "         ...,\n",
      "         [ 1.2651e-01,  1.9031e-02,  1.1396e-01,  ...,  1.3397e+00,\n",
      "           1.6888e+00, -8.2023e-01],\n",
      "         [-2.9086e-01, -2.6596e-02,  2.3345e-02,  ...,  1.1296e+00,\n",
      "           1.5016e+00, -6.5985e-01],\n",
      "         [-3.3893e-01, -3.7280e-02, -1.9570e-02,  ...,  1.4183e+00,\n",
      "           1.5529e+00, -7.6604e-01]],\n",
      "\n",
      "        [[-2.4837e-01, -8.5181e-02,  2.3467e-02,  ...,  3.0729e-01,\n",
      "           1.5281e+00, -6.9497e-01],\n",
      "         [-2.4337e-01, -2.6048e-02,  1.1332e-01,  ...,  1.1572e+00,\n",
      "           3.0939e-01, -7.4752e-01],\n",
      "         [-3.0806e-01,  2.0886e-02,  1.5322e-02,  ...,  1.4196e+00,\n",
      "           1.7879e+00, -8.3815e-01],\n",
      "         ...,\n",
      "         [-3.6025e-01, -1.9585e-01,  1.1496e-01,  ...,  1.3126e+00,\n",
      "           1.5201e+00, -6.5335e-01],\n",
      "         [-2.2041e-01, -1.9075e-02,  5.8490e-02,  ...,  8.7989e-01,\n",
      "           1.7228e+00, -6.3381e-01],\n",
      "         [-2.5737e-01,  6.9359e-02,  9.9629e-03,  ...,  1.5703e+00,\n",
      "           3.1254e-01, -5.4278e-01]],\n",
      "\n",
      "        [[-2.9427e-01,  6.0309e-02,  1.0643e-02,  ...,  1.4199e+00,\n",
      "           1.1307e+00, -7.5789e-02],\n",
      "         [-3.0164e-01,  5.1384e-02, -4.4561e-02,  ...,  1.4131e+00,\n",
      "           1.7372e+00, -6.7344e-02],\n",
      "         [-2.5592e-01, -1.0377e-01,  5.0945e-02,  ...,  1.2768e+00,\n",
      "           3.1879e-01, -7.0936e-01],\n",
      "         ...,\n",
      "         [-2.5489e-01,  3.2164e-02,  5.2293e-02,  ...,  6.2125e-01,\n",
      "           1.7141e+00, -8.5373e-01],\n",
      "         [-2.4019e-01, -7.3044e-02,  2.1457e-02,  ...,  1.0658e+00,\n",
      "           1.8457e+00, -7.3480e-01],\n",
      "         [-2.7968e-01,  2.5942e-02,  1.4254e-02,  ...,  1.4302e+00,\n",
      "           1.6757e+00, -7.5565e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.8246e-01,  4.6783e-02,  1.2554e-02,  ...,  1.0289e+00,\n",
      "           1.7010e+00, -6.1342e-01],\n",
      "         [-2.2728e-01,  5.7073e-03,  1.1297e-01,  ...,  1.3118e+00,\n",
      "           1.7851e+00, -5.9277e-01],\n",
      "         [-1.3669e-01,  6.4609e-02,  1.1394e-01,  ...,  1.4595e+00,\n",
      "           1.7840e+00, -5.6037e-01],\n",
      "         ...,\n",
      "         [-2.6747e-01,  5.3026e-02,  1.1208e-01,  ...,  1.3235e+00,\n",
      "           1.7558e+00, -5.9563e-02],\n",
      "         [-2.7414e-01, -1.0528e-03,  2.8006e-02,  ...,  1.4507e+00,\n",
      "           1.7889e+00, -4.0602e-02],\n",
      "         [-2.3525e-01,  2.5335e-02, -3.7483e-02,  ...,  8.6366e-01,\n",
      "           1.0821e+00, -5.1775e-01]],\n",
      "\n",
      "        [[-2.5465e-01, -3.3380e-02,  7.1122e-03,  ...,  1.7513e+00,\n",
      "           1.7023e+00, -9.0217e-01],\n",
      "         [-2.3010e-01,  8.6558e-02, -1.4121e-02,  ...,  1.5723e+00,\n",
      "           1.2696e+00, -5.2131e-01],\n",
      "         [-2.8689e-01,  1.4568e-01, -1.7564e-02,  ...,  1.2277e+00,\n",
      "           1.7215e+00, -7.1904e-01],\n",
      "         ...,\n",
      "         [-2.8969e-01,  5.8399e-03, -1.1568e-02,  ...,  1.1559e+00,\n",
      "           1.5476e+00, -8.0964e-02],\n",
      "         [-2.4464e-01,  4.2463e-02, -2.7064e-03,  ...,  1.3270e+00,\n",
      "           1.6667e+00, -7.8299e-01],\n",
      "         [ 1.7085e-01,  6.9576e-02, -3.0755e-02,  ...,  1.2934e+00,\n",
      "           1.8018e+00, -6.5542e-01]],\n",
      "\n",
      "        [[-2.7096e-01,  1.3851e-01,  3.6787e-03,  ...,  1.1840e+00,\n",
      "           1.6311e+00, -7.5172e-01],\n",
      "         [-2.7873e-01, -1.6109e-02,  2.7045e-02,  ...,  1.3472e+00,\n",
      "           1.7463e+00, -1.1171e+00],\n",
      "         [-2.9089e-01,  1.5230e-01, -1.1519e-02,  ...,  1.3849e+00,\n",
      "           1.6060e+00, -3.6200e-01],\n",
      "         ...,\n",
      "         [-3.1844e-01,  1.5568e-01,  7.9050e-03,  ...,  9.7953e-01,\n",
      "           2.9288e-01, -7.4947e-01],\n",
      "         [-3.4092e-01, -6.8153e-02, -4.7929e-02,  ...,  1.3027e+00,\n",
      "           3.3487e-01, -6.1262e-01],\n",
      "         [-3.0243e-01, -5.3823e-02,  8.7653e-03,  ...,  6.4805e-01,\n",
      "           1.5871e+00, -8.4017e-01]]]), tensor([[[-0.0976,  0.3760, -0.0108,  ...,  0.7741,  0.0878,  0.1203],\n",
      "         [-0.1504,  0.3210,  0.1156,  ...,  0.6875,  0.2900, -0.1534],\n",
      "         [-0.1152,  0.2960,  0.1179,  ...,  0.8114,  0.1995,  0.1638],\n",
      "         ...,\n",
      "         [-0.1335,  0.2996, -0.0439,  ...,  0.5912,  0.2391,  0.3877],\n",
      "         [-0.1265,  0.3933, -0.0957,  ...,  0.5057,  0.0621,  0.1577],\n",
      "         [-0.1317, -0.0769, -0.0545,  ...,  0.6935,  0.1118,  0.1708]],\n",
      "\n",
      "        [[-0.0350,  0.2513,  0.1340,  ...,  0.3210,  0.1446,  0.0590],\n",
      "         [-0.0739,  0.4635, -0.0829,  ...,  0.4887, -0.2432,  0.0637],\n",
      "         [-0.0741,  0.4117, -0.0638,  ...,  0.5627,  0.1492,  0.2217],\n",
      "         ...,\n",
      "         [-0.1325,  0.2478,  0.0221,  ...,  0.9143,  0.2120,  0.4567],\n",
      "         [-0.0904,  0.4704,  0.1310,  ...,  0.1688,  0.2527,  0.0709],\n",
      "         [-0.1421,  0.4244, -0.1298,  ...,  0.6439, -0.0231,  0.1395]],\n",
      "\n",
      "        [[-0.1184,  0.3609, -0.0551,  ...,  0.6763,  0.0414,  0.3670],\n",
      "         [-0.1163,  0.3829, -0.1094,  ...,  0.5309,  0.1278,  0.2272],\n",
      "         [-0.0135,  0.3944, -0.1466,  ...,  0.5411, -0.1631, -0.2135],\n",
      "         ...,\n",
      "         [-0.0970, -0.0276, -0.1427,  ...,  0.1833,  0.2788,  0.1029],\n",
      "         [-0.0713,  0.3113, -0.0753,  ...,  0.6893,  0.3715,  0.0646],\n",
      "         [-0.0922,  0.3827, -0.0698,  ...,  0.6587,  0.0565,  0.0997]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1107,  0.4087, -0.1651,  ...,  0.3437,  0.1294, -0.0181],\n",
      "         [ 0.0204,  0.4350, -0.1764,  ...,  0.4259,  0.0946,  0.0426],\n",
      "         [-0.1064,  0.4551, -0.1701,  ...,  0.4273,  0.0140,  0.3132],\n",
      "         ...,\n",
      "         [-0.1329,  0.4417, -0.0849,  ...,  0.7283,  0.1735,  0.4718],\n",
      "         [-0.0809,  0.3565,  0.1349,  ...,  0.6275,  0.1591,  0.3537],\n",
      "         [-0.1164, -0.0812, -0.0125,  ...,  0.6035,  0.2542,  0.2714]],\n",
      "\n",
      "        [[-0.1071,  0.3926, -0.0953,  ...,  0.9139,  0.1077,  0.1125],\n",
      "         [-0.1155,  0.5045, -0.1095,  ...,  0.6541,  0.1069,  0.2806],\n",
      "         [-0.1395,  0.2938,  0.0080,  ...,  0.6691,  0.1439,  0.4100],\n",
      "         ...,\n",
      "         [-0.1390,  0.3429, -0.0589,  ...,  0.4640,  0.2075,  0.4970],\n",
      "         [-0.0911,  0.4055, -0.0594,  ...,  0.5841,  0.4302,  0.1471],\n",
      "         [-0.1379,  0.4687, -0.1102,  ...,  0.4275,  0.1332,  0.1842]],\n",
      "\n",
      "        [[-0.1346, -0.0553,  0.0135,  ...,  0.5229,  0.0221,  0.3847],\n",
      "         [-0.1047,  0.4491, -0.1291,  ...,  0.4866,  0.0974,  0.0062],\n",
      "         [-0.1346,  0.4800, -0.0674,  ...,  0.7945,  0.0611,  0.2737],\n",
      "         ...,\n",
      "         [-0.1249,  0.4260, -0.1138,  ...,  0.2451,  0.1726,  0.0252],\n",
      "         [-0.1447,  0.4196,  0.1287,  ...,  0.5332, -0.1131,  0.2971],\n",
      "         [-0.1440,  0.3408, -0.0606,  ...,  0.3181,  0.1190, -0.2212]]]), tensor([[[-1.7564e-01,  3.1775e-02,  7.2695e-02,  ...,  4.1664e-01,\n",
      "           9.8597e-01,  7.0630e-01],\n",
      "         [-1.6567e-01,  9.6606e-02,  1.1471e-01,  ...,  7.5878e-01,\n",
      "           1.0837e+00,  3.9808e-01],\n",
      "         [-1.0564e-01,  1.2506e-02, -6.0163e-02,  ...,  5.7873e-01,\n",
      "           9.1900e-01,  5.3680e-01],\n",
      "         ...,\n",
      "         [-1.0268e-01,  2.3773e-02, -1.0841e-03,  ...,  3.6654e-01,\n",
      "           8.0908e-01,  9.0690e-01],\n",
      "         [-1.0150e-01,  1.5014e-01, -3.5610e-02,  ...,  2.4083e-01,\n",
      "           4.6067e-01,  5.6107e-01],\n",
      "         [-7.9509e-02, -5.1065e-04,  1.2184e-01,  ...,  3.9155e-01,\n",
      "           8.4011e-01,  4.6107e-01]],\n",
      "\n",
      "        [[-1.1005e-01,  7.5880e-02,  4.9293e-02,  ...,  6.3681e-02,\n",
      "           1.5750e-01,  6.2502e-01],\n",
      "         [-1.0558e-01,  3.4605e-02, -2.1646e-01,  ...,  1.8196e-01,\n",
      "           3.5853e-01,  5.0822e-01],\n",
      "         [-1.0428e-01,  5.1478e-02,  5.4198e-02,  ...,  3.1730e-01,\n",
      "           6.4013e-01,  7.5602e-01],\n",
      "         ...,\n",
      "         [-8.6754e-02,  1.1137e-01, -1.4673e-01,  ...,  6.1675e-01,\n",
      "           8.8576e-01,  1.0554e+00],\n",
      "         [-7.4514e-02, -5.8294e-02,  4.8722e-02,  ...,  2.1784e-01,\n",
      "           6.8704e-01,  5.3194e-01],\n",
      "         [-6.7744e-02, -9.1571e-03,  1.2747e-01,  ...,  5.3520e-01,\n",
      "           4.5745e-01,  8.1608e-01]],\n",
      "\n",
      "        [[-1.5823e-01, -1.5295e-01,  2.8792e-02,  ...,  3.9175e-01,\n",
      "           8.6666e-01,  8.2938e-01],\n",
      "         [-1.1602e-01,  3.1143e-03,  8.4120e-02,  ...,  1.9192e-01,\n",
      "           1.0220e+00,  6.2463e-01],\n",
      "         [-7.2710e-02,  2.2985e-02,  8.8289e-02,  ...,  6.5188e-01,\n",
      "           8.6076e-01,  3.1973e-01],\n",
      "         ...,\n",
      "         [-1.1798e-01, -9.3898e-02,  2.4996e-02,  ..., -4.5738e-02,\n",
      "           9.2267e-01,  5.0352e-01],\n",
      "         [-2.8927e-02,  1.4248e-01,  3.8282e-02,  ...,  5.4855e-01,\n",
      "           9.6133e-01,  5.0736e-01],\n",
      "         [-8.5403e-02,  1.5547e-01,  6.1779e-02,  ...,  4.9957e-01,\n",
      "           7.9954e-01,  6.4615e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-8.7614e-02, -2.2894e-02,  1.7069e-01,  ..., -8.1479e-03,\n",
      "           1.1242e+00,  3.4609e-01],\n",
      "         [-7.6288e-02, -7.9467e-02,  1.4292e-01,  ...,  1.5593e-01,\n",
      "           1.1989e+00,  3.6214e-01],\n",
      "         [-1.1372e-01, -1.2329e-03,  1.0309e-01,  ...,  1.1265e-01,\n",
      "           8.7236e-01,  6.4307e-01],\n",
      "         ...,\n",
      "         [-1.0176e-01, -2.4466e-03,  8.9944e-02,  ...,  8.4446e-01,\n",
      "           1.0296e+00,  8.7912e-01],\n",
      "         [-1.1245e-01,  4.5680e-02,  1.8572e-01,  ...,  3.0849e-01,\n",
      "           9.9888e-01,  6.5308e-01],\n",
      "         [-1.3703e-01,  1.5775e-02,  1.2358e-01,  ...,  3.1840e-01,\n",
      "           8.9885e-01,  6.4907e-01]],\n",
      "\n",
      "        [[-1.1372e-01,  3.9815e-02,  6.5314e-02,  ...,  6.9348e-01,\n",
      "           1.0218e+00,  5.8577e-01],\n",
      "         [-1.0268e-01,  1.7466e-02, -2.1585e-01,  ...,  4.2762e-01,\n",
      "           9.2220e-01,  6.7531e-01],\n",
      "         [-1.1928e-01,  7.3003e-02,  1.3080e-01,  ...,  4.3044e-01,\n",
      "           9.2448e-01,  8.2178e-01],\n",
      "         ...,\n",
      "         [-8.7309e-02, -5.9002e-02,  1.3351e-01,  ...,  1.9246e-01,\n",
      "           8.1528e-01,  7.8339e-01],\n",
      "         [-1.1633e-01,  4.9528e-02, -5.2582e-03,  ...,  4.2614e-01,\n",
      "           1.3458e+00,  4.8882e-01],\n",
      "         [-1.0962e-01, -1.5499e-01,  7.1544e-02,  ...,  1.8045e-01,\n",
      "           1.0294e+00,  5.0904e-01]],\n",
      "\n",
      "        [[-7.1941e-02, -9.3293e-02, -1.3890e-02,  ...,  3.0553e-01,\n",
      "           7.4158e-01,  8.6045e-01],\n",
      "         [-1.8105e-01,  1.5259e-01, -2.8839e-02,  ...,  2.8330e-01,\n",
      "           1.0120e+00,  4.8463e-01],\n",
      "         [-6.0252e-02, -8.7461e-02, -4.9338e-02,  ...,  4.4308e-01,\n",
      "           6.7979e-01,  4.8590e-01],\n",
      "         ...,\n",
      "         [-1.3733e-01, -1.0676e-02,  4.3914e-02,  ..., -2.6664e-02,\n",
      "           1.0150e+00,  5.3148e-01],\n",
      "         [-4.2440e-02,  1.5087e-02,  9.6876e-02,  ...,  2.4075e-01,\n",
      "           6.2479e-01,  5.7897e-01],\n",
      "         [-8.7117e-02,  4.3607e-02,  7.8999e-02,  ...,  3.7388e-01,\n",
      "           8.7041e-01,  2.4317e-01]]]), tensor([[[ 2.8853e-01, -4.7892e-02, -7.0296e-02,  ...,  6.4778e-01,\n",
      "           7.2313e-01,  2.2367e-01],\n",
      "         [ 2.4318e-01, -5.9688e-02, -2.1649e-02,  ...,  9.0040e-01,\n",
      "           5.5479e-01, -1.5262e-01],\n",
      "         [ 2.1404e-01, -2.2920e-02, -4.2837e-02,  ...,  7.9121e-01,\n",
      "           6.4317e-01,  3.9556e-01],\n",
      "         ...,\n",
      "         [ 3.4882e-01, -4.2560e-02, -3.9171e-02,  ...,  6.7878e-01,\n",
      "           5.1533e-01,  6.6290e-01],\n",
      "         [ 3.3807e-01, -3.7642e-03,  4.8359e-02,  ...,  6.4016e-01,\n",
      "           3.6962e-01,  4.2544e-01],\n",
      "         [ 3.8281e-01, -3.0108e-02, -7.8194e-02,  ...,  7.6941e-01,\n",
      "           4.1109e-01, -3.3644e-02]],\n",
      "\n",
      "        [[ 3.4711e-01, -5.6512e-02, -5.0618e-02,  ...,  4.3352e-01,\n",
      "          -2.0520e-03,  5.3423e-01],\n",
      "         [-1.3686e-02, -5.5426e-02, -1.4417e-01,  ...,  5.0411e-01,\n",
      "           1.1966e-01,  5.6732e-02],\n",
      "         [ 4.2294e-01, -2.5661e-02, -1.5763e-01,  ...,  1.8822e-01,\n",
      "           3.3790e-01,  3.3742e-01],\n",
      "         ...,\n",
      "         [ 4.4337e-01, -3.0132e-02, -9.1307e-02,  ...,  8.8974e-01,\n",
      "           4.5516e-01,  3.2351e-01],\n",
      "         [ 3.5483e-01, -1.6517e-02, -1.4648e-01,  ...,  4.8055e-01,\n",
      "           6.4478e-01,  2.5475e-01],\n",
      "         [ 4.7531e-01, -1.2546e-01, -1.4455e-01,  ...,  1.0770e+00,\n",
      "           6.2617e-01,  3.9417e-01]],\n",
      "\n",
      "        [[ 3.0658e-01, -2.4986e-02, -1.4245e-01,  ...,  6.4148e-01,\n",
      "           7.3535e-01,  3.3199e-01],\n",
      "         [-1.7776e-02, -5.4294e-02, -7.7355e-02,  ...,  1.1859e-01,\n",
      "           8.8437e-01,  2.9828e-01],\n",
      "         [ 2.9131e-01, -2.7949e-03, -6.4511e-02,  ...,  4.8790e-01,\n",
      "           6.4008e-01,  2.2895e-01],\n",
      "         ...,\n",
      "         [ 3.8221e-01, -9.3808e-02,  3.6801e-02,  ...,  3.9920e-01,\n",
      "           5.4613e-01,  1.5532e-01],\n",
      "         [ 4.2026e-01, -1.0172e-02, -7.0265e-02,  ...,  8.6621e-01,\n",
      "           6.4101e-01,  1.2557e-01],\n",
      "         [ 3.8933e-01, -8.2215e-02, -1.0403e-01,  ...,  3.7170e-01,\n",
      "           6.7041e-01,  2.1946e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.7177e-01, -3.3374e-02, -7.4910e-02,  ...,  4.3441e-01,\n",
      "           8.4955e-01, -3.9482e-02],\n",
      "         [ 2.8148e-01, -3.9882e-02, -8.3155e-02,  ...,  5.4269e-01,\n",
      "           9.6050e-01, -4.7209e-02],\n",
      "         [ 2.8843e-01, -7.5992e-02, -8.7602e-02,  ...,  4.4541e-01,\n",
      "           6.8987e-01,  2.1990e-01],\n",
      "         ...,\n",
      "         [ 3.2680e-01, -5.7516e-02, -8.5686e-02,  ...,  1.0105e+00,\n",
      "           8.2731e-01,  4.1935e-01],\n",
      "         [ 2.5512e-01,  2.7408e-02, -3.7725e-02,  ...,  5.8001e-01,\n",
      "           9.2239e-01,  3.3012e-01],\n",
      "         [ 1.4124e-02, -2.4187e-02, -6.3882e-02,  ...,  7.0025e-01,\n",
      "           6.4729e-01,  3.3677e-01]],\n",
      "\n",
      "        [[ 3.7447e-02, -1.1677e-02, -4.4041e-02,  ...,  1.0263e+00,\n",
      "           8.9752e-01,  1.6618e-01],\n",
      "         [ 3.6758e-01, -2.7472e-02, -1.0794e-01,  ...,  7.2949e-01,\n",
      "           8.3016e-01,  4.0715e-01],\n",
      "         [ 3.2575e-01, -5.0619e-02, -4.0636e-02,  ...,  8.5587e-01,\n",
      "           5.7328e-01,  7.2412e-01],\n",
      "         ...,\n",
      "         [ 4.1588e-02, -4.6298e-04, -8.6074e-02,  ...,  4.8407e-01,\n",
      "           6.0139e-01,  2.9518e-01],\n",
      "         [ 3.4837e-01, -1.5400e-02, -1.3337e-01,  ...,  2.7953e-01,\n",
      "           9.4263e-01,  5.9866e-02],\n",
      "         [ 3.1952e-01, -3.0428e-02, -1.2801e-01,  ...,  6.8977e-01,\n",
      "           6.7662e-01,  1.6869e-01]],\n",
      "\n",
      "        [[ 3.5868e-01, -1.2130e-01, -9.4026e-02,  ...,  6.5087e-01,\n",
      "           2.3806e-01,  2.7628e-01],\n",
      "         [ 4.5828e-01, -7.9029e-02, -1.5275e-01,  ...,  5.1868e-01,\n",
      "           5.9879e-01,  2.6186e-01],\n",
      "         [ 8.6211e-02, -1.0382e-01, -1.3228e-01,  ...,  5.9836e-01,\n",
      "           2.7881e-01,  3.2707e-01],\n",
      "         ...,\n",
      "         [ 3.2830e-01, -1.6080e-02, -9.4176e-02,  ..., -7.0606e-02,\n",
      "           7.0244e-01,  1.2114e-01],\n",
      "         [ 3.2402e-01, -5.4645e-02, -8.3083e-02,  ...,  5.7404e-01,\n",
      "           5.7037e-01,  1.4757e-01],\n",
      "         [ 3.3021e-01, -4.7026e-02, -7.5678e-02,  ...,  7.2017e-01,\n",
      "           6.9487e-01, -1.3498e-01]]]), tensor([[[ 0.2518, -0.0435, -0.1162,  ...,  0.6903,  0.3381,  0.8141],\n",
      "         [ 0.1939, -0.0610, -0.0711,  ...,  0.7129,  0.0508,  0.1657],\n",
      "         [ 0.2171, -0.0619, -0.0944,  ...,  0.5076,  0.1530,  0.9316],\n",
      "         ...,\n",
      "         [ 0.2579, -0.0307, -0.0799,  ...,  0.4419,  0.1140,  1.1340],\n",
      "         [ 0.1789, -0.0344,  0.1071,  ...,  0.4529,  0.0023,  1.1211],\n",
      "         [ 0.2137,  0.0030, -0.1345,  ...,  0.6578,  0.0063, -0.2477]],\n",
      "\n",
      "        [[ 0.2642, -0.0715, -0.0485,  ...,  0.2191, -0.2890,  1.1247],\n",
      "         [ 0.1133, -0.0645, -0.0902,  ...,  0.3168,  0.0282,  0.5081],\n",
      "         [ 0.3040, -0.0588, -0.1188,  ...,  0.2255,  0.0761,  0.8288],\n",
      "         ...,\n",
      "         [ 0.2971, -0.0432, -0.1354,  ...,  0.7680,  0.1749,  0.7978],\n",
      "         [ 0.2994, -0.0321, -0.1175,  ...,  0.1803,  0.2232,  0.8407],\n",
      "         [ 0.2893, -0.0475, -0.1447,  ...,  0.8223,  0.3290,  0.8178]],\n",
      "\n",
      "        [[ 0.1753, -0.0463, -0.1188,  ...,  0.3420,  0.2108,  0.8343],\n",
      "         [ 0.2024, -0.0433, -0.0862,  ...,  0.0728,  0.3671,  0.9132],\n",
      "         [ 0.2854, -0.0327, -0.0824,  ...,  0.1696,  0.1369,  0.7986],\n",
      "         ...,\n",
      "         [ 0.2661, -0.0443, -0.0731,  ...,  0.2488,  0.1187,  0.6972],\n",
      "         [ 0.2502,  0.0140, -0.1456,  ...,  1.0406,  0.5436,  0.5883],\n",
      "         [ 0.1549,  0.0132, -0.2151,  ...,  0.0927,  0.2578,  0.6818]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2291, -0.0199, -0.0976,  ...,  0.0664,  0.3863,  0.4425],\n",
      "         [ 0.2582,  0.0022, -0.0888,  ...,  0.1711,  0.3687,  0.4474],\n",
      "         [ 0.2034, -0.0789, -0.0973,  ...,  0.0825,  0.0992,  0.7092],\n",
      "         ...,\n",
      "         [ 0.2650, -0.0499, -0.0843,  ...,  0.7725,  0.2878,  0.8370],\n",
      "         [ 0.1564, -0.0405, -0.0860,  ...,  0.2887,  0.3171,  0.8063],\n",
      "         [ 0.1578,  0.0112, -0.0958,  ...,  0.4186,  0.0480,  0.7549]],\n",
      "\n",
      "        [[ 0.1853,  0.0137, -0.0855,  ...,  1.2060,  0.4064,  0.6926],\n",
      "         [ 0.2485, -0.0295, -0.2178,  ...,  0.4944,  0.3855,  0.8453],\n",
      "         [ 0.1806, -0.0845, -0.0809,  ...,  1.1909,  0.1373,  1.3002],\n",
      "         ...,\n",
      "         [ 0.2326,  0.0066,  0.0509,  ...,  0.2447,  0.1260,  0.8269],\n",
      "         [ 0.3057, -0.0238, -0.1219,  ..., -0.0380,  0.4733,  0.7706],\n",
      "         [ 0.2501, -0.0525, -0.0898,  ...,  0.3879,  0.2146,  0.7002]],\n",
      "\n",
      "        [[ 0.2307, -0.0109, -0.1188,  ...,  0.3198, -0.0505,  0.8211],\n",
      "         [ 0.3574, -0.0143,  0.0539,  ...,  0.1783,  0.4149,  0.8156],\n",
      "         [ 0.1732, -0.0452, -0.1495,  ...,  0.2191, -0.0661,  0.7883],\n",
      "         ...,\n",
      "         [ 0.1654, -0.0729, -0.0982,  ..., -0.4011,  0.1625,  0.7130],\n",
      "         [ 0.2533, -0.0199,  0.0242,  ...,  0.3209,  0.1083,  0.5480],\n",
      "         [ 0.2723, -0.0128,  0.0348,  ...,  0.5118,  0.2422, -0.2785]]]), tensor([[[ 9.0911e-03, -8.2119e-02, -5.6567e-03,  ...,  3.4413e-01,\n",
      "           3.7800e-01,  8.9219e-01],\n",
      "         [-3.1352e-02, -1.2085e-01, -6.1250e-03,  ...,  4.4883e-01,\n",
      "           7.7627e-02,  3.2000e-01],\n",
      "         [-4.9363e-03,  7.0044e-03,  1.6132e-02,  ...,  2.5727e-01,\n",
      "           4.8100e-01,  8.4456e-01],\n",
      "         ...,\n",
      "         [ 6.6261e-02,  3.5922e-02,  5.7256e-02,  ...,  1.3324e-01,\n",
      "           5.2774e-01,  1.0933e+00],\n",
      "         [ 2.2525e-02, -1.1683e-01,  1.0088e-01,  ...,  5.4429e-01,\n",
      "           4.1629e-01,  1.0757e+00],\n",
      "         [-2.5449e-02, -5.4863e-02,  8.9993e-02,  ...,  2.2538e-01,\n",
      "           3.9527e-01,  7.0742e-02]],\n",
      "\n",
      "        [[ 1.9785e-02, -3.6260e-03,  3.0659e-02,  ..., -8.3535e-02,\n",
      "           3.3290e-01,  8.8406e-01],\n",
      "         [-6.8827e-02, -1.2474e-01,  4.8839e-02,  ..., -5.0826e-03,\n",
      "           4.3650e-01,  6.4351e-01],\n",
      "         [-7.7122e-04, -7.4814e-02,  1.7647e-02,  ..., -1.0661e-02,\n",
      "           3.9952e-01,  9.5452e-01],\n",
      "         ...,\n",
      "         [ 9.6719e-03, -5.8384e-02,  1.9770e-02,  ...,  3.1535e-01,\n",
      "           3.5417e-01,  8.5605e-01],\n",
      "         [ 4.5114e-02, -4.9000e-02, -4.8705e-03,  ..., -7.9851e-02,\n",
      "           4.7651e-01,  7.8211e-01],\n",
      "         [-7.3960e-04, -1.2617e-01,  5.7027e-02,  ...,  3.4880e-01,\n",
      "           7.2873e-01,  8.4429e-01]],\n",
      "\n",
      "        [[ 1.6633e-02, -8.8451e-02,  2.8702e-02,  ..., -4.7044e-02,\n",
      "           5.0796e-01,  6.6142e-01],\n",
      "         [ 2.5846e-02, -9.2268e-03,  1.9959e-02,  ..., -9.1761e-02,\n",
      "           3.5357e-01,  8.5171e-01],\n",
      "         [ 3.5834e-02, -1.1252e-02, -6.5161e-02,  ..., -4.1232e-02,\n",
      "           4.9624e-01,  7.8575e-01],\n",
      "         ...,\n",
      "         [ 9.0017e-04, -5.2230e-02,  3.7548e-03,  ..., -1.4911e-02,\n",
      "           5.4085e-01,  8.0310e-01],\n",
      "         [ 1.8449e-03, -4.4546e-02, -4.3451e-02,  ...,  5.6113e-01,\n",
      "           7.7730e-01,  6.3901e-01],\n",
      "         [-1.2184e-02, -5.6237e-02,  2.9532e-02,  ..., -2.4558e-01,\n",
      "           5.5238e-01,  8.1124e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.9392e-02, -1.5647e-02,  7.5187e-02,  ..., -5.9609e-02,\n",
      "           5.6349e-01,  5.4430e-01],\n",
      "         [-3.5850e-02, -6.7758e-02,  1.2217e-01,  ..., -5.0091e-02,\n",
      "           5.6334e-01,  5.0951e-01],\n",
      "         [ 1.6828e-02, -5.1278e-03,  8.9107e-02,  ..., -7.0330e-02,\n",
      "           4.0029e-01,  7.2263e-01],\n",
      "         ...,\n",
      "         [ 2.9696e-02, -7.4017e-02,  7.3955e-02,  ...,  3.9373e-01,\n",
      "           4.5948e-01,  8.9543e-01],\n",
      "         [-3.9267e-02, -3.0924e-02,  5.6187e-02,  ...,  9.3193e-02,\n",
      "           5.7317e-01,  8.3099e-01],\n",
      "         [-6.1865e-02, -1.3233e-01, -8.0057e-02,  ...,  1.9072e-01,\n",
      "           4.5301e-01,  8.7284e-01]],\n",
      "\n",
      "        [[-1.7961e-02, -2.1694e-02, -4.4692e-03,  ...,  6.6152e-01,\n",
      "           6.1190e-01,  8.5769e-01],\n",
      "         [ 3.4829e-02, -6.3141e-02, -2.2045e-03,  ...,  4.8417e-02,\n",
      "           4.4446e-01,  8.8490e-01],\n",
      "         [-2.6946e-02, -1.7733e-02,  1.7390e-01,  ...,  1.0578e+00,\n",
      "           3.0902e-01,  1.1219e+00],\n",
      "         ...,\n",
      "         [ 1.1732e-02, -4.1934e-02,  3.4879e-02,  ..., -2.8922e-02,\n",
      "           4.7573e-01,  7.6196e-01],\n",
      "         [ 2.8063e-02, -5.6770e-02, -5.3531e-02,  ..., -2.0613e-01,\n",
      "           7.9539e-01,  7.4620e-01],\n",
      "         [ 1.4212e-02, -1.0320e-02, -6.6116e-02,  ...,  7.6544e-02,\n",
      "           5.3319e-01,  8.6476e-01]],\n",
      "\n",
      "        [[ 2.8747e-02, -1.7206e-02, -4.9827e-03,  ...,  6.0560e-02,\n",
      "           4.3918e-01,  8.1780e-01],\n",
      "         [-8.0254e-02, -1.5898e-01,  3.1621e-02,  ..., -1.1811e-01,\n",
      "           7.0463e-01,  8.9002e-01],\n",
      "         [-4.2038e-03, -8.5219e-02,  3.9243e-02,  ..., -8.7573e-02,\n",
      "           3.9418e-01,  8.4841e-01],\n",
      "         ...,\n",
      "         [-2.8863e-02, -6.7896e-02,  2.5051e-02,  ..., -4.3929e-01,\n",
      "           3.1391e-01,  7.9813e-01],\n",
      "         [ 2.9826e-02, -8.7309e-02,  4.3797e-02,  ...,  1.9820e-02,\n",
      "           4.8108e-01,  6.5345e-01],\n",
      "         [-3.5604e-02, -5.7144e-02,  1.1233e-01,  ...,  2.1852e-01,\n",
      "           3.5314e-01,  2.1350e-01]]]), tensor([[[-0.1123, -0.2418, -0.0936,  ...,  0.5349,  0.0332,  1.3243],\n",
      "         [-0.0184, -0.3163, -0.1679,  ...,  0.8356, -0.0909,  0.5069],\n",
      "         [ 0.0343, -0.1132, -0.1684,  ...,  0.4011,  0.2040,  1.1310],\n",
      "         ...,\n",
      "         [ 0.0755, -0.2305, -0.1816,  ...,  0.3472,  0.2752,  1.1652],\n",
      "         [ 0.0018, -0.3834, -0.1963,  ...,  0.8487,  0.0684,  1.1420],\n",
      "         [-0.1564, -0.0051, -0.1482,  ...,  0.3952,  0.2230,  0.0131]],\n",
      "\n",
      "        [[ 0.0996, -0.1480, -0.0636,  ...,  0.0430,  0.1473,  0.9308],\n",
      "         [ 0.0282, -0.2319, -0.2414,  ...,  0.1344,  0.2395,  0.8174],\n",
      "         [-0.0353, -0.2777, -0.2347,  ...,  0.1465,  0.0061,  1.1420],\n",
      "         ...,\n",
      "         [-0.0215, -0.2455, -0.1648,  ...,  0.4682,  0.1821,  1.1245],\n",
      "         [ 0.0243, -0.4037, -0.1924,  ...,  0.0965,  0.0607,  0.8741],\n",
      "         [ 0.1042, -0.2877, -0.1439,  ...,  0.4006,  0.3789,  1.0384]],\n",
      "\n",
      "        [[-0.0930, -0.1419, -0.2331,  ...,  0.1757,  0.1942,  0.5767],\n",
      "         [ 0.0992, -0.2463, -0.2052,  ...,  0.1514,  0.1994,  0.9417],\n",
      "         [-0.0071, -0.3279, -0.2299,  ...,  0.2229,  0.3027,  0.8672],\n",
      "         ...,\n",
      "         [-0.0092, -0.3114, -0.1631,  ...,  0.1331,  0.3638,  1.0067],\n",
      "         [ 0.0568, -0.2080, -0.2551,  ...,  0.7287,  0.4011,  0.8211],\n",
      "         [-0.0968, -0.3016, -0.0782,  ..., -0.1740,  0.2104,  1.1352]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0298, -0.3159, -0.2356,  ...,  0.1906,  0.1697,  0.6728],\n",
      "         [-0.0275, -0.3966, -0.1756,  ...,  0.2194,  0.2978,  0.6025],\n",
      "         [-0.0046, -0.2562, -0.2463,  ...,  0.1454,  0.0943,  0.8330],\n",
      "         ...,\n",
      "         [-0.0475, -0.1129, -0.2275,  ...,  0.3707,  0.1308,  1.1389],\n",
      "         [ 0.0068, -0.1227, -0.1918,  ...,  0.3124,  0.2847,  1.0068],\n",
      "         [-0.1219, -0.2972, -0.2576,  ...,  0.4279,  0.1638,  1.0889]],\n",
      "\n",
      "        [[ 0.0272, -0.3016, -0.2414,  ...,  0.8848,  0.3852,  0.9782],\n",
      "         [ 0.1897, -0.1303, -0.1791,  ...,  0.2398,  0.3068,  0.9719],\n",
      "         [-0.0314, -0.0359, -0.2422,  ...,  1.3511,  0.1195,  1.2025],\n",
      "         ...,\n",
      "         [ 0.1013, -0.2198, -0.2242,  ...,  0.1542,  0.2692,  0.8745],\n",
      "         [ 0.0323, -0.1282, -0.2078,  ..., -0.0756,  0.4575,  0.6061],\n",
      "         [ 0.0995, -0.2543, -0.2046,  ...,  0.1843,  0.3393,  0.9700]],\n",
      "\n",
      "        [[ 0.0971, -0.2294, -0.1282,  ...,  0.3712,  0.2635,  0.6774],\n",
      "         [ 0.0411, -0.3083, -0.1657,  ...,  0.0929,  0.4167,  0.8506],\n",
      "         [ 0.1228, -0.3190, -0.0974,  ...,  0.0880,  0.0897,  0.8290],\n",
      "         ...,\n",
      "         [-0.0282, -0.2515, -0.1638,  ..., -0.3057,  0.0569,  0.9458],\n",
      "         [-0.0078, -0.2198, -0.2577,  ...,  0.1629,  0.1789,  0.8455],\n",
      "         [-0.0241, -0.2894, -0.2065,  ...,  0.3648,  0.1954,  0.4995]]]), tensor([[[ 0.0522,  0.0275,  0.0816,  ...,  0.5454,  0.2874,  0.9456],\n",
      "         [ 0.0567,  0.0960,  0.1057,  ...,  0.8073,  0.1039,  0.3221],\n",
      "         [ 0.1013,  0.0757,  0.0368,  ...,  0.4265,  0.4189,  0.7821],\n",
      "         ...,\n",
      "         [ 0.1501, -0.0137,  0.0293,  ...,  0.3782,  0.6050,  0.9070],\n",
      "         [ 0.1204, -0.0141, -0.0177,  ...,  0.9321,  0.4886,  0.7916],\n",
      "         [ 0.0951,  0.0810,  0.0948,  ...,  0.4260,  0.5418, -0.1451]],\n",
      "\n",
      "        [[ 0.1164, -0.0600,  0.1060,  ...,  0.0549,  0.3842,  0.6048],\n",
      "         [ 0.1077,  0.0700, -0.0393,  ...,  0.1350,  0.4657,  0.5092],\n",
      "         [ 0.1277,  0.0211,  0.0257,  ...,  0.2505,  0.3633,  0.7586],\n",
      "         ...,\n",
      "         [ 0.0251,  0.0789,  0.0605,  ...,  0.4223,  0.4379,  0.8362],\n",
      "         [ 0.1691,  0.0046,  0.0478,  ...,  0.1649,  0.3259,  0.5574],\n",
      "         [ 0.0788,  0.0258,  0.0658,  ...,  0.4941,  0.6275,  0.7506]],\n",
      "\n",
      "        [[ 0.0239, -0.0816,  0.0468,  ...,  0.1907,  0.4872,  0.3171],\n",
      "         [ 0.1621, -0.1210,  0.0437,  ...,  0.1761,  0.2596,  0.6066],\n",
      "         [ 0.0698,  0.0410,  0.0691,  ...,  0.2564,  0.3450,  0.6762],\n",
      "         ...,\n",
      "         [ 0.1717,  0.0201,  0.0593,  ...,  0.2253,  0.5878,  0.6027],\n",
      "         [ 0.1455, -0.1252,  0.0303,  ...,  0.6860,  0.5811,  0.7331],\n",
      "         [ 0.0352,  0.0968,  0.0787,  ..., -0.0923,  0.5494,  0.7772]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1021,  0.0269,  0.0798,  ...,  0.2543,  0.5093,  0.3316],\n",
      "         [ 0.1855, -0.0110, -0.0039,  ...,  0.3825,  0.6663,  0.3818],\n",
      "         [ 0.1037, -0.0266,  0.0450,  ...,  0.2667,  0.2620,  0.4708],\n",
      "         ...,\n",
      "         [ 0.0518,  0.1314, -0.0397,  ...,  0.3904,  0.3527,  0.7221],\n",
      "         [ 0.0852, -0.0875,  0.0831,  ...,  0.3201,  0.4062,  0.6421],\n",
      "         [ 0.0479,  0.0750,  0.0673,  ...,  0.4181,  0.3855,  0.7361]],\n",
      "\n",
      "        [[ 0.0606,  0.1035,  0.0240,  ...,  0.8725,  0.4761,  0.7229],\n",
      "         [ 0.1674,  0.0516,  0.0530,  ...,  0.1839,  0.5289,  0.6902],\n",
      "         [-0.0021,  0.1869,  0.0826,  ...,  1.1730,  0.3577,  0.8892],\n",
      "         ...,\n",
      "         [ 0.1583,  0.0521, -0.0374,  ...,  0.1723,  0.5004,  0.5115],\n",
      "         [ 0.1771, -0.1238,  0.0371,  ..., -0.0921,  0.6505,  0.5903],\n",
      "         [ 0.1754,  0.0160,  0.0059,  ...,  0.1967,  0.6671,  0.7314]],\n",
      "\n",
      "        [[ 0.1633,  0.0494,  0.0384,  ...,  0.3303,  0.6222,  0.5470],\n",
      "         [ 0.1295,  0.0450,  0.0314,  ...,  0.1775,  0.8056,  0.6003],\n",
      "         [ 0.1928,  0.0181,  0.0873,  ...,  0.1075,  0.3077,  0.6581],\n",
      "         ...,\n",
      "         [ 0.0970, -0.1114,  0.0184,  ..., -0.1992,  0.1577,  0.6690],\n",
      "         [ 0.1097,  0.1144,  0.0591,  ...,  0.1203,  0.3902,  0.5525],\n",
      "         [ 0.0884,  0.0530,  0.0568,  ...,  0.3554,  0.4420,  0.2558]]]), tensor([[[ 0.0258,  0.3202, -0.1187,  ...,  0.5446,  0.3172,  1.4977],\n",
      "         [ 0.2926,  0.5206, -0.2128,  ...,  1.1774,  0.0797,  0.4219],\n",
      "         [ 0.2538,  0.4598, -0.1189,  ...,  0.5677,  0.3104,  0.9715],\n",
      "         ...,\n",
      "         [ 0.3724,  0.2557, -0.3504,  ...,  0.4809,  0.5493,  0.8638],\n",
      "         [ 0.2701,  0.3941, -0.2121,  ...,  1.5661,  0.3929,  1.0561],\n",
      "         [ 0.1326,  0.5525, -0.1653,  ...,  0.5598,  0.3393, -0.2629]],\n",
      "\n",
      "        [[ 0.2014,  0.2721, -0.1676,  ..., -0.0801,  0.2346,  0.5630],\n",
      "         [ 0.4033,  0.5171, -0.1720,  ...,  0.1095,  0.3614,  0.6792],\n",
      "         [ 0.2765,  0.4548, -0.2528,  ...,  0.1015,  0.1830,  1.1216],\n",
      "         ...,\n",
      "         [ 0.2416,  0.4635, -0.3483,  ...,  0.5144,  0.2489,  1.0210],\n",
      "         [ 0.3597,  0.3965, -0.1071,  ...,  0.2642,  0.1072,  0.5626],\n",
      "         [ 0.2873,  0.4817, -0.0600,  ...,  0.5038,  0.5651,  1.1145]],\n",
      "\n",
      "        [[ 0.2720,  0.1609, -0.2438,  ...,  0.2254,  0.4441,  0.4302],\n",
      "         [ 0.4192,  0.4579, -0.0332,  ...,  0.1691,  0.1371,  0.7476],\n",
      "         [ 0.2947,  0.5665, -0.1863,  ...,  0.2665,  0.1810,  0.8375],\n",
      "         ...,\n",
      "         [ 0.3952,  0.0578, -0.0349,  ...,  0.2580,  0.5482,  0.6396],\n",
      "         [ 0.4178,  0.4746, -0.2032,  ...,  0.9574,  0.5476,  0.9087],\n",
      "         [ 0.2700,  0.5321, -0.1995,  ..., -0.1340,  0.4857,  1.0483]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2592,  0.2485, -0.0804,  ...,  0.4855,  0.3359,  0.4621],\n",
      "         [ 0.2724,  0.4246, -0.0515,  ...,  0.3514,  0.5633,  0.4998],\n",
      "         [ 0.3401,  0.4082, -0.1235,  ...,  0.4219,  0.0588,  0.5059],\n",
      "         ...,\n",
      "         [ 0.3271,  0.5129, -0.0527,  ...,  0.3130,  0.1347,  0.9957],\n",
      "         [ 0.2888,  0.4085, -0.0831,  ...,  0.4586,  0.1914,  0.7955],\n",
      "         [ 0.3670,  0.1230, -0.2119,  ...,  0.5774,  0.3448,  0.8041]],\n",
      "\n",
      "        [[ 0.0590,  0.4990, -0.2208,  ...,  1.1416,  0.2508,  0.7724],\n",
      "         [ 0.2030,  0.4368, -0.0555,  ...,  0.1946,  0.4282,  0.8589],\n",
      "         [ 0.2931,  0.5809, -0.1918,  ...,  1.3946,  0.2223,  0.8686],\n",
      "         ...,\n",
      "         [ 0.4866,  0.6558, -0.0640,  ...,  0.1726,  0.2969,  0.4390],\n",
      "         [ 0.2986,  0.2156, -0.0452,  ..., -0.2113,  0.4494,  0.5363],\n",
      "         [ 0.3097,  0.4860, -0.1229,  ...,  0.1287,  0.6008,  0.9735]],\n",
      "\n",
      "        [[ 0.4571,  0.4323, -0.1949,  ...,  0.3019,  0.5833,  0.3835],\n",
      "         [ 0.3088,  0.4926,  0.1043,  ...,  0.1230,  0.8130,  0.8893],\n",
      "         [ 0.2799,  0.4842, -0.1299,  ..., -0.0183,  0.2451,  0.7221],\n",
      "         ...,\n",
      "         [ 0.4055,  0.4841, -0.2235,  ..., -0.2961, -0.0473,  0.7011],\n",
      "         [ 0.2829,  0.2645, -0.0854,  ...,  0.1037,  0.3664,  0.6682],\n",
      "         [ 0.2531,  0.3025, -0.1701,  ...,  0.3785,  0.2998,  0.3619]]]), tensor([[[-1.1702e-01, -8.9007e-02,  1.6290e-01,  ...,  5.1307e-01,\n",
      "           3.4221e-01,  1.4616e+00],\n",
      "         [ 1.7943e-01, -1.6206e-01,  9.8992e-02,  ...,  1.1529e+00,\n",
      "           1.6549e-01,  3.0259e-01],\n",
      "         [ 6.6613e-02, -9.7590e-02,  4.3944e-02,  ...,  5.4392e-01,\n",
      "           2.5503e-01,  9.4606e-01],\n",
      "         ...,\n",
      "         [ 1.3603e-01, -1.8150e-01,  7.4207e-03,  ...,  4.0347e-01,\n",
      "           3.0984e-01,  8.3515e-01],\n",
      "         [ 2.3191e-01, -1.6774e-01, -4.7651e-03,  ...,  1.5852e+00,\n",
      "           2.3868e-01,  8.7400e-01],\n",
      "         [ 6.7980e-02, -1.3875e-01, -5.8317e-02,  ...,  5.4658e-01,\n",
      "           2.1333e-01,  7.9664e-04]],\n",
      "\n",
      "        [[ 9.2896e-02, -1.8783e-02,  4.2666e-02,  ..., -7.8426e-02,\n",
      "           7.0076e-02,  6.0860e-01],\n",
      "         [ 1.1757e-01, -1.6507e-01,  6.7900e-02,  ...,  6.7485e-02,\n",
      "           2.9815e-01,  5.4405e-01],\n",
      "         [ 1.2331e-01, -2.0260e-01,  5.9143e-02,  ...,  5.1280e-02,\n",
      "           2.5114e-01,  1.1809e+00],\n",
      "         ...,\n",
      "         [ 1.2818e-01, -1.1217e-01, -1.6131e-02,  ...,  4.9745e-01,\n",
      "           2.3465e-02,  1.0224e+00],\n",
      "         [ 2.7056e-01, -1.7509e-01,  7.9734e-02,  ...,  2.0807e-01,\n",
      "           1.8420e-01,  5.4923e-01],\n",
      "         [ 1.5571e-01, -2.0446e-01, -3.4484e-02,  ...,  6.5462e-01,\n",
      "           3.9669e-01,  8.5853e-01]],\n",
      "\n",
      "        [[ 9.2038e-02, -4.3624e-02, -5.2167e-02,  ...,  1.1463e-01,\n",
      "           2.1444e-01,  4.4431e-01],\n",
      "         [ 1.5387e-01,  3.1523e-02,  8.4946e-02,  ...,  1.4292e-01,\n",
      "           1.1569e-01,  7.7358e-01],\n",
      "         [ 1.0246e-01,  5.8491e-02, -2.4175e-02,  ...,  2.3233e-01,\n",
      "           1.0568e-01,  9.0673e-01],\n",
      "         ...,\n",
      "         [ 1.6225e-01, -3.7329e-01, -1.5452e-02,  ...,  2.5006e-01,\n",
      "           4.8565e-01,  7.5335e-01],\n",
      "         [ 1.6765e-01, -2.1739e-01,  3.3936e-02,  ...,  9.5256e-01,\n",
      "           2.5245e-01,  9.3526e-01],\n",
      "         [ 1.3666e-01, -1.9786e-01, -1.5235e-01,  ..., -1.6492e-01,\n",
      "           2.5863e-01,  1.0083e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 7.0852e-02, -2.4238e-01, -3.0901e-02,  ...,  4.0605e-01,\n",
      "           2.6997e-01,  5.6625e-01],\n",
      "         [ 9.2567e-02, -1.9660e-01,  3.0523e-03,  ...,  2.7970e-01,\n",
      "           4.7353e-01,  6.7655e-01],\n",
      "         [ 1.8425e-01, -2.8181e-01,  3.2616e-02,  ...,  3.1820e-01,\n",
      "           7.6794e-02,  5.5684e-01],\n",
      "         ...,\n",
      "         [ 8.3391e-02, -7.3403e-02,  1.3332e-01,  ...,  3.1962e-01,\n",
      "           2.0455e-01,  1.0186e+00],\n",
      "         [ 9.8815e-02, -2.4326e-01,  1.5146e-01,  ...,  4.4200e-01,\n",
      "           2.1662e-01,  8.1322e-01],\n",
      "         [ 1.0887e-01, -3.0423e-01,  1.5340e-01,  ...,  5.7665e-01,\n",
      "           2.3948e-01,  7.5247e-01]],\n",
      "\n",
      "        [[ 1.7444e-02, -9.8160e-02, -5.5657e-02,  ...,  1.2622e+00,\n",
      "           1.6933e-01,  8.9209e-01],\n",
      "         [-4.1009e-02, -1.3850e-01,  8.7896e-02,  ...,  1.8354e-01,\n",
      "           3.1641e-01,  8.1672e-01],\n",
      "         [ 1.4071e-01, -6.7592e-02, -1.8189e-02,  ...,  1.3494e+00,\n",
      "           1.6704e-01,  8.6075e-01],\n",
      "         ...,\n",
      "         [ 1.6255e-01, -1.0505e-01,  1.2542e-01,  ...,  2.4670e-01,\n",
      "           2.5715e-01,  4.6645e-01],\n",
      "         [ 1.3486e-01, -1.6224e-01,  1.4068e-01,  ..., -2.4271e-01,\n",
      "           2.1716e-01,  3.9216e-01],\n",
      "         [ 8.6634e-02, -1.0154e-01, -3.7261e-04,  ...,  2.2323e-01,\n",
      "           3.1582e-01,  9.0910e-01]],\n",
      "\n",
      "        [[ 1.5101e-01,  5.8184e-02, -5.6511e-02,  ...,  3.3925e-01,\n",
      "           4.0139e-01,  3.9815e-01],\n",
      "         [ 7.1266e-02, -1.4180e-01,  6.2842e-02,  ...,  1.2991e-01,\n",
      "           7.8527e-01,  7.0525e-01],\n",
      "         [ 1.7467e-01, -2.2112e-01,  1.8211e-02,  ...,  1.9567e-02,\n",
      "           3.0428e-01,  6.8359e-01],\n",
      "         ...,\n",
      "         [ 1.5903e-01, -2.9984e-01, -4.2866e-02,  ..., -4.0466e-01,\n",
      "           5.2329e-02,  6.5664e-01],\n",
      "         [ 2.0701e-02, -2.5043e-01,  1.1788e-01,  ...,  1.1670e-01,\n",
      "           2.1224e-01,  6.9704e-01],\n",
      "         [-3.9201e-02, -1.9948e-01,  9.0736e-02,  ...,  3.8135e-01,\n",
      "           2.2351e-01,  5.3637e-01]]]), tensor([[[-1.9468e-01,  3.2091e-02,  5.1961e-01,  ...,  6.8808e-01,\n",
      "           3.5152e-01,  7.7613e-01],\n",
      "         [ 3.5803e-02, -7.5475e-02,  8.4665e-02,  ...,  1.0875e+00,\n",
      "          -6.4017e-02, -1.1909e-01],\n",
      "         [-3.3701e-01,  3.3411e-03,  5.5910e-01,  ...,  5.8029e-01,\n",
      "          -1.7475e-01,  4.3739e-01],\n",
      "         ...,\n",
      "         [-9.8064e-02, -1.6103e-02,  3.1518e-01,  ...,  4.8719e-01,\n",
      "          -5.9261e-02,  2.1692e-01],\n",
      "         [-1.1754e-01,  7.6808e-03,  3.6468e-01,  ...,  1.3603e+00,\n",
      "          -1.4479e-01,  2.4764e-01],\n",
      "         [-2.0591e-01, -7.5110e-03,  2.7767e-01,  ...,  6.5425e-01,\n",
      "          -1.0620e-01, -2.4070e-01]],\n",
      "\n",
      "        [[-1.3754e-01,  5.2735e-02,  3.1845e-01,  ...,  2.1814e-01,\n",
      "          -1.2856e-01,  1.8707e-02],\n",
      "         [-2.0791e-01, -5.7802e-02,  3.3689e-01,  ...,  2.0806e-01,\n",
      "          -1.2422e-01,  5.1163e-02],\n",
      "         [ 3.8008e-02, -4.4891e-02,  5.0911e-01,  ...,  3.0294e-01,\n",
      "          -1.5650e-01,  4.1313e-01],\n",
      "         ...,\n",
      "         [-2.5418e-01, -7.5998e-02,  4.1518e-01,  ...,  6.3539e-01,\n",
      "          -2.7066e-01,  4.1372e-01],\n",
      "         [-4.0603e-02,  2.8525e-02,  9.0571e-02,  ...,  1.7871e-01,\n",
      "          -1.8871e-01, -1.4585e-01],\n",
      "         [-1.2578e-01, -1.9789e-02,  3.9134e-01,  ...,  4.6945e-01,\n",
      "          -7.1506e-02,  1.5134e-01]],\n",
      "\n",
      "        [[-3.8729e-02,  3.1945e-02,  3.1511e-01,  ...,  2.9766e-01,\n",
      "          -1.7492e-01,  5.6705e-02],\n",
      "         [-8.7396e-02,  9.3784e-02,  3.8295e-01,  ...,  2.5248e-01,\n",
      "          -2.8084e-01,  1.0523e-01],\n",
      "         [-1.9602e-01,  1.3284e-01,  4.3365e-01,  ...,  3.3484e-01,\n",
      "          -1.0111e-01,  1.9853e-01],\n",
      "         ...,\n",
      "         [-2.3456e-01, -1.5634e-02,  4.9107e-01,  ...,  2.1610e-01,\n",
      "          -4.5330e-02,  6.3505e-02],\n",
      "         [-1.7798e-01, -1.1390e-01,  3.4458e-01,  ...,  7.2959e-01,\n",
      "          -9.3092e-02,  2.3549e-01],\n",
      "         [-1.8322e-01, -6.7712e-02,  2.3642e-01,  ...,  1.5661e-01,\n",
      "           2.6739e-01,  3.5149e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.2837e-02,  8.1981e-02,  4.6150e-01,  ...,  5.0838e-01,\n",
      "          -1.8155e-01,  1.7462e-01],\n",
      "         [-2.1186e-01, -6.5191e-03,  4.3082e-02,  ...,  2.6382e-01,\n",
      "          -2.1116e-02,  2.4999e-01],\n",
      "         [-1.5772e-01,  3.8305e-02,  4.2656e-01,  ...,  2.9373e-01,\n",
      "          -2.1703e-01, -1.9665e-02],\n",
      "         ...,\n",
      "         [-9.9713e-02,  6.1271e-02,  5.7865e-01,  ...,  5.8028e-01,\n",
      "          -1.3030e-01,  4.0027e-01],\n",
      "         [-2.8431e-01, -9.1061e-02,  4.1912e-01,  ...,  4.2715e-01,\n",
      "          -1.0284e-01,  3.4437e-01],\n",
      "         [-9.2434e-03,  2.4933e-02,  4.0492e-01,  ...,  7.1978e-01,\n",
      "          -2.9361e-02,  1.4084e-01]],\n",
      "\n",
      "        [[-2.1757e-01, -1.0685e-03,  4.0904e-01,  ...,  9.4597e-01,\n",
      "           2.1596e-01,  3.7103e-01],\n",
      "         [-1.8259e-01,  2.5049e-02,  4.8129e-01,  ...,  2.9890e-01,\n",
      "          -3.6702e-02,  1.3805e-01],\n",
      "         [-2.4271e-01,  2.2272e-02,  4.2430e-01,  ...,  1.1282e+00,\n",
      "           1.5763e-01,  3.6000e-01],\n",
      "         ...,\n",
      "         [-9.8793e-02,  1.0791e-01,  5.7665e-01,  ...,  3.3547e-01,\n",
      "           1.4577e-02, -2.6073e-02],\n",
      "         [-8.8983e-02, -3.5319e-02,  4.0946e-01,  ...,  9.1896e-02,\n",
      "           2.3739e-01, -1.2909e-01],\n",
      "         [-1.9799e-01,  6.8135e-02,  4.2103e-01,  ...,  3.5190e-01,\n",
      "          -4.1583e-02,  2.3235e-01]],\n",
      "\n",
      "        [[-1.8844e-01,  3.3099e-02,  3.3747e-01,  ...,  4.7949e-01,\n",
      "          -9.7942e-02, -1.1585e-01],\n",
      "         [-2.0940e-01,  8.5701e-02,  3.6525e-01,  ...,  2.8267e-01,\n",
      "           2.0441e-01,  6.3551e-02],\n",
      "         [-2.3121e-01, -2.3474e-02,  3.8086e-01,  ...,  1.7953e-01,\n",
      "           4.2285e-02,  1.0846e-01],\n",
      "         ...,\n",
      "         [-1.2415e-01, -1.7627e-01,  3.3894e-01,  ..., -3.6561e-02,\n",
      "          -1.2522e-01,  5.0141e-02],\n",
      "         [-1.4944e-01, -6.7869e-02,  1.0941e-01,  ...,  4.5649e-01,\n",
      "          -1.7488e-02,  2.7370e-01],\n",
      "         [-1.4864e-01, -1.1530e-01,  4.3817e-01,  ...,  5.7813e-01,\n",
      "          -1.2659e-01,  2.4911e-02]]]), tensor([[[ 0.0730, -0.3754,  0.7391,  ...,  0.4291,  0.5099, -0.0267],\n",
      "         [ 0.2851, -0.4773, -0.0198,  ...,  0.5198,  0.3499, -0.4636],\n",
      "         [ 0.0768, -0.1505,  0.6044,  ...,  0.1544,  0.2474, -0.2487],\n",
      "         ...,\n",
      "         [-0.0637, -0.2434,  0.4420,  ...,  0.3178,  0.2866,  0.0657],\n",
      "         [ 0.3156, -0.3128,  0.7295,  ...,  0.6103,  0.3452, -0.2428],\n",
      "         [ 0.0121, -0.1662,  0.4801,  ...,  0.3415,  0.3224, -0.5108]],\n",
      "\n",
      "        [[ 0.1949, -0.4917,  0.4972,  ...,  0.2012, -0.0181, -0.4319],\n",
      "         [ 0.1193, -0.1686,  0.6645,  ...,  0.3248,  0.3163, -0.4534],\n",
      "         [ 0.0673, -0.3221,  0.7631,  ...,  0.2616,  0.3109, -0.1358],\n",
      "         ...,\n",
      "         [ 0.1371, -0.4481,  0.6512,  ...,  0.4214,  0.3136, -0.2292],\n",
      "         [ 0.3459, -0.3516,  0.4059,  ...,  0.1353,  0.2391, -0.4858],\n",
      "         [ 0.1675, -0.3057,  0.8408,  ...,  0.1693,  0.3888, -0.2933]],\n",
      "\n",
      "        [[ 0.1736, -0.4204,  0.4361,  ...,  0.2670,  0.2595, -0.4360],\n",
      "         [ 0.1782, -0.4079,  0.4511,  ...,  0.1565,  0.1590, -0.3787],\n",
      "         [ 0.1329, -0.4358,  0.6178,  ...,  0.2370,  0.3518, -0.3120],\n",
      "         ...,\n",
      "         [ 0.1548, -0.3917,  0.6133,  ...,  0.2043,  0.3498, -0.4552],\n",
      "         [ 0.2055, -0.4647,  0.5612,  ...,  0.3849,  0.3018, -0.3433],\n",
      "         [ 0.0843, -0.1828,  0.5461,  ...,  0.0047,  0.5104, -0.1818]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2160, -0.4088,  0.1457,  ...,  0.2594,  0.2377, -0.3692],\n",
      "         [ 0.1159, -0.3680,  0.3314,  ...,  0.1951,  0.3245, -0.3499],\n",
      "         [ 0.0344, -0.4078,  0.5862,  ...,  0.2055,  0.2750, -0.4847],\n",
      "         ...,\n",
      "         [ 0.1365, -0.3709,  0.2199,  ...,  0.3176,  0.3782, -0.1776],\n",
      "         [ 0.1038, -0.4417,  0.7010,  ...,  0.2546,  0.3015,  0.1831],\n",
      "         [ 0.2247, -0.3604,  0.6230,  ...,  0.2175,  0.3459, -0.3338]],\n",
      "\n",
      "        [[ 0.1296, -0.3556,  0.6575,  ...,  0.5009,  0.1574, -0.2006],\n",
      "         [ 0.0468, -0.3575,  0.1465,  ...,  0.2193,  0.3117, -0.3751],\n",
      "         [ 0.1396, -0.4701,  0.5472,  ...,  0.3645,  0.4282, -0.2658],\n",
      "         ...,\n",
      "         [ 0.1643, -0.3805,  0.6022,  ...,  0.2796,  0.3838, -0.4392],\n",
      "         [ 0.1584, -0.4609,  0.6939,  ...,  0.1608,  0.4409, -0.4028],\n",
      "         [ 0.0457, -0.4468,  0.7762,  ...,  0.1375,  0.4140, -0.2773]],\n",
      "\n",
      "        [[ 0.0981, -0.4405,  0.5983,  ...,  0.3046,  0.2601, -0.4962],\n",
      "         [ 0.1515, -0.2592,  0.6903,  ...,  0.2949,  0.5358, -0.3125],\n",
      "         [ 0.1805, -0.4900,  0.5675,  ...,  0.3115,  0.4129, -0.3871],\n",
      "         ...,\n",
      "         [ 0.1298, -0.4634,  0.5571,  ...,  0.1704,  0.2852, -0.3617],\n",
      "         [ 0.2568, -0.4395,  0.3408,  ...,  0.3096,  0.2919,  0.2499],\n",
      "         [ 0.2304, -0.4107,  0.7310,  ...,  0.3533,  0.3237, -0.3294]]]))}\n"
     ]
    }
   ],
   "source": [
    "# model.transformer.model.add_adapter(\"acu\", \"pfeiffer\")  # Before activated, resul is the same as original mockingjay in original s3prl\n",
    "wavs = [torch.ones(160000, dtype=torch.float) for _ in range(16)]\n",
    "model.train()  # will have random effect\n",
    "# model.eval()\n",
    "with torch.no_grad():\n",
    "    print(model(wavs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'adapters': {}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n",
      "None\n",
      "{'adapters': {}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n",
      "None\n",
      "{'adapters': {}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n"
     ]
    }
   ],
   "source": [
    "from s3prl.adapters.layer import AdapterLayer, AdapterLayerBase\n",
    "from s3prl.adapters.configuration import AdapterConfig\n",
    "for (i, layer) in model.transformer.model.iter_layers():\n",
    "    for module in layer.modules():\n",
    "        if (isinstance(module, AdapterLayerBase)):\n",
    "            print(module.config.adapters.match(\n",
    "                \"acu\",\n",
    "                config_type=AdapterConfig,\n",
    "                layer_idx=i,\n",
    "                location_key=\"self\",\n",
    "            ))\n",
    "            print(module.config.adapters.to_dict())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerConfig {\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hahaha\": \"hahaha\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"transformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pre_layer_norm\": false,\n",
       "  \"share_layer\": false,\n",
       "  \"transformers_version\": \"4.17.0\"\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.config.hahaha = \"hahaha\"\n",
    "model.transformer.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerConfig {\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hahaha\": \"hahaha\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"transformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pre_layer_norm\": false,\n",
       "  \"share_layer\": false,\n",
       "  \"transformers_version\": \"4.17.0\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.encoder.layer[0].output.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerConfig {\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"transformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pre_layer_norm\": false,\n",
       "  \"share_layer\": false,\n",
       "  \"transformers_version\": \"4.17.0\"\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.base_model.encoder.layer[0].output.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 19:53:17 | INFO | s3prl.adapters.configuration | Adding adapter 'acu'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adapters': {'acu': 'pfeiffer'}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n",
      "UpstreamExpert(\n",
      "  (transformer): PretrainedTransformer(\n",
      "    (extracter): OnlinePreprocessor(\n",
      "      (_melscale): MelScale()\n",
      "      (_mfcc_trans): MFCC(\n",
      "        (amplitude_to_DB): AmplitudeToDB()\n",
      "        (MelSpectrogram): MelSpectrogram(\n",
      "          (spectrogram): Spectrogram()\n",
      "          (mel_scale): MelScale()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (model): TransformerModel(\n",
      "      (shared_parameters): ModuleDict()\n",
      "      (invertible_adapters): ModuleDict()\n",
      "      (input_representations): TransformerInputRepresentations(\n",
      "        (spec_transform): Linear(in_features=80, out_features=768, bias=True)\n",
      "        (LayerNorm): TransformerLayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (1): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (2): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (3): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (4): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (5): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (6): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (7): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (8): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (9): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (10): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (11): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (prefix_tuning): PrefixTuningPool(\n",
      "        (prefix_tunings): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.transformer.model.add_adapter(\"acu\")\n",
    "print(model.transformer.model.config.adapters.to_dict())\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59c55fd4b3978c0def058b870785763f0b3768b417004cdd166463757ca86d4c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Multimodal_Schizo_adapter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
