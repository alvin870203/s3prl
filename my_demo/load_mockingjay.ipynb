{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "import s3prl.hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/chihyuan/.cache/torch/hub/s3prl_cache/a2b432be9adba2cb59f5cf89a4cf84d5fff8ec3c9fe248ad53349694565ef8c9\n",
      "for https://www.dropbox.com/s/zwsfa6w2iy2cc68/states-500000.ckpt?dl=0\n",
      "[UpstreamExpert] - Using the default upstream expert config\n",
      "UpstreamExpert(\n",
      "  (transformer): PretrainedTransformer(\n",
      "    (extracter): OnlinePreprocessor(\n",
      "      (_melscale): MelScale()\n",
      "      (_mfcc_trans): MFCC(\n",
      "        (amplitude_to_DB): AmplitudeToDB()\n",
      "        (MelSpectrogram): MelSpectrogram(\n",
      "          (spectrogram): Spectrogram()\n",
      "          (mel_scale): MelScale()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (model): TransformerModel(\n",
      "      (shared_parameters): ModuleDict()\n",
      "      (invertible_adapters): ModuleDict()\n",
      "      (input_representations): TransformerInputRepresentations(\n",
      "        (spec_transform): Linear(in_features=80, out_features=768, bias=True)\n",
      "        (LayerNorm): TransformerLayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (1): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (2): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (3): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (4): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (5): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (6): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (7): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (8): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (9): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (10): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (11): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (prefix_tuning): PrefixTuningPool(\n",
      "        (prefix_tunings): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = getattr(hub, \"mockingjay\")()  # build the Mockingjay model with pre-trained weights\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                                                 Param #\n",
       "===============================================================================================\n",
       "UpstreamExpert                                                         --\n",
       "├─PretrainedTransformer: 1-1                                           --\n",
       "│    └─OnlinePreprocessor: 2-1                                         --\n",
       "│    │    └─MelScale: 3-1                                              --\n",
       "│    │    └─MFCC: 3-2                                                  --\n",
       "│    └─TransformerModel: 2-2                                           --\n",
       "│    │    └─ModuleDict: 3-3                                            --\n",
       "│    │    └─ModuleDict: 3-4                                            --\n",
       "│    │    └─TransformerInputRepresentations: 3-5                       63,744\n",
       "│    │    └─TransformerEncoder: 3-6                                    85,054,464\n",
       "│    │    └─PrefixTuningPool: 3-7                                      --\n",
       "===============================================================================================\n",
       "Total params: 85,118,208\n",
       "Trainable params: 85,118,208\n",
       "Non-trainable params: 0\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3prl.upstream.mockingjay.model.TransformerModel"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.transformer.model)  # the model with adpaters' functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerConfig {\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"transformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pre_layer_norm\": false,\n",
       "  \"share_layer\": false,\n",
       "  \"transformers_version\": \"4.17.0\"\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UpstreamExpert(\n",
      "  (transformer): PretrainedTransformer(\n",
      "    (extracter): OnlinePreprocessor(\n",
      "      (_melscale): MelScale()\n",
      "      (_mfcc_trans): MFCC(\n",
      "        (amplitude_to_DB): AmplitudeToDB()\n",
      "        (MelSpectrogram): MelSpectrogram(\n",
      "          (spectrogram): Spectrogram()\n",
      "          (mel_scale): MelScale()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (model): TransformerModel(\n",
      "      (shared_parameters): ModuleDict()\n",
      "      (invertible_adapters): ModuleDict()\n",
      "      (input_representations): TransformerInputRepresentations(\n",
      "        (spec_transform): Linear(in_features=80, out_features=768, bias=True)\n",
      "        (LayerNorm): TransformerLayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict(\n",
      "                  (acu): Adapter(\n",
      "                    (non_linearity): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                    (adapter_down): Sequential(\n",
      "                      (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                      (1): Activation_Function_Class(\n",
      "                        (f): SiLUActivation()\n",
      "                      )\n",
      "                    )\n",
      "                    (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  )\n",
      "                )\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (1): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict(\n",
      "                  (acu): Adapter(\n",
      "                    (non_linearity): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                    (adapter_down): Sequential(\n",
      "                      (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                      (1): Activation_Function_Class(\n",
      "                        (f): SiLUActivation()\n",
      "                      )\n",
      "                    )\n",
      "                    (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  )\n",
      "                )\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (2): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict(\n",
      "                  (acu): Adapter(\n",
      "                    (non_linearity): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                    (adapter_down): Sequential(\n",
      "                      (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                      (1): Activation_Function_Class(\n",
      "                        (f): SiLUActivation()\n",
      "                      )\n",
      "                    )\n",
      "                    (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  )\n",
      "                )\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (3): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict(\n",
      "                  (acu): Adapter(\n",
      "                    (non_linearity): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                    (adapter_down): Sequential(\n",
      "                      (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                      (1): Activation_Function_Class(\n",
      "                        (f): SiLUActivation()\n",
      "                      )\n",
      "                    )\n",
      "                    (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  )\n",
      "                )\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (4): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict(\n",
      "                  (acu): Adapter(\n",
      "                    (non_linearity): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                    (adapter_down): Sequential(\n",
      "                      (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                      (1): Activation_Function_Class(\n",
      "                        (f): SiLUActivation()\n",
      "                      )\n",
      "                    )\n",
      "                    (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  )\n",
      "                )\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (5): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict(\n",
      "                  (acu): Adapter(\n",
      "                    (non_linearity): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                    (adapter_down): Sequential(\n",
      "                      (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                      (1): Activation_Function_Class(\n",
      "                        (f): SiLUActivation()\n",
      "                      )\n",
      "                    )\n",
      "                    (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  )\n",
      "                )\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (6): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict(\n",
      "                  (acu): Adapter(\n",
      "                    (non_linearity): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                    (adapter_down): Sequential(\n",
      "                      (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                      (1): Activation_Function_Class(\n",
      "                        (f): SiLUActivation()\n",
      "                      )\n",
      "                    )\n",
      "                    (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  )\n",
      "                )\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (7): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict(\n",
      "                  (acu): Adapter(\n",
      "                    (non_linearity): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                    (adapter_down): Sequential(\n",
      "                      (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                      (1): Activation_Function_Class(\n",
      "                        (f): SiLUActivation()\n",
      "                      )\n",
      "                    )\n",
      "                    (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  )\n",
      "                )\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (8): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict(\n",
      "                  (acu): Adapter(\n",
      "                    (non_linearity): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                    (adapter_down): Sequential(\n",
      "                      (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                      (1): Activation_Function_Class(\n",
      "                        (f): SiLUActivation()\n",
      "                      )\n",
      "                    )\n",
      "                    (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  )\n",
      "                )\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (9): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict(\n",
      "                  (acu): Adapter(\n",
      "                    (non_linearity): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                    (adapter_down): Sequential(\n",
      "                      (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                      (1): Activation_Function_Class(\n",
      "                        (f): SiLUActivation()\n",
      "                      )\n",
      "                    )\n",
      "                    (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  )\n",
      "                )\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (10): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict(\n",
      "                  (acu): Adapter(\n",
      "                    (non_linearity): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                    (adapter_down): Sequential(\n",
      "                      (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                      (1): Activation_Function_Class(\n",
      "                        (f): SiLUActivation()\n",
      "                      )\n",
      "                    )\n",
      "                    (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  )\n",
      "                )\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (11): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict(\n",
      "                  (acu): Adapter(\n",
      "                    (non_linearity): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                    (adapter_down): Sequential(\n",
      "                      (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                      (1): Activation_Function_Class(\n",
      "                        (f): SiLUActivation()\n",
      "                      )\n",
      "                    )\n",
      "                    (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  )\n",
      "                )\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict(\n",
      "                (acu): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (prefix_tuning): PrefixTuningPool(\n",
      "        (prefix_tunings): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.transformer.model.add_adapter(\"acu\", \"houlsby\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                                                                     Param #\n",
       "===================================================================================================================\n",
       "UpstreamExpert                                                                             --\n",
       "├─PretrainedTransformer: 1-1                                                               --\n",
       "│    └─OnlinePreprocessor: 2-1                                                             --\n",
       "│    │    └─MelScale: 3-1                                                                  --\n",
       "│    │    └─MFCC: 3-2                                                                      --\n",
       "│    └─TransformerModel: 2-2                                                               --\n",
       "│    │    └─ModuleDict: 3-3                                                                --\n",
       "│    │    └─ModuleDict: 3-4                                                                --\n",
       "│    │    └─TransformerInputRepresentations: 3-5                                           (63,744)\n",
       "│    │    └─TransformerEncoder: 3-6                                                        86,843,520\n",
       "│    │    └─PrefixTuningPool: 3-7                                                          --\n",
       "===================================================================================================================\n",
       "Total params: 86,907,264\n",
       "Trainable params: 1,789,056\n",
       "Non-trainable params: 85,118,208\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.train_adapter(\"acu\")\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name):depth-idx)                                                                    Param #\n",
       "=============================================================================================================================\n",
       "UpstreamExpert                                                                                       --\n",
       "├─PretrainedTransformer (transformer): 1-1                                                           --\n",
       "│    └─OnlinePreprocessor (extracter): 2-1                                                           --\n",
       "│    │    └─MelScale (_melscale): 3-1                                                                --\n",
       "│    │    └─MFCC (_mfcc_trans): 3-2                                                                  --\n",
       "│    │    │    └─AmplitudeToDB (amplitude_to_DB): 4-1                                                --\n",
       "│    │    │    └─MelSpectrogram (MelSpectrogram): 4-2                                                --\n",
       "│    │    │    │    └─Spectrogram (spectrogram): 5-1                                                 --\n",
       "│    │    │    │    └─MelScale (mel_scale): 5-2                                                      --\n",
       "│    └─TransformerModel (model): 2-2                                                                 --\n",
       "│    │    └─ModuleDict (shared_parameters): 3-3                                                      --\n",
       "│    │    └─ModuleDict (invertible_adapters): 3-4                                                    --\n",
       "│    │    └─TransformerInputRepresentations (input_representations): 3-5                             --\n",
       "│    │    │    └─Linear (spec_transform): 4-3                                                        (62,208)\n",
       "│    │    │    └─TransformerLayerNorm (LayerNorm): 4-4                                               (1,536)\n",
       "│    │    │    └─Dropout (dropout): 4-5                                                              --\n",
       "│    │    └─TransformerEncoder (encoder): 3-6                                                        --\n",
       "│    │    │    └─ModuleList (layer): 4-6                                                             --\n",
       "│    │    │    │    └─TransformerLayer (0): 5-3                                                      --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-1                                     --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-1                                 --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-1                                             (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-2                                               (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-3                                             (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-4                                          --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-5                           --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-1                               --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-1                     --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-2                                  --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-6                                             (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-7                                          --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-8                           (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-9                                      --\n",
       "│    │    │    │    │    │    │    │    └─Adapter (acu): 9-2                                         --\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 10-2       --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-1                         --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (adapter_down): 10-3                       --\n",
       "│    │    │    │    │    │    │    │    │    │    └─Linear (0): 11-2                                 36,912\n",
       "│    │    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 11-3              --\n",
       "│    │    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 12-1                    --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (adapter_up): 10-4                             37,632\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-10                         --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-2                               --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-3                                                  (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-3                                           --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-4                                                  (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-5                                               --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-6                                (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-7                                           --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-11                                             --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-3             --\n",
       "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-5                              --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-4                             --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-6                                      36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-7                   --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-4                         --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-5                                   37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-8                               --\n",
       "│    │    │    │    └─TransformerLayer (1): 5-4                                                      --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-4                                     --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-9                                 --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-12                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-13                                              (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-14                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-15                                         --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-16                          --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-6                               --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-8                     --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-10                                 --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-17                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-18                                         --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-19                          (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-20                                     --\n",
       "│    │    │    │    │    │    │    │    └─Adapter (acu): 9-7                                         --\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 10-9       --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-5                         --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (adapter_down): 10-10                      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─Linear (0): 11-6                                 36,912\n",
       "│    │    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 11-7              --\n",
       "│    │    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 12-2                    --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (adapter_up): 10-11                            37,632\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-21                         --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-5                               --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-11                                                 (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-6                                           --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-12                                                 (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-13                                              --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-14                               (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-15                                          --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-22                                             --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-8             --\n",
       "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-12                             --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-9                             --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-13                                     36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-14                  --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-8                         --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-10                                  37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-16                              --\n",
       "│    │    │    │    └─TransformerLayer (2): 5-5                                                      --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-7                                     --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-17                                --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-23                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-24                                              (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-25                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-26                                         --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-27                          --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-11                              --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-15                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-18                                 --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-28                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-29                                         --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-30                          (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-31                                     --\n",
       "│    │    │    │    │    │    │    │    └─Adapter (acu): 9-12                                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 10-16      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-9                         --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (adapter_down): 10-17                      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─Linear (0): 11-10                                36,912\n",
       "│    │    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 11-11             --\n",
       "│    │    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 12-3                    --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (adapter_up): 10-18                            37,632\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-32                         --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-8                               --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-19                                                 (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-9                                           --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-20                                                 (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-21                                              --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-22                               (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-23                                          --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-33                                             --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-13            --\n",
       "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-19                             --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-14                            --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-20                                     36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-21                  --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-12                        --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-15                                  37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-24                              --\n",
       "│    │    │    │    └─TransformerLayer (3): 5-6                                                      --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-10                                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-25                                --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-34                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-35                                              (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-36                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-37                                         --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-38                          --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-16                              --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-22                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-26                                 --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-39                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-40                                         --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-41                          (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-42                                     --\n",
       "│    │    │    │    │    │    │    │    └─Adapter (acu): 9-17                                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 10-23      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-13                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (adapter_down): 10-24                      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─Linear (0): 11-14                                36,912\n",
       "│    │    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 11-15             --\n",
       "│    │    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 12-4                    --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (adapter_up): 10-25                            37,632\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-43                         --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-11                              --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-27                                                 (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-12                                          --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-28                                                 (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-29                                              --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-30                               (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-31                                          --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-44                                             --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-18            --\n",
       "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-26                             --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-19                            --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-27                                     36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-28                  --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-16                        --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-20                                  37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-32                              --\n",
       "│    │    │    │    └─TransformerLayer (4): 5-7                                                      --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-13                                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-33                                --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-45                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-46                                              (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-47                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-48                                         --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-49                          --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-21                              --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-29                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-34                                 --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-50                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-51                                         --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-52                          (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-53                                     --\n",
       "│    │    │    │    │    │    │    │    └─Adapter (acu): 9-22                                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 10-30      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-17                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (adapter_down): 10-31                      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─Linear (0): 11-18                                36,912\n",
       "│    │    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 11-19             --\n",
       "│    │    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 12-5                    --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (adapter_up): 10-32                            37,632\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-54                         --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-14                              --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-35                                                 (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-15                                          --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-36                                                 (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-37                                              --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-38                               (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-39                                          --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-55                                             --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-23            --\n",
       "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-33                             --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-24                            --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-34                                     36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-35                  --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-20                        --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-25                                  37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-40                              --\n",
       "│    │    │    │    └─TransformerLayer (5): 5-8                                                      --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-16                                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-41                                --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-56                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-57                                              (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-58                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-59                                         --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-60                          --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-26                              --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-36                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-42                                 --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-61                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-62                                         --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-63                          (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-64                                     --\n",
       "│    │    │    │    │    │    │    │    └─Adapter (acu): 9-27                                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 10-37      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-21                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (adapter_down): 10-38                      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─Linear (0): 11-22                                36,912\n",
       "│    │    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 11-23             --\n",
       "│    │    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 12-6                    --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (adapter_up): 10-39                            37,632\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-65                         --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-17                              --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-43                                                 (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-18                                          --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-44                                                 (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-45                                              --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-46                               (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-47                                          --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-66                                             --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-28            --\n",
       "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-40                             --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-29                            --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-41                                     36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-42                  --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-24                        --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-30                                  37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-48                              --\n",
       "│    │    │    │    └─TransformerLayer (6): 5-9                                                      --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-19                                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-49                                --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-67                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-68                                              (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-69                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-70                                         --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-71                          --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-31                              --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-43                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-50                                 --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-72                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-73                                         --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-74                          (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-75                                     --\n",
       "│    │    │    │    │    │    │    │    └─Adapter (acu): 9-32                                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 10-44      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-25                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (adapter_down): 10-45                      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─Linear (0): 11-26                                36,912\n",
       "│    │    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 11-27             --\n",
       "│    │    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 12-7                    --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (adapter_up): 10-46                            37,632\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-76                         --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-20                              --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-51                                                 (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-21                                          --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-52                                                 (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-53                                              --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-54                               (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-55                                          --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-77                                             --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-33            --\n",
       "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-47                             --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-34                            --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-48                                     36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-49                  --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-28                        --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-35                                  37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-56                              --\n",
       "│    │    │    │    └─TransformerLayer (7): 5-10                                                     --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-22                                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-57                                --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-78                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-79                                              (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-80                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-81                                         --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-82                          --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-36                              --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-50                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-58                                 --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-83                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-84                                         --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-85                          (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-86                                     --\n",
       "│    │    │    │    │    │    │    │    └─Adapter (acu): 9-37                                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 10-51      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-29                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (adapter_down): 10-52                      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─Linear (0): 11-30                                36,912\n",
       "│    │    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 11-31             --\n",
       "│    │    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 12-8                    --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (adapter_up): 10-53                            37,632\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-87                         --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-23                              --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-59                                                 (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-24                                          --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-60                                                 (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-61                                              --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-62                               (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-63                                          --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-88                                             --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-38            --\n",
       "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-54                             --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-39                            --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-55                                     36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-56                  --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-32                        --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-40                                  37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-64                              --\n",
       "│    │    │    │    └─TransformerLayer (8): 5-11                                                     --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-25                                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-65                                --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-89                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-90                                              (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-91                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-92                                         --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-93                          --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-41                              --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-57                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-66                                 --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-94                                            (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-95                                         --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-96                          (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-97                                     --\n",
       "│    │    │    │    │    │    │    │    └─Adapter (acu): 9-42                                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 10-58      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-33                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (adapter_down): 10-59                      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─Linear (0): 11-34                                36,912\n",
       "│    │    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 11-35             --\n",
       "│    │    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 12-9                    --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (adapter_up): 10-60                            37,632\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-98                         --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-26                              --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-67                                                 (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-27                                          --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-68                                                 (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-69                                              --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-70                               (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-71                                          --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-99                                             --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-43            --\n",
       "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-61                             --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-44                            --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-62                                     36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-63                  --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-36                        --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-45                                  37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-72                              --\n",
       "│    │    │    │    └─TransformerLayer (9): 5-12                                                     --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-28                                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-73                                --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-100                                           (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-101                                             (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-102                                           (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-103                                        --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-104                         --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-46                              --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-64                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-74                                 --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-105                                           (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-106                                        --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-107                         (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-108                                    --\n",
       "│    │    │    │    │    │    │    │    └─Adapter (acu): 9-47                                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 10-65      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-37                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (adapter_down): 10-66                      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─Linear (0): 11-38                                36,912\n",
       "│    │    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 11-39             --\n",
       "│    │    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 12-10                   --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (adapter_up): 10-67                            37,632\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-109                        --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-29                              --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-75                                                 (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-30                                          --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-76                                                 (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-77                                              --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-78                               (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-79                                          --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-110                                            --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-48            --\n",
       "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-68                             --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-49                            --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-69                                     36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-70                  --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-40                        --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-50                                  37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-80                              --\n",
       "│    │    │    │    └─TransformerLayer (10): 5-13                                                    --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-31                                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-81                                --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-111                                           (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-112                                             (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-113                                           (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-114                                        --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-115                         --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-51                              --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-71                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-82                                 --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-116                                           (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-117                                        --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-118                         (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-119                                    --\n",
       "│    │    │    │    │    │    │    │    └─Adapter (acu): 9-52                                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 10-72      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-41                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (adapter_down): 10-73                      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─Linear (0): 11-42                                36,912\n",
       "│    │    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 11-43             --\n",
       "│    │    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 12-11                   --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (adapter_up): 10-74                            37,632\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-120                        --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-32                              --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-83                                                 (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-33                                          --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-84                                                 (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-85                                              --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-86                               (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-87                                          --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-121                                            --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-53            --\n",
       "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-75                             --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-54                            --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-76                                     36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-77                  --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-44                        --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-55                                  37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-88                              --\n",
       "│    │    │    │    └─TransformerLayer (11): 5-14                                                    --\n",
       "│    │    │    │    │    └─TransformerAttention (attention): 6-34                                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfAttention (self): 7-89                                --\n",
       "│    │    │    │    │    │    │    └─Linear (query): 8-122                                           (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (key): 8-123                                             (590,592)\n",
       "│    │    │    │    │    │    │    └─Linear (value): 8-124                                           (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-125                                        --\n",
       "│    │    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 8-126                         --\n",
       "│    │    │    │    │    │    │    │    └─PrefixTuningPool (pool): 9-56                              --\n",
       "│    │    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 10-78                    --\n",
       "│    │    │    │    │    │    └─TransformerSelfOutput (output): 7-90                                 --\n",
       "│    │    │    │    │    │    │    └─Linear (dense): 8-127                                           (590,592)\n",
       "│    │    │    │    │    │    │    └─Dropout (dropout): 8-128                                        --\n",
       "│    │    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 8-129                         (1,536)\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapters): 8-130                                    --\n",
       "│    │    │    │    │    │    │    │    └─Adapter (acu): 9-57                                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 10-79      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-45                        --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (adapter_down): 10-80                      --\n",
       "│    │    │    │    │    │    │    │    │    │    └─Linear (0): 11-46                                36,912\n",
       "│    │    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 11-47             --\n",
       "│    │    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 12-12                   --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (adapter_up): 10-81                            37,632\n",
       "│    │    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 8-131                        --\n",
       "│    │    │    │    │    └─TransformerIntermediate (intermediate): 6-35                              --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-91                                                 (2,362,368)\n",
       "│    │    │    │    │    └─TransformerOutput (output): 6-36                                          --\n",
       "│    │    │    │    │    │    └─Linear (dense): 7-92                                                 (2,360,064)\n",
       "│    │    │    │    │    │    └─Dropout (dropout): 7-93                                              --\n",
       "│    │    │    │    │    │    └─TransformerLayerNorm (LayerNorm): 7-94                               (1,536)\n",
       "│    │    │    │    │    │    └─ModuleDict (adapters): 7-95                                          --\n",
       "│    │    │    │    │    │    │    └─Adapter (acu): 8-132                                            --\n",
       "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-58            --\n",
       "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-82                             --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-59                            --\n",
       "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-83                                     36,912\n",
       "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-84                  --\n",
       "│    │    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 11-48                        --\n",
       "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-60                                  37,632\n",
       "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-96                              --\n",
       "│    │    └─PrefixTuningPool (prefix_tuning): 3-7                                                    --\n",
       "│    │    │    └─ModuleDict (prefix_tunings): 4-7                                                    --\n",
       "=============================================================================================================================\n",
       "Total params: 86,907,264\n",
       "Trainable params: 1,789,056\n",
       "Non-trainable params: 85,118,208\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, depth=20, row_settings=[\"depth\", \"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Psuedo Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stack[acu]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.active_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'last_hidden_state': tensor([[[ 6.7645e-04, -3.3540e-01,  5.6363e-01,  ...,  4.5578e-01,\n",
      "           3.2929e-01, -5.7725e-01],\n",
      "         [ 3.7545e-02, -4.7331e-01,  5.7534e-01,  ...,  5.3402e-02,\n",
      "           3.6354e-01, -3.7674e-01],\n",
      "         [ 5.2452e-02, -3.6531e-01,  5.9790e-01,  ...,  3.6646e-01,\n",
      "           3.2378e-01, -4.2081e-01],\n",
      "         ...,\n",
      "         [ 7.9032e-02, -3.8343e-01,  6.1830e-01,  ...,  3.4670e-01,\n",
      "           3.7949e-01, -5.2737e-01],\n",
      "         [ 2.0494e-01, -3.9395e-01,  5.7528e-01,  ...,  1.6519e-01,\n",
      "           3.3090e-01, -5.2240e-01],\n",
      "         [ 1.6175e-01, -3.0023e-01,  7.3612e-01,  ...,  2.8633e-01,\n",
      "           3.2639e-01, -4.8198e-01]],\n",
      "\n",
      "        [[ 1.1004e-01, -2.5976e-01,  5.8541e-01,  ...,  2.2522e-01,\n",
      "           2.8089e-01, -4.5776e-01],\n",
      "         [ 9.8611e-02, -3.7397e-01,  1.3657e-01,  ...,  8.2264e-02,\n",
      "           4.2774e-01, -3.3455e-01],\n",
      "         [ 7.6987e-02, -3.9069e-01,  2.4916e-01,  ...,  1.0845e-01,\n",
      "           4.1700e-01, -3.0493e-01],\n",
      "         ...,\n",
      "         [ 9.6137e-02, -3.8884e-01,  1.5387e-01,  ...,  3.1031e-01,\n",
      "           3.2738e-01, -4.6243e-01],\n",
      "         [ 1.1692e-01, -3.6908e-01,  5.8226e-01,  ...,  2.6958e-01,\n",
      "           4.2057e-01, -6.8774e-01],\n",
      "         [ 2.2142e-01, -4.9795e-01,  5.4039e-01,  ...,  5.8898e-02,\n",
      "           3.7916e-01, -5.3899e-01]],\n",
      "\n",
      "        [[-3.0844e-02, -3.9689e-01,  5.6877e-01,  ...,  3.0332e-01,\n",
      "           3.5392e-01, -5.5262e-01],\n",
      "         [ 6.8341e-02, -4.0164e-01,  4.3760e-01,  ...,  1.7420e-01,\n",
      "           3.4622e-01, -2.6677e-01],\n",
      "         [ 1.6856e-01, -4.6879e-01,  5.3412e-01,  ...,  4.8641e-02,\n",
      "           4.4109e-01, -4.3997e-01],\n",
      "         ...,\n",
      "         [ 2.6536e-01, -2.6210e-01,  4.8522e-01,  ...,  2.1339e-01,\n",
      "           2.3391e-01, -3.3602e-01],\n",
      "         [ 2.1866e-02, -3.2937e-01,  6.6756e-01,  ...,  1.6354e-01,\n",
      "           3.2141e-01, -3.8622e-01],\n",
      "         [-1.7547e-02, -4.1615e-01,  4.0595e-01,  ...,  3.6169e-01,\n",
      "           1.7316e-01, -4.7562e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.4810e-01, -1.4595e-01,  7.6189e-01,  ...,  2.0408e-01,\n",
      "           3.6106e-01, -5.7101e-01],\n",
      "         [ 1.5660e-01, -3.3288e-01,  2.0338e-01,  ...,  2.2276e-01,\n",
      "           3.3743e-01, -5.3970e-01],\n",
      "         [ 3.1666e-01, -3.2953e-01,  7.1221e-01,  ...,  2.4971e-01,\n",
      "           5.7104e-01, -2.8621e-01],\n",
      "         ...,\n",
      "         [-3.2217e-02, -1.9284e-01,  6.6273e-01,  ...,  2.8166e-01,\n",
      "           1.3898e-01, -4.2918e-01],\n",
      "         [-9.9313e-02, -4.0209e-01,  6.5812e-01,  ...,  2.9461e-01,\n",
      "           4.1170e-01, -5.0086e-01],\n",
      "         [ 2.6032e-01, -4.1226e-01,  7.7739e-01,  ...,  3.9881e-01,\n",
      "           4.2493e-01, -3.1808e-01]],\n",
      "\n",
      "        [[ 5.1374e-02, -3.8465e-01,  5.6217e-01,  ...,  4.2719e-01,\n",
      "           3.3103e-01, -4.5804e-01],\n",
      "         [ 1.2376e-01, -3.5190e-01,  5.4413e-01,  ...,  3.4416e-01,\n",
      "           2.4928e-01, -3.2901e-01],\n",
      "         [ 1.3270e-01, -4.3133e-01,  6.2764e-01,  ...,  1.1662e-01,\n",
      "           3.2189e-01, -3.8362e-01],\n",
      "         ...,\n",
      "         [ 6.2181e-02, -4.3538e-01,  4.6834e-01,  ...,  7.0113e-02,\n",
      "           2.8255e-01, -2.7836e-01],\n",
      "         [ 4.4186e-02, -3.1861e-01,  6.0215e-01,  ...,  1.6198e-01,\n",
      "           3.9772e-01, -2.9143e-01],\n",
      "         [ 1.1378e-01, -4.2661e-01,  1.7280e-01,  ..., -1.6209e-02,\n",
      "           4.2973e-01, -3.7907e-01]],\n",
      "\n",
      "        [[-5.7628e-02, -2.1905e-01,  6.4472e-01,  ...,  2.9702e-01,\n",
      "           4.1774e-01, -4.2196e-01],\n",
      "         [ 8.3517e-02, -4.5720e-01,  5.5301e-01,  ..., -2.6334e-02,\n",
      "           4.4814e-01, -3.4193e-01],\n",
      "         [-4.8525e-02, -3.2782e-01,  8.1297e-01,  ...,  4.2120e-01,\n",
      "           4.0913e-01, -3.9038e-01],\n",
      "         ...,\n",
      "         [ 1.5526e-01, -4.0256e-01,  6.1540e-01,  ...,  3.8053e-01,\n",
      "           4.4018e-01, -3.7695e-01],\n",
      "         [ 1.2608e-01, -3.3920e-01,  1.2463e-01,  ...,  3.7013e-01,\n",
      "           3.0703e-01, -4.0446e-01],\n",
      "         [ 1.2378e-01, -3.8992e-01,  4.5465e-01,  ...,  2.4825e-01,\n",
      "           3.5429e-01, -4.5873e-01]]]), 'hidden_states': (tensor([[[ 0.1565, -0.1321,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1321,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.0000,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         ...,\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0000],\n",
      "         [ 0.1565, -0.1321,  0.1802,  ...,  2.5051, -0.0000, -0.0782]],\n",
      "\n",
      "        [[ 0.1565, -0.1321,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1321,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.0000, -0.1322,  0.1803,  ...,  0.0000, -0.5862, -0.0783],\n",
      "         ...,\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1321,  0.0000,  ...,  0.0000, -0.5862, -0.0782]],\n",
      "\n",
      "        [[ 0.1565, -0.1321,  0.1803,  ...,  2.5051, -0.0000, -0.0783],\n",
      "         [ 0.1566, -0.1321,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0000],\n",
      "         ...,\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1321,  0.1802,  ...,  2.5051, -0.5862, -0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1565, -0.1321,  0.0000,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1321,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         ...,\n",
      "         [ 0.1565, -0.0000,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1321,  0.1802,  ...,  2.5051, -0.5862, -0.0782]],\n",
      "\n",
      "        [[ 0.1565, -0.1321,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1321,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         [ 0.1566, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         ...,\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.0000,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1321,  0.1802,  ...,  2.5051, -0.0000, -0.0782]],\n",
      "\n",
      "        [[ 0.1565, -0.1321,  0.1803,  ...,  2.5051, -0.5862, -0.0000],\n",
      "         [ 0.1566, -0.1321,  0.0000,  ...,  2.5051, -0.5862, -0.0000],\n",
      "         [ 0.1566, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0783],\n",
      "         ...,\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.0000, -0.0782],\n",
      "         [ 0.1565, -0.1322,  0.1803,  ...,  2.5051, -0.5862, -0.0782],\n",
      "         [ 0.1565, -0.1321,  0.1802,  ...,  2.5051, -0.5862, -0.0782]]]), tensor([[[-2.5192e-01,  1.2820e-01, -2.4041e-03,  ...,  1.5720e+00,\n",
      "           1.6987e+00, -9.7567e-01],\n",
      "         [-3.2908e-01, -6.8539e-02, -2.9717e-02,  ...,  1.2232e+00,\n",
      "           1.2535e+00, -4.9894e-01],\n",
      "         [-2.7691e-01, -3.1290e-02,  1.8006e-02,  ...,  1.5200e+00,\n",
      "           1.5783e+00, -5.8366e-01],\n",
      "         ...,\n",
      "         [ 1.2587e-01, -3.7250e-02,  1.6682e-03,  ...,  1.2595e+00,\n",
      "           1.7354e+00, -6.2927e-01],\n",
      "         [-2.1138e-01,  1.9921e-01,  3.2841e-02,  ...,  9.7761e-01,\n",
      "           3.4419e-01, -3.0194e-01],\n",
      "         [-2.5949e-01,  2.2237e-01, -2.0955e-02,  ...,  1.4675e+00,\n",
      "           1.7720e+00, -4.7476e-01]],\n",
      "\n",
      "        [[ 1.2645e-01,  8.8486e-03,  4.1689e-03,  ...,  1.3726e+00,\n",
      "           3.1023e-01, -8.2031e-01],\n",
      "         [-3.0613e-01,  5.3912e-02, -1.4299e-02,  ...,  1.2578e+00,\n",
      "           1.6249e+00, -8.0928e-01],\n",
      "         [-3.3783e-01, -1.2290e-01,  3.7581e-02,  ...,  6.8594e-01,\n",
      "           1.6319e+00, -8.1541e-01],\n",
      "         ...,\n",
      "         [-2.7432e-01,  8.7109e-02,  4.5726e-02,  ...,  1.2590e+00,\n",
      "           1.6750e+00, -4.4073e-01],\n",
      "         [-2.3356e-01,  8.0525e-02, -1.7145e-02,  ...,  9.7779e-01,\n",
      "           1.5632e+00, -7.0983e-01],\n",
      "         [-2.7810e-01,  1.2907e-01,  1.8489e-02,  ...,  6.8083e-01,\n",
      "           1.3259e+00, -4.1846e-01]],\n",
      "\n",
      "        [[-3.1264e-01,  7.1011e-02,  3.1465e-02,  ...,  1.2043e+00,\n",
      "           1.7301e+00, -8.9696e-01],\n",
      "         [-3.3881e-01, -6.9524e-02, -2.5817e-02,  ...,  1.4023e+00,\n",
      "           1.6660e+00, -6.6999e-01],\n",
      "         [-3.1562e-01, -7.7255e-03,  2.9653e-02,  ...,  1.1488e+00,\n",
      "           1.6420e+00, -5.5963e-02],\n",
      "         ...,\n",
      "         [-2.6892e-01,  1.9775e-01,  1.1414e-02,  ...,  1.2812e+00,\n",
      "           1.8431e+00, -5.0876e-01],\n",
      "         [-3.6146e-01,  1.4976e-01, -5.2332e-02,  ...,  1.0868e+00,\n",
      "           1.4939e+00, -5.6215e-01],\n",
      "         [-2.6370e-01,  1.3759e-02, -2.2799e-02,  ...,  1.2143e+00,\n",
      "           1.4749e+00, -9.3275e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.8766e-01, -4.2095e-02,  1.5931e-02,  ...,  1.1288e+00,\n",
      "           1.6345e+00, -7.6260e-01],\n",
      "         [-2.3476e-01,  2.7279e-02,  6.0434e-03,  ...,  1.0960e+00,\n",
      "           3.1584e-01, -8.2148e-01],\n",
      "         [-2.7979e-01,  3.2140e-02, -1.4762e-02,  ...,  1.3229e+00,\n",
      "           1.3451e+00, -7.0183e-01],\n",
      "         ...,\n",
      "         [-3.1704e-01, -3.4992e-02,  8.6262e-03,  ...,  1.0830e+00,\n",
      "           1.5592e+00, -8.3765e-01],\n",
      "         [-3.2186e-01, -2.2967e-01, -2.0794e-02,  ...,  8.2037e-01,\n",
      "           1.4264e+00, -9.9273e-01],\n",
      "         [-2.8695e-01,  7.7138e-02,  4.7949e-03,  ...,  1.5649e+00,\n",
      "           2.9782e-01, -6.5166e-01]],\n",
      "\n",
      "        [[-2.7136e-01, -5.9427e-02,  5.0824e-03,  ...,  1.1947e+00,\n",
      "           1.3202e+00, -4.7901e-02],\n",
      "         [ 1.2111e-01,  1.0519e-02, -1.0668e-04,  ...,  1.0989e+00,\n",
      "           1.5173e+00, -9.4695e-01],\n",
      "         [-2.5877e-01,  5.2830e-02,  2.9755e-02,  ...,  1.1240e+00,\n",
      "           1.6779e+00, -7.6005e-02],\n",
      "         ...,\n",
      "         [-2.3971e-01, -1.1665e-02, -1.0415e-02,  ...,  1.1653e+00,\n",
      "           1.7942e+00, -6.2632e-01],\n",
      "         [-2.5365e-01,  2.1583e-02,  6.4197e-04,  ...,  6.6055e-01,\n",
      "           1.7512e+00, -7.6526e-01],\n",
      "         [-2.6587e-01,  1.7003e-03,  1.2679e-02,  ...,  6.1910e-01,\n",
      "           1.7333e+00, -6.8268e-01]],\n",
      "\n",
      "        [[-2.2455e-01,  1.6998e-01,  2.3300e-02,  ...,  1.3432e+00,\n",
      "           1.8131e+00, -5.8036e-01],\n",
      "         [-2.5324e-01, -7.6910e-02,  7.5370e-02,  ...,  1.2653e+00,\n",
      "           1.7218e+00, -7.6147e-01],\n",
      "         [-2.5834e-01,  8.2968e-02,  6.1112e-02,  ...,  1.1641e+00,\n",
      "           1.8479e+00, -5.8423e-01],\n",
      "         ...,\n",
      "         [-3.1901e-01, -7.8669e-02,  1.0803e-02,  ...,  1.2278e+00,\n",
      "           1.7710e+00, -8.8213e-01],\n",
      "         [-2.6407e-01,  1.5813e-01,  1.3764e-02,  ...,  1.1921e+00,\n",
      "           1.7285e+00, -5.6204e-01],\n",
      "         [-2.4786e-01,  1.7307e-01,  5.0648e-03,  ...,  1.3777e+00,\n",
      "           1.7913e+00, -6.0623e-01]]]), tensor([[[-0.1275,  0.3527, -0.0153,  ...,  0.9521,  0.1768, -0.0047],\n",
      "         [-0.0954, -0.0552,  0.1399,  ...,  1.0320,  0.1170,  0.2827],\n",
      "         [-0.0371,  0.2440,  0.0024,  ...,  1.0977,  0.2228,  0.3804],\n",
      "         ...,\n",
      "         [-0.0602,  0.3737, -0.0931,  ...,  0.7725,  0.0977,  0.2721],\n",
      "         [-0.1205,  0.4550, -0.2082,  ...,  0.2534,  0.0778,  0.1270],\n",
      "         [-0.1072, -0.0317, -0.1432,  ...,  0.3829,  0.1259,  0.0628]],\n",
      "\n",
      "        [[-0.1153,  0.4762,  0.1305,  ...,  0.5156, -0.0891, -0.2643],\n",
      "         [-0.1370,  0.2702, -0.0506,  ...,  0.5149,  0.2326,  0.1225],\n",
      "         [-0.1002, -0.0281, -0.1616,  ...,  0.2506,  0.1466,  0.1329],\n",
      "         ...,\n",
      "         [-0.0939,  0.4184, -0.1378,  ...,  0.4500,  0.0853,  0.2841],\n",
      "         [-0.1301, -0.0496, -0.0668,  ...,  0.5609,  0.2120,  0.1883],\n",
      "         [-0.1198, -0.0376, -0.1470,  ...,  0.1277,  0.0517,  0.3141]],\n",
      "\n",
      "        [[-0.1003, -0.0452, -0.0820,  ...,  0.4978,  0.2905,  0.0460],\n",
      "         [-0.1139,  0.4990,  0.1312,  ...,  0.6984,  0.2928,  0.1303],\n",
      "         [-0.0799,  0.4761, -0.1510,  ...,  0.5095,  0.1043,  0.3924],\n",
      "         ...,\n",
      "         [-0.0119,  0.4765, -0.1924,  ...,  0.6037,  0.1936, -0.2187],\n",
      "         [-0.1504,  0.3372,  0.0113,  ...,  0.4755,  0.1956,  0.2406],\n",
      "         [-0.0665,  0.3972, -0.1156,  ...,  0.4489,  0.0759,  0.0108]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1253,  0.4475, -0.1194,  ...,  0.5118,  0.3894,  0.1160],\n",
      "         [-0.1107,  0.4204, -0.1126,  ...,  0.3405, -0.0930, -0.2168],\n",
      "         [-0.1276,  0.5023, -0.1204,  ...,  0.4433,  0.2250,  0.1661],\n",
      "         ...,\n",
      "         [-0.1072,  0.3258, -0.0945,  ...,  0.5863,  0.1504,  0.1438],\n",
      "         [-0.0605,  0.1360,  0.1173,  ...,  0.6359,  0.3365, -0.0371],\n",
      "         [-0.1478,  0.4111, -0.0618,  ...,  0.8585, -0.0832,  0.0783]],\n",
      "\n",
      "        [[-0.1151,  0.4115, -0.0391,  ...,  0.4944,  0.1245,  0.4058],\n",
      "         [-0.1368,  0.3048, -0.1348,  ...,  0.5862,  0.0891, -0.0285],\n",
      "         [-0.0969,  0.3764, -0.1331,  ...,  0.5736,  0.2660,  0.2943],\n",
      "         ...,\n",
      "         [-0.1224,  0.2978, -0.1196,  ...,  0.4254,  0.3867,  0.1252],\n",
      "         [-0.0274,  0.3161, -0.0947,  ...,  0.2298,  0.1832,  0.1262],\n",
      "         [-0.1104,  0.4004, -0.0900,  ...,  0.2272,  0.1470,  0.0997]],\n",
      "\n",
      "        [[-0.0980,  0.4357, -0.1492,  ...,  0.5143,  0.2309,  0.0900],\n",
      "         [-0.0991,  0.3768, -0.1215,  ...,  0.6177,  0.0730,  0.0772],\n",
      "         [ 0.0217,  0.4933, -0.1847,  ...,  0.5851,  0.1393,  0.0340],\n",
      "         ...,\n",
      "         [-0.0186,  0.3258, -0.0851,  ...,  0.5568,  0.4770,  0.1572],\n",
      "         [-0.0726,  0.4094, -0.0868,  ...,  0.5745,  0.2129, -0.1918],\n",
      "         [-0.1189,  0.3572, -0.1381,  ...,  0.5573,  0.1925,  0.2315]]]), tensor([[[-1.5942e-01,  9.3770e-04,  1.0894e-01,  ...,  7.8541e-01,\n",
      "           8.3085e-01,  4.9240e-01],\n",
      "         [-1.6589e-01,  3.0738e-02,  7.2532e-02,  ...,  7.7459e-01,\n",
      "           7.9363e-01,  8.1897e-01],\n",
      "         [-1.6738e-01,  3.3672e-02,  7.1710e-02,  ...,  8.8306e-01,\n",
      "           8.9451e-01,  7.7055e-01],\n",
      "         ...,\n",
      "         [-9.2461e-02,  8.2266e-02,  4.8008e-02,  ...,  5.3261e-01,\n",
      "           8.1297e-01,  6.9417e-01],\n",
      "         [-1.3045e-01, -7.6182e-02,  1.1151e-01,  ..., -8.8135e-03,\n",
      "           1.2177e+00,  2.2495e-01],\n",
      "         [-1.0510e-01, -1.3636e-01,  4.9228e-02,  ...,  1.3995e-01,\n",
      "           1.1277e+00,  4.9986e-01]],\n",
      "\n",
      "        [[-1.0236e-01, -5.2069e-02,  1.6908e-01,  ...,  2.3185e-01,\n",
      "           3.3012e-01,  1.9640e-01],\n",
      "         [-1.5338e-01,  4.9035e-02,  1.5041e-01,  ...,  2.3267e-01,\n",
      "           1.0106e+00,  6.9175e-01],\n",
      "         [-7.1861e-02,  8.3246e-02, -1.5403e-01,  ..., -1.7287e-02,\n",
      "           1.3520e+00,  3.8952e-01],\n",
      "         ...,\n",
      "         [-5.3676e-02,  4.5007e-02,  1.0381e-01,  ...,  3.0698e-01,\n",
      "           6.8594e-01,  7.4268e-01],\n",
      "         [-7.6342e-02, -6.9412e-02,  9.3007e-02,  ...,  4.1402e-01,\n",
      "           9.3316e-01,  4.1407e-01],\n",
      "         [-7.1968e-02, -1.4760e-02,  1.1146e-01,  ...,  2.3169e-01,\n",
      "           7.1831e-01,  7.4933e-01]],\n",
      "\n",
      "        [[-1.3181e-01,  7.6699e-03,  4.2956e-02,  ...,  2.1167e-01,\n",
      "           9.5352e-01,  5.8763e-01],\n",
      "         [-9.5055e-02, -4.6654e-02,  1.0954e-01,  ...,  8.0883e-01,\n",
      "           1.0885e+00,  4.5934e-01],\n",
      "         [-9.8293e-02,  4.2376e-02,  8.5118e-02,  ...,  6.4006e-01,\n",
      "           1.0702e+00,  7.5869e-01],\n",
      "         ...,\n",
      "         [-1.5487e-01, -4.2562e-02,  1.8348e-01,  ...,  2.5181e-01,\n",
      "           1.6587e+00,  3.1993e-01],\n",
      "         [-1.8502e-01,  5.9124e-02,  8.9962e-02,  ...,  1.4881e-01,\n",
      "           1.0828e+00,  8.3863e-01],\n",
      "         [-1.5200e-01,  1.2068e-01,  1.7812e-01,  ...,  5.4609e-02,\n",
      "           1.4366e+00,  7.3678e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.1678e-01,  7.4228e-03,  7.7427e-02,  ...,  2.1856e-01,\n",
      "           1.0482e+00,  5.5019e-01],\n",
      "         [-9.9648e-02, -3.4748e-03,  7.1122e-02,  ...,  1.6312e-01,\n",
      "           7.5797e-01,  2.2888e-01],\n",
      "         [-1.1511e-01, -1.0604e-01,  2.5808e-02,  ...,  2.1883e-01,\n",
      "           1.0689e+00,  5.4218e-01],\n",
      "         ...,\n",
      "         [-1.3880e-01,  5.5683e-02,  1.7211e-01,  ...,  3.5177e-01,\n",
      "           9.7646e-01,  6.9661e-01],\n",
      "         [-1.5382e-01,  1.7684e-01,  1.5717e-01,  ...,  3.4477e-01,\n",
      "           1.1722e+00,  2.8769e-01],\n",
      "         [-1.4839e-01,  1.2159e-02,  1.1408e-01,  ...,  6.0219e-01,\n",
      "           9.4098e-01,  4.2223e-01]],\n",
      "\n",
      "        [[-1.3344e-01, -1.2085e-02,  6.6338e-04,  ...,  3.0332e-01,\n",
      "           6.4714e-01,  8.6222e-01],\n",
      "         [-9.7042e-02,  6.7125e-02,  5.2894e-02,  ...,  3.5395e-01,\n",
      "           6.1485e-01,  4.3274e-01],\n",
      "         [-1.0313e-01,  4.4531e-02,  3.2020e-02,  ...,  3.8601e-01,\n",
      "           9.4376e-01,  6.2782e-01],\n",
      "         ...,\n",
      "         [-1.8466e-01,  6.4599e-02,  1.2806e-01,  ...,  1.4341e-01,\n",
      "           1.2786e+00,  8.1666e-01],\n",
      "         [-2.2567e-01,  1.1528e-01, -1.3799e-01,  ..., -1.7824e-01,\n",
      "           1.2169e+00,  5.4905e-01],\n",
      "         [-1.9942e-01, -9.7700e-03,  1.2763e-01,  ...,  2.6652e-01,\n",
      "           1.1456e+00,  4.6913e-01]],\n",
      "\n",
      "        [[-1.3861e-01, -6.5992e-02,  3.3787e-02,  ...,  2.4010e-01,\n",
      "           9.8905e-01,  5.6601e-01],\n",
      "         [-1.0861e-01,  1.5797e-01,  1.2378e-01,  ...,  3.5782e-01,\n",
      "           8.0471e-01,  6.2725e-01],\n",
      "         [-8.5887e-02, -3.8114e-02,  3.1455e-02,  ...,  3.2067e-01,\n",
      "           2.9619e-01,  5.1883e-01],\n",
      "         ...,\n",
      "         [-1.1125e-01,  8.3848e-02,  9.4054e-02,  ...,  7.2471e-01,\n",
      "           1.1913e+00,  4.9876e-01],\n",
      "         [-1.1246e-01,  1.3508e-02,  7.1836e-02,  ...,  4.2619e-01,\n",
      "           1.0370e+00,  2.5250e-01],\n",
      "         [-1.0967e-01,  5.6496e-02,  8.6650e-02,  ...,  3.7595e-01,\n",
      "           9.8488e-01,  7.0062e-01]]]), tensor([[[ 0.3244, -0.0499, -0.0137,  ...,  0.9686,  0.5039, -0.0249],\n",
      "         [ 0.2964, -0.0367, -0.0274,  ...,  0.9422,  0.4239,  0.2597],\n",
      "         [ 0.3101, -0.0055, -0.0393,  ...,  1.2238,  0.6519,  0.1511],\n",
      "         ...,\n",
      "         [ 0.2811, -0.0359, -0.0190,  ...,  0.8205,  0.7046,  0.1628],\n",
      "         [ 0.2670, -0.0426, -0.0575,  ...,  0.3289,  1.2440, -0.2408],\n",
      "         [ 0.0110, -0.0606, -0.0460,  ...,  0.4301,  0.8620,  0.3247]],\n",
      "\n",
      "        [[ 0.2468, -0.0233, -0.0398,  ...,  0.1635,  0.4426, -0.0974],\n",
      "         [ 0.2316, -0.0304, -0.0354,  ...,  0.1575,  0.6529,  0.1675],\n",
      "         [ 0.0101,  0.0262, -0.0612,  ...,  0.3321,  1.0249, -0.1303],\n",
      "         ...,\n",
      "         [ 0.3761, -0.0782, -0.1466,  ...,  0.6827,  0.5608,  0.3306],\n",
      "         [ 0.4050, -0.0396, -0.1144,  ...,  0.7846,  0.5394, -0.1875],\n",
      "         [ 0.3719, -0.0661, -0.1146,  ...,  0.0616,  0.7044,  0.3423]],\n",
      "\n",
      "        [[ 0.3409, -0.0028, -0.0717,  ...,  0.4201,  0.7854,  0.0874],\n",
      "         [ 0.3245, -0.0574, -0.0648,  ...,  0.5991,  0.9723,  0.0563],\n",
      "         [ 0.3333, -0.0358, -0.0452,  ...,  0.4931,  0.6289,  0.1467],\n",
      "         ...,\n",
      "         [ 0.1592, -0.0059, -0.0322,  ...,  0.1386,  1.0721, -0.0576],\n",
      "         [ 0.1641, -0.0100, -0.1221,  ...,  0.2511,  0.6675,  0.2024],\n",
      "         [ 0.2230,  0.0075, -0.0152,  ...,  0.1570,  0.9736,  0.2034]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0235, -0.0289, -0.0603,  ...,  0.4984,  0.6917,  0.3763],\n",
      "         [ 0.3138, -0.0678, -0.0596,  ...,  0.5205,  0.6752,  0.1254],\n",
      "         [ 0.3231, -0.0400, -0.0698,  ...,  0.5256,  0.6010,  0.0815],\n",
      "         ...,\n",
      "         [ 0.2688, -0.0213, -0.0455,  ...,  0.5268,  0.7613,  0.2356],\n",
      "         [ 0.0015, -0.0188, -0.0279,  ...,  0.7718,  0.9377, -0.2948],\n",
      "         [ 0.2563, -0.0159, -0.0584,  ...,  0.7673,  0.6886, -0.0123]],\n",
      "\n",
      "        [[ 0.3248, -0.0032, -0.0666,  ...,  0.5863,  0.4279,  0.2570],\n",
      "         [ 0.2209, -0.0500, -0.0511,  ...,  0.5580,  0.4184, -0.0953],\n",
      "         [ 0.2955, -0.0285, -0.0686,  ...,  0.2352,  0.7025,  0.1761],\n",
      "         ...,\n",
      "         [ 0.1773,  0.0042, -0.1162,  ...,  0.3208,  0.8196,  0.4730],\n",
      "         [ 0.1941, -0.0175,  0.0560,  ...,  0.0968,  0.6458, -0.0737],\n",
      "         [ 0.2295, -0.0161, -0.0966,  ...,  0.1797,  0.6754, -0.1027]],\n",
      "\n",
      "        [[ 0.2743, -0.0160, -0.0268,  ...,  0.5908,  0.7148,  0.1206],\n",
      "         [ 0.2739, -0.0312, -0.0426,  ...,  0.5040,  0.6131,  0.0150],\n",
      "         [ 0.2783, -0.0408, -0.0549,  ...,  0.2044,  0.2670,  0.0545],\n",
      "         ...,\n",
      "         [ 0.3268, -0.0700, -0.0552,  ...,  1.0474,  1.1891, -0.0332],\n",
      "         [ 0.3512, -0.0600,  0.0864,  ...,  0.7763,  0.7962, -0.1939],\n",
      "         [ 0.3518, -0.0650, -0.0595,  ...,  0.2986,  0.5541,  0.2339]]]), tensor([[[ 2.3944e-01, -4.3506e-02, -1.2897e-01,  ...,  7.5411e-01,\n",
      "           1.4190e-01,  5.0713e-01],\n",
      "         [ 2.5947e-01,  7.0707e-03,  4.3180e-02,  ...,  7.0660e-01,\n",
      "          -9.8990e-03,  8.2593e-01],\n",
      "         [ 2.0851e-01, -6.0876e-02, -1.4667e-01,  ...,  8.9115e-01,\n",
      "           2.0739e-01,  5.6589e-01],\n",
      "         ...,\n",
      "         [ 1.7948e-01, -1.0437e-01, -1.1554e-01,  ...,  5.2395e-01,\n",
      "           2.4701e-01,  6.0041e-01],\n",
      "         [ 1.6499e-01, -1.4422e-02, -8.0523e-02,  ..., -1.0014e-01,\n",
      "           6.4978e-01,  2.1095e-01],\n",
      "         [ 1.1732e-01, -3.2102e-02, -8.8899e-02,  ...,  5.9798e-02,\n",
      "           4.0763e-01,  2.2561e-01]],\n",
      "\n",
      "        [[ 1.7823e-01, -4.4808e-02, -1.1140e-01,  ..., -1.7762e-01,\n",
      "          -2.6064e-02,  4.6937e-01],\n",
      "         [ 1.4245e-01, -7.6572e-02, -7.8839e-02,  ..., -2.7490e-01,\n",
      "           1.6090e-01,  5.3534e-01],\n",
      "         [ 4.9919e-02, -4.4323e-02, -1.0186e-01,  ..., -1.5139e-01,\n",
      "           7.3315e-01,  2.8450e-01],\n",
      "         ...,\n",
      "         [ 2.4172e-01,  6.4323e-05, -1.6708e-01,  ...,  4.4977e-01,\n",
      "           1.9822e-01,  2.9167e-01],\n",
      "         [ 2.3822e-01, -2.3518e-02, -1.6278e-01,  ...,  5.0583e-01,\n",
      "           2.8092e-01,  2.6520e-01],\n",
      "         [ 2.3242e-01, -5.5735e-02, -1.3906e-01,  ..., -4.1918e-01,\n",
      "           3.7738e-01,  3.3476e-01]],\n",
      "\n",
      "        [[ 2.8710e-01, -3.4052e-02,  2.6535e-02,  ...,  9.5677e-02,\n",
      "           3.6368e-01,  4.8743e-04],\n",
      "         [ 2.4671e-01, -3.1490e-02, -1.1396e-01,  ...,  3.2458e-01,\n",
      "           5.4638e-01,  6.6063e-01],\n",
      "         [ 2.8325e-01, -1.6709e-02, -1.1073e-01,  ...,  2.1607e-01,\n",
      "           2.7327e-01,  9.6475e-01],\n",
      "         ...,\n",
      "         [ 7.6253e-02, -3.2376e-02, -6.2872e-02,  ...,  1.1102e-01,\n",
      "           6.3240e-01,  3.2468e-01],\n",
      "         [ 1.2359e-01, -8.7290e-02,  1.9459e-02,  ..., -1.4499e-01,\n",
      "           8.0199e-02,  5.8872e-01],\n",
      "         [ 1.4965e-01, -4.4041e-02, -6.6943e-02,  ..., -1.4819e-01,\n",
      "           5.0086e-01,  6.9293e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.1033e-01,  6.6455e-03, -1.6904e-01,  ...,  5.3732e-01,\n",
      "           3.8334e-01,  1.0129e+00],\n",
      "         [ 2.7414e-01,  6.8124e-03, -1.2032e-01,  ...,  1.1417e-01,\n",
      "           2.4658e-01,  7.7696e-01],\n",
      "         [ 2.8161e-01, -6.8714e-02, -1.2119e-01,  ...,  8.5875e-02,\n",
      "           7.8967e-01,  7.2364e-01],\n",
      "         ...,\n",
      "         [ 2.0513e-01,  1.1470e-02, -9.4436e-02,  ...,  1.8344e-01,\n",
      "           2.7546e-01,  7.7237e-01],\n",
      "         [ 1.3445e-01, -3.7779e-02, -2.3522e-01,  ...,  5.6170e-01,\n",
      "           5.7567e-01,  1.7541e-01],\n",
      "         [ 2.0896e-01, -5.0838e-02, -1.3247e-01,  ...,  4.5146e-01,\n",
      "           2.7021e-01,  4.7205e-01]],\n",
      "\n",
      "        [[ 2.3118e-01, -3.0810e-02, -1.2975e-01,  ...,  3.3086e-01,\n",
      "           1.3019e-01,  8.1449e-01],\n",
      "         [ 1.4877e-01, -5.3182e-02, -2.2508e-01,  ...,  1.9173e-01,\n",
      "           4.7486e-03,  4.1058e-01],\n",
      "         [ 2.2180e-01,  1.2398e-02, -1.0461e-01,  ..., -1.3218e-01,\n",
      "           4.3640e-01,  8.8465e-02],\n",
      "         ...,\n",
      "         [ 1.4167e-01, -3.2342e-02, -1.0529e-01,  ...,  8.8665e-03,\n",
      "           3.8536e-01,  1.0512e+00],\n",
      "         [ 1.6503e-01, -2.6887e-02, -7.3793e-02,  ...,  2.7120e-02,\n",
      "           6.1786e-01,  4.1525e-01],\n",
      "         [ 1.6689e-01, -7.2395e-02, -8.8296e-02,  ..., -4.6595e-02,\n",
      "           2.3824e-01, -3.0307e-01]],\n",
      "\n",
      "        [[ 2.1314e-01, -5.7702e-02, -1.9872e-01,  ...,  3.1352e-01,\n",
      "           2.4822e-01,  8.3121e-01],\n",
      "         [ 2.3995e-01,  3.9236e-04, -9.0212e-02,  ...,  4.7646e-01,\n",
      "           2.0507e-01,  6.2542e-01],\n",
      "         [ 2.0256e-01, -4.5838e-02,  3.1488e-02,  ..., -1.4088e-01,\n",
      "           2.6211e-01,  7.6797e-01],\n",
      "         ...,\n",
      "         [ 2.2969e-01,  1.3724e-02, -1.2504e-01,  ...,  8.2653e-01,\n",
      "           7.8448e-01,  4.2410e-01],\n",
      "         [ 2.2056e-01, -2.0357e-02, -1.6654e-01,  ...,  4.7376e-01,\n",
      "           4.0864e-01,  2.4211e-01],\n",
      "         [ 1.7581e-01, -4.9878e-02, -1.2808e-01,  ..., -9.0018e-02,\n",
      "           1.2457e-01,  5.6259e-01]]]), tensor([[[-0.0527, -0.0966,  0.0529,  ...,  0.2439,  0.4800,  0.6154],\n",
      "         [ 0.0223, -0.0167,  0.1296,  ...,  0.2225,  0.3896,  0.8625],\n",
      "         [-0.1057, -0.0783,  0.0623,  ...,  0.4817,  0.5371,  0.6215],\n",
      "         ...,\n",
      "         [-0.0400, -0.0197,  0.0557,  ...,  0.4717,  0.4943,  0.7077],\n",
      "         [-0.0695, -0.0830,  0.0605,  ..., -0.1711,  0.6792,  0.4401],\n",
      "         [-0.0501, -0.0410,  0.0724,  ..., -0.2318,  0.3475,  0.4446]],\n",
      "\n",
      "        [[-0.0622, -0.0667, -0.0085,  ..., -0.2563,  0.1604,  0.4603],\n",
      "         [-0.0368, -0.0763,  0.0198,  ..., -0.4368,  0.2009,  0.6694],\n",
      "         [-0.1091,  0.0371,  0.0143,  ..., -0.3594,  0.7335,  0.4031],\n",
      "         ...,\n",
      "         [-0.0867, -0.0283,  0.0601,  ...,  0.1019,  0.5517,  0.5540],\n",
      "         [ 0.0122, -0.1289,  0.0656,  ...,  0.1872,  0.6516,  0.3193],\n",
      "         [-0.0740, -0.1021,  0.0278,  ..., -0.4633,  0.6400,  0.4797]],\n",
      "\n",
      "        [[-0.0692, -0.0182,  0.0820,  ..., -0.0852,  0.5385,  0.2253],\n",
      "         [ 0.0361,  0.0067,  0.0537,  ...,  0.0799,  0.4777,  0.7374],\n",
      "         [ 0.0406, -0.1371, -0.0617,  ..., -0.0297,  0.3257,  1.0097],\n",
      "         ...,\n",
      "         [-0.0231, -0.0474,  0.0027,  ..., -0.1196,  0.6990,  0.3630],\n",
      "         [-0.0714, -0.1172,  0.0808,  ..., -0.1725,  0.4023,  0.5314],\n",
      "         [-0.0450, -0.0908, -0.0139,  ..., -0.1925,  0.4967,  0.5514]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0669, -0.0771,  0.0610,  ...,  0.2198,  0.4665,  0.8487],\n",
      "         [ 0.0057, -0.0789,  0.0819,  ..., -0.0788,  0.5123,  0.8124],\n",
      "         [-0.0577, -0.1057,  0.1251,  ..., -0.1227,  0.8574,  0.5747],\n",
      "         ...,\n",
      "         [-0.0844, -0.0820,  0.0345,  ...,  0.0436,  0.4683,  0.6452],\n",
      "         [-0.1176, -0.1128, -0.0255,  ...,  0.2707,  0.4752,  0.2366],\n",
      "         [-0.0228, -0.0879,  0.0364,  ...,  0.2498,  0.5194,  0.3612]],\n",
      "\n",
      "        [[-0.0055, -0.0426,  0.0257,  ...,  0.4109,  0.0879,  0.7853],\n",
      "         [-0.0640, -0.0547,  0.0531,  ..., -0.0610,  0.4044,  0.2549],\n",
      "         [-0.0251, -0.0472,  0.0567,  ..., -0.4038,  0.4681,  0.4189],\n",
      "         ...,\n",
      "         [-0.0964, -0.0026,  0.0097,  ..., -0.0186,  0.5235,  0.8430],\n",
      "         [-0.0213, -0.0013, -0.0044,  ..., -0.0507,  0.6671,  0.4424],\n",
      "         [-0.1256, -0.0305,  0.0835,  ..., -0.2351,  0.3732, -0.0644]],\n",
      "\n",
      "        [[-0.0418,  0.0051,  0.0574,  ...,  0.0908,  0.3525,  0.7834],\n",
      "         [-0.0885,  0.0204,  0.1080,  ...,  0.1594,  0.5429,  0.7804],\n",
      "         [-0.0911, -0.0256, -0.0437,  ...,  0.0717,  0.4476,  0.8002],\n",
      "         ...,\n",
      "         [-0.0089, -0.0340,  0.0779,  ...,  0.4338,  0.7843,  0.5658],\n",
      "         [-0.0486, -0.0435,  0.0124,  ...,  0.1670,  0.5296,  0.5001],\n",
      "         [-0.0396, -0.0501,  0.0311,  ...,  0.0529,  0.2206,  0.6062]]]), tensor([[[-0.0841, -0.3183, -0.2091,  ...,  0.4923,  0.2537,  0.6848],\n",
      "         [ 0.0431, -0.1864, -0.1348,  ...,  0.2351,  0.1813,  1.1017],\n",
      "         [-0.0573, -0.1782, -0.2074,  ...,  0.7012,  0.3137,  0.9103],\n",
      "         ...,\n",
      "         [-0.0030, -0.2380, -0.1884,  ...,  0.5915,  0.1885,  1.0273],\n",
      "         [-0.0185, -0.4034, -0.2324,  ...,  0.0712,  0.3832,  0.4511],\n",
      "         [ 0.0547, -0.3726, -0.1840,  ..., -0.2713,  0.1191,  0.5197]],\n",
      "\n",
      "        [[-0.0213, -0.3575, -0.1500,  ..., -0.1656, -0.2447,  0.7500],\n",
      "         [-0.0426, -0.3445, -0.2815,  ..., -0.3125, -0.1045,  1.0468],\n",
      "         [-0.0918, -0.2012, -0.1352,  ..., -0.1676,  0.5646,  0.4522],\n",
      "         ...,\n",
      "         [-0.0376, -0.2910, -0.2324,  ...,  0.2323,  0.2435,  0.7057],\n",
      "         [-0.0018, -0.2995, -0.2103,  ...,  0.4293,  0.3982,  0.2709],\n",
      "         [-0.0165, -0.4051, -0.1862,  ..., -0.5453,  0.3152,  0.4798]],\n",
      "\n",
      "        [[-0.0677, -0.3342, -0.0636,  ...,  0.1600,  0.0564,  0.4864],\n",
      "         [ 0.0075, -0.2875, -0.1704,  ...,  0.1084,  0.1899,  0.9986],\n",
      "         [ 0.0966, -0.3469, -0.2482,  ...,  0.1515,  0.1995,  1.0759],\n",
      "         ...,\n",
      "         [-0.0432, -0.3390, -0.1408,  ..., -0.1228,  0.3705,  0.5296],\n",
      "         [-0.1055, -0.1894, -0.1403,  ..., -0.0783,  0.1894,  0.8163],\n",
      "         [-0.0414, -0.2908, -0.1981,  ...,  0.0252,  0.1748,  0.7183]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0399, -0.1833, -0.2064,  ...,  0.3887,  0.0948,  1.0796],\n",
      "         [ 0.0456, -0.3275, -0.0656,  ...,  0.1458,  0.1727,  0.9569],\n",
      "         [-0.1126, -0.1696, -0.2021,  ...,  0.0709,  0.5303,  0.7628],\n",
      "         ...,\n",
      "         [-0.0700, -0.2523, -0.2011,  ...,  0.2016,  0.1672,  0.9594],\n",
      "         [-0.1315, -0.2375, -0.1522,  ...,  0.4147,  0.4529,  0.4883],\n",
      "         [-0.0377, -0.2861, -0.1615,  ...,  0.2529,  0.2344,  0.7534]],\n",
      "\n",
      "        [[ 0.0484, -0.2470, -0.2713,  ...,  0.6470, -0.0298,  0.9252],\n",
      "         [ 0.0260, -0.1971, -0.2321,  ...,  0.1859,  0.1488,  0.4714],\n",
      "         [ 0.0315, -0.3734, -0.2351,  ..., -0.2509,  0.2671,  0.6117],\n",
      "         ...,\n",
      "         [-0.1441, -0.1281, -0.2395,  ...,  0.1727,  0.2609,  1.0423],\n",
      "         [-0.1030, -0.2097, -0.1562,  ...,  0.0740,  0.4959,  0.6649],\n",
      "         [-0.0951, -0.2389, -0.2316,  ..., -0.1294,  0.1404,  0.1545]],\n",
      "\n",
      "        [[-0.0260, -0.3015, -0.2494,  ...,  0.3064,  0.2001,  0.9032],\n",
      "         [-0.0340, -0.2378, -0.1764,  ...,  0.2805,  0.2644,  0.9424],\n",
      "         [-0.0180, -0.2898, -0.2918,  ...,  0.3148,  0.2936,  0.9699],\n",
      "         ...,\n",
      "         [-0.1124, -0.0116, -0.3416,  ...,  0.6986,  0.6137,  0.8490],\n",
      "         [-0.1248, -0.0206, -0.2609,  ...,  0.1058,  0.3501,  0.7566],\n",
      "         [-0.0486, -0.2293, -0.2545,  ...,  0.2141,  0.0201,  0.7716]]]), tensor([[[ 0.0676,  0.0793,  0.0778,  ...,  0.4695,  0.5459,  0.4314],\n",
      "         [ 0.0074,  0.1310, -0.0205,  ...,  0.1751,  0.5183,  0.6621],\n",
      "         [ 0.0388,  0.1119,  0.0740,  ...,  0.5932,  0.5424,  0.4882],\n",
      "         ...,\n",
      "         [ 0.1254,  0.0764,  0.0669,  ...,  0.5518,  0.4543,  0.6917],\n",
      "         [ 0.0785,  0.0329, -0.0436,  ...,  0.1628,  0.6222,  0.0804],\n",
      "         [ 0.1643, -0.0197,  0.0566,  ..., -0.1206,  0.3217,  0.1977]],\n",
      "\n",
      "        [[ 0.0828,  0.0194,  0.0548,  ..., -0.0579, -0.0073,  0.3724],\n",
      "         [ 0.0825,  0.0326,  0.0279,  ..., -0.1893,  0.2435,  0.8717],\n",
      "         [-0.0222,  0.0354,  0.0572,  ..., -0.0390,  0.7435,  0.2488],\n",
      "         ...,\n",
      "         [ 0.1216,  0.0947,  0.0428,  ...,  0.1962,  0.4388,  0.3633],\n",
      "         [ 0.0580,  0.1203,  0.0255,  ...,  0.2917,  0.7175,  0.0634],\n",
      "         [ 0.1168, -0.0541,  0.0377,  ..., -0.3974,  0.6095,  0.2727]],\n",
      "\n",
      "        [[ 0.0083,  0.0739,  0.0994,  ...,  0.2218,  0.4040,  0.1488],\n",
      "         [ 0.0820,  0.0190,  0.0347,  ...,  0.1616,  0.4882,  0.8181],\n",
      "         [ 0.0887,  0.0468, -0.0017,  ...,  0.0820,  0.3983,  0.6613],\n",
      "         ...,\n",
      "         [-0.0030,  0.0394,  0.0670,  ..., -0.0367,  0.4376,  0.2534],\n",
      "         [ 0.0389,  0.0454,  0.0701,  ..., -0.0614,  0.4110,  0.5485],\n",
      "         [-0.0060,  0.0826,  0.0626,  ...,  0.0917,  0.2489,  0.4968]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1319,  0.0476,  0.0416,  ...,  0.3909,  0.3271,  0.5497],\n",
      "         [ 0.1698,  0.0334,  0.1169,  ...,  0.1905,  0.5106,  0.4703],\n",
      "         [ 0.0437, -0.0113,  0.0268,  ...,  0.2006,  0.7672,  0.3210],\n",
      "         ...,\n",
      "         [ 0.0148,  0.0971,  0.0212,  ...,  0.2015,  0.4565,  0.5915],\n",
      "         [-0.0825,  0.1690, -0.0170,  ...,  0.3182,  0.6614,  0.1533],\n",
      "         [ 0.0458,  0.0718,  0.0756,  ...,  0.2209,  0.4669,  0.4389]],\n",
      "\n",
      "        [[ 0.1336,  0.0277, -0.0290,  ...,  0.6358,  0.3893,  0.6242],\n",
      "         [ 0.0111,  0.1295,  0.0646,  ...,  0.1804,  0.5010,  0.2319],\n",
      "         [ 0.0549,  0.0201,  0.0365,  ..., -0.1802,  0.3505,  0.3018],\n",
      "         ...,\n",
      "         [ 0.0452, -0.0464,  0.0406,  ...,  0.1813,  0.5044,  0.8281],\n",
      "         [ 0.0379,  0.1059, -0.0529,  ...,  0.0359,  0.5130,  0.3422],\n",
      "         [-0.0176,  0.0535,  0.0139,  ..., -0.0522,  0.4489,  0.0368]],\n",
      "\n",
      "        [[ 0.1049,  0.0765,  0.0278,  ...,  0.3120,  0.4877,  0.5829],\n",
      "         [ 0.0608,  0.0642,  0.0546,  ...,  0.2593,  0.5391,  0.5799],\n",
      "         [ 0.1206,  0.0495,  0.0800,  ...,  0.3482,  0.5815,  0.5580],\n",
      "         ...,\n",
      "         [ 0.0541,  0.1009,  0.0370,  ...,  0.7283,  0.8255,  0.5118],\n",
      "         [ 0.0229,  0.1086,  0.0663,  ...,  0.1828,  0.6046,  0.5930],\n",
      "         [ 0.0830,  0.0545,  0.0918,  ...,  0.2498,  0.4970,  0.3974]]]), tensor([[[ 0.2587,  0.5643, -0.2093,  ...,  0.6435,  0.4758,  0.5014],\n",
      "         [ 0.2173,  0.5518, -0.2897,  ..., -0.0068,  0.5200,  0.8888],\n",
      "         [ 0.3223,  0.5608, -0.0607,  ...,  0.7852,  0.4070,  0.4755],\n",
      "         ...,\n",
      "         [ 0.2285,  0.5810, -0.1054,  ...,  0.7084,  0.4049,  0.8798],\n",
      "         [ 0.2837,  0.4580, -0.2265,  ...,  0.3948,  0.2837,  0.1167],\n",
      "         [ 0.3157,  0.4915, -0.2416,  ...,  0.0814,  0.0652,  0.2704]],\n",
      "\n",
      "        [[ 0.3396,  0.3620, -0.2065,  ..., -0.1022, -0.2763,  0.7019],\n",
      "         [ 0.3040,  0.2807, -0.4723,  ..., -0.4919,  0.0060,  1.0127],\n",
      "         [ 0.2914,  0.6010, -0.0660,  ..., -0.0026,  0.4039,  0.1756],\n",
      "         ...,\n",
      "         [ 0.3995,  0.3478, -0.0566,  ...,  0.2477,  0.2990,  0.2852],\n",
      "         [ 0.3227,  0.6417, -0.2021,  ...,  0.5914,  0.5976, -0.1844],\n",
      "         [ 0.3036,  0.2633, -0.1737,  ..., -0.4822,  0.3689,  0.2634]],\n",
      "\n",
      "        [[ 0.2207,  0.4948, -0.1615,  ...,  0.0902,  0.2523,  0.1378],\n",
      "         [ 0.2889,  0.4863, -0.2122,  ...,  0.1091,  0.3569,  1.0186],\n",
      "         [ 0.3531,  0.5331, -0.1109,  ...,  0.0061,  0.3181,  0.6542],\n",
      "         ...,\n",
      "         [ 0.1816,  0.4714, -0.2169,  ...,  0.0286,  0.0382,  0.4748],\n",
      "         [ 0.3242,  0.3684, -0.1486,  ..., -0.1353,  0.1994,  0.6958],\n",
      "         [ 0.2131,  0.5125, -0.0499,  ...,  0.4242,  0.0614,  0.6246]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0355,  0.5627, -0.2679,  ...,  0.6609,  0.0400,  0.6670],\n",
      "         [ 0.0697,  0.4505, -0.0951,  ...,  0.2404,  0.2274,  0.5342],\n",
      "         [ 0.1892,  0.3414, -0.0180,  ...,  0.2112,  0.6751,  0.5971],\n",
      "         ...,\n",
      "         [ 0.2893,  0.4843, -0.2154,  ...,  0.2647,  0.2970,  0.7336],\n",
      "         [-0.0152,  0.3809, -0.3756,  ...,  0.3350,  0.3901,  0.2405],\n",
      "         [ 0.1668,  0.3677, -0.0913,  ...,  0.2347,  0.2596,  0.7795]],\n",
      "\n",
      "        [[ 0.2425,  0.2112, -0.2457,  ...,  0.6327,  0.1367,  0.8737],\n",
      "         [ 0.3581,  0.5312, -0.2634,  ...,  0.1604,  0.3447,  0.1687],\n",
      "         [ 0.2686,  0.4182, -0.1746,  ..., -0.2620,  0.1484,  0.3652],\n",
      "         ...,\n",
      "         [ 0.3609,  0.4013, -0.3400,  ...,  0.2732,  0.4158,  0.9983],\n",
      "         [ 0.2075,  0.3300, -0.0709,  ...,  0.0158,  0.3954,  0.5319],\n",
      "         [ 0.1525,  0.4654, -0.2834,  ..., -0.1199,  0.2165,  0.2289]],\n",
      "\n",
      "        [[ 0.2184,  0.0492, -0.2510,  ...,  0.5177,  0.5156,  0.6236],\n",
      "         [ 0.3782,  0.5722, -0.0650,  ...,  0.2492,  0.5860,  0.6495],\n",
      "         [ 0.2454,  0.4834,  0.0365,  ...,  0.5039,  0.2711,  0.6059],\n",
      "         ...,\n",
      "         [ 0.1652,  0.4024, -0.2480,  ...,  0.9807,  0.6576,  0.5927],\n",
      "         [ 0.1449,  0.5552, -0.2856,  ...,  0.2670,  0.2673,  0.5181],\n",
      "         [ 0.2861,  0.6596, -0.2150,  ...,  0.2931,  0.3021,  0.1648]]]), tensor([[[ 0.0638, -0.2040,  0.1359,  ...,  0.5784,  0.4952,  0.5826],\n",
      "         [-0.0037, -0.0827,  0.0769,  ..., -0.0669,  0.4679,  0.9217],\n",
      "         [ 0.0621, -0.0884,  0.0978,  ...,  0.7189,  0.3292,  0.5500],\n",
      "         ...,\n",
      "         [-0.0108, -0.1004,  0.0093,  ...,  0.6706,  0.4192,  0.9188],\n",
      "         [ 0.0294, -0.1422,  0.0754,  ...,  0.3108,  0.2824,  0.3464],\n",
      "         [-0.0122, -0.0981,  0.1190,  ..., -0.0090,  0.2443,  0.4481]],\n",
      "\n",
      "        [[-0.0404, -0.1999, -0.0086,  ..., -0.0900,  0.1117,  0.6855],\n",
      "         [ 0.0522, -0.1760,  0.0476,  ..., -0.5457,  0.1527,  0.9643],\n",
      "         [ 0.1203, -0.0265,  0.0069,  ..., -0.1462,  0.3506,  0.4005],\n",
      "         ...,\n",
      "         [-0.0172, -0.2562,  0.0825,  ...,  0.3016,  0.2200,  0.4207],\n",
      "         [ 0.1282, -0.1259,  0.0357,  ...,  0.5610,  0.4337,  0.0095],\n",
      "         [ 0.1270,  0.0289,  0.0099,  ..., -0.5876,  0.3124,  0.3265]],\n",
      "\n",
      "        [[ 0.0856, -0.1387,  0.0242,  ..., -0.0223,  0.2919,  0.2372],\n",
      "         [-0.0772, -0.1407,  0.0734,  ...,  0.0161,  0.4333,  0.9377],\n",
      "         [ 0.3000, -0.1697,  0.0654,  ..., -0.1342,  0.4008,  0.6853],\n",
      "         ...,\n",
      "         [-0.0590, -0.0508,  0.0906,  ..., -0.0596,  0.3182,  0.6456],\n",
      "         [-0.0926, -0.1112,  0.0986,  ..., -0.1259,  0.3424,  0.6058],\n",
      "         [ 0.0162, -0.1698,  0.1661,  ...,  0.3790,  0.0877,  0.7337]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1357, -0.1005,  0.0346,  ...,  0.5214,  0.1838,  0.6575],\n",
      "         [-0.0236, -0.1223,  0.1690,  ...,  0.1340,  0.2299,  0.5401],\n",
      "         [ 0.0650, -0.1365,  0.1216,  ...,  0.1295,  0.6971,  0.5813],\n",
      "         ...,\n",
      "         [ 0.0090, -0.0486, -0.0138,  ...,  0.1852,  0.3430,  0.7091],\n",
      "         [-0.0845,  0.1247,  0.0454,  ...,  0.2520,  0.3053,  0.4753],\n",
      "         [-0.0223, -0.2603,  0.1396,  ...,  0.3194,  0.4163,  0.7057]],\n",
      "\n",
      "        [[ 0.0096, -0.2677,  0.0399,  ...,  0.6367,  0.2362,  0.8849],\n",
      "         [-0.0348, -0.1089, -0.0891,  ...,  0.1498,  0.1283,  0.3700],\n",
      "         [-0.0070, -0.1175, -0.0483,  ..., -0.3516,  0.1666,  0.4114],\n",
      "         ...,\n",
      "         [ 0.0336, -0.1167,  0.1445,  ...,  0.1887,  0.4724,  1.0941],\n",
      "         [-0.0038,  0.0125,  0.1277,  ..., -0.1129,  0.4937,  0.6994],\n",
      "         [ 0.0218, -0.1068, -0.0292,  ..., -0.2140,  0.3134,  0.5229]],\n",
      "\n",
      "        [[ 0.0290, -0.2187,  0.0571,  ...,  0.4430,  0.5051,  0.5720],\n",
      "         [ 0.0397, -0.0657,  0.0894,  ...,  0.1539,  0.4895,  0.7306],\n",
      "         [-0.0052, -0.1036,  0.1350,  ...,  0.6011,  0.2777,  0.7092],\n",
      "         ...,\n",
      "         [ 0.0639, -0.1344,  0.0118,  ...,  0.9833,  0.5380,  0.7765],\n",
      "         [ 0.0622, -0.1064,  0.0268,  ...,  0.2913,  0.1971,  0.6505],\n",
      "         [-0.0822, -0.1494,  0.0123,  ...,  0.1917,  0.3529,  0.2749]]]), tensor([[[-0.2608,  0.0042,  0.5866,  ...,  0.7868, -0.0371,  0.0254],\n",
      "         [-0.0775,  0.0322,  0.4665,  ...,  0.3401,  0.0270,  0.2854],\n",
      "         [-0.1650, -0.0596,  0.4972,  ...,  0.6074, -0.0944,  0.1932],\n",
      "         ...,\n",
      "         [-0.1154, -0.0580,  0.5149,  ...,  0.5838,  0.0107,  0.1769],\n",
      "         [-0.1593, -0.0171,  0.4897,  ...,  0.2034, -0.1537, -0.0812],\n",
      "         [-0.0214,  0.1120,  0.6245,  ...,  0.2812, -0.2088, -0.1252]],\n",
      "\n",
      "        [[-0.1816,  0.1512,  0.4294,  ...,  0.2940, -0.2835,  0.0844],\n",
      "         [-0.2394, -0.0626,  0.4399,  ..., -0.0980,  0.1470,  0.4391],\n",
      "         [-0.1977,  0.0338,  0.0942,  ...,  0.0133,  0.2406,  0.2371],\n",
      "         ...,\n",
      "         [-0.2688, -0.0265,  0.6010,  ...,  0.3954, -0.1612,  0.0673],\n",
      "         [-0.1258, -0.0593,  0.3397,  ...,  0.8788,  0.0469, -0.3844],\n",
      "         [-0.0429,  0.0802,  0.5302,  ..., -0.2096, -0.1274, -0.1097]],\n",
      "\n",
      "        [[-0.1926, -0.0974,  0.4696,  ...,  0.3923, -0.1340, -0.2190],\n",
      "         [-0.1609, -0.0170,  0.1090,  ...,  0.1157,  0.0812,  0.2282],\n",
      "         [-0.0957, -0.0400,  0.4488,  ...,  0.3227,  0.0987,  0.1250],\n",
      "         ...,\n",
      "         [-0.1186,  0.0601,  0.6919,  ...,  0.2315, -0.2169,  0.2056],\n",
      "         [-0.1866,  0.0028,  0.5852,  ...,  0.1143,  0.0214,  0.1984],\n",
      "         [-0.1919, -0.0944,  0.4690,  ...,  0.6524, -0.2211,  0.1163]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1857,  0.0273,  0.4826,  ...,  0.6900, -0.0900,  0.0218],\n",
      "         [-0.1563,  0.0952,  0.6354,  ...,  0.3526, -0.0498,  0.0114],\n",
      "         [-0.0700,  0.0849,  0.5621,  ...,  0.3337,  0.4713,  0.1854],\n",
      "         ...,\n",
      "         [-0.2582, -0.0390,  0.4856,  ...,  0.4240,  0.2308,  0.2009],\n",
      "         [-0.4498,  0.0348,  0.4290,  ...,  0.5824,  0.0629, -0.1115],\n",
      "         [-0.1446, -0.0419,  0.5309,  ...,  0.6713, -0.0634,  0.2591]],\n",
      "\n",
      "        [[-0.1149, -0.0587,  0.4086,  ...,  0.7718, -0.0916,  0.2096],\n",
      "         [-0.2173, -0.0343,  0.4252,  ...,  0.4182, -0.2104,  0.0830],\n",
      "         [-0.3156, -0.0735,  0.4977,  ..., -0.0719, -0.1388,  0.2285],\n",
      "         ...,\n",
      "         [-0.3442, -0.0465,  0.5415,  ...,  0.3554,  0.0775,  0.4019],\n",
      "         [-0.3447, -0.0336,  0.4949,  ...,  0.2016,  0.0944,  0.3838],\n",
      "         [-0.0821,  0.0377,  0.5335,  ..., -0.0543,  0.0359,  0.1856]],\n",
      "\n",
      "        [[-0.2585, -0.0818,  0.4911,  ...,  0.4992,  0.0595,  0.1262],\n",
      "         [-0.0063,  0.0325,  0.4839,  ...,  0.0935,  0.1109,  0.2595],\n",
      "         [-0.2239, -0.0384,  0.5544,  ...,  0.7579, -0.0738,  0.1136],\n",
      "         ...,\n",
      "         [-0.1470, -0.0495,  0.4419,  ...,  1.0308,  0.0779,  0.1124],\n",
      "         [-0.2343,  0.0162,  0.4199,  ...,  0.5423, -0.1349,  0.1255],\n",
      "         [-0.2442,  0.0177,  0.3522,  ...,  0.5165, -0.0109, -0.0852]]]), tensor([[[ 6.7645e-04, -3.3540e-01,  5.6363e-01,  ...,  4.5578e-01,\n",
      "           3.2929e-01, -5.7725e-01],\n",
      "         [ 3.7545e-02, -4.7331e-01,  5.7534e-01,  ...,  5.3402e-02,\n",
      "           3.6354e-01, -3.7674e-01],\n",
      "         [ 5.2452e-02, -3.6531e-01,  5.9790e-01,  ...,  3.6646e-01,\n",
      "           3.2378e-01, -4.2081e-01],\n",
      "         ...,\n",
      "         [ 7.9032e-02, -3.8343e-01,  6.1830e-01,  ...,  3.4670e-01,\n",
      "           3.7949e-01, -5.2737e-01],\n",
      "         [ 2.0494e-01, -3.9395e-01,  5.7528e-01,  ...,  1.6519e-01,\n",
      "           3.3090e-01, -5.2240e-01],\n",
      "         [ 1.6175e-01, -3.0023e-01,  7.3612e-01,  ...,  2.8633e-01,\n",
      "           3.2639e-01, -4.8198e-01]],\n",
      "\n",
      "        [[ 1.1004e-01, -2.5976e-01,  5.8541e-01,  ...,  2.2522e-01,\n",
      "           2.8089e-01, -4.5776e-01],\n",
      "         [ 9.8611e-02, -3.7397e-01,  1.3657e-01,  ...,  8.2264e-02,\n",
      "           4.2774e-01, -3.3455e-01],\n",
      "         [ 7.6987e-02, -3.9069e-01,  2.4916e-01,  ...,  1.0845e-01,\n",
      "           4.1700e-01, -3.0493e-01],\n",
      "         ...,\n",
      "         [ 9.6137e-02, -3.8884e-01,  1.5387e-01,  ...,  3.1031e-01,\n",
      "           3.2738e-01, -4.6243e-01],\n",
      "         [ 1.1692e-01, -3.6908e-01,  5.8226e-01,  ...,  2.6958e-01,\n",
      "           4.2057e-01, -6.8774e-01],\n",
      "         [ 2.2142e-01, -4.9795e-01,  5.4039e-01,  ...,  5.8898e-02,\n",
      "           3.7916e-01, -5.3899e-01]],\n",
      "\n",
      "        [[-3.0844e-02, -3.9689e-01,  5.6877e-01,  ...,  3.0332e-01,\n",
      "           3.5392e-01, -5.5262e-01],\n",
      "         [ 6.8341e-02, -4.0164e-01,  4.3760e-01,  ...,  1.7420e-01,\n",
      "           3.4622e-01, -2.6677e-01],\n",
      "         [ 1.6856e-01, -4.6879e-01,  5.3412e-01,  ...,  4.8641e-02,\n",
      "           4.4109e-01, -4.3997e-01],\n",
      "         ...,\n",
      "         [ 2.6536e-01, -2.6210e-01,  4.8522e-01,  ...,  2.1339e-01,\n",
      "           2.3391e-01, -3.3602e-01],\n",
      "         [ 2.1866e-02, -3.2937e-01,  6.6756e-01,  ...,  1.6354e-01,\n",
      "           3.2141e-01, -3.8622e-01],\n",
      "         [-1.7547e-02, -4.1615e-01,  4.0595e-01,  ...,  3.6169e-01,\n",
      "           1.7316e-01, -4.7562e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.4810e-01, -1.4595e-01,  7.6189e-01,  ...,  2.0408e-01,\n",
      "           3.6106e-01, -5.7101e-01],\n",
      "         [ 1.5660e-01, -3.3288e-01,  2.0338e-01,  ...,  2.2276e-01,\n",
      "           3.3743e-01, -5.3970e-01],\n",
      "         [ 3.1666e-01, -3.2953e-01,  7.1221e-01,  ...,  2.4971e-01,\n",
      "           5.7104e-01, -2.8621e-01],\n",
      "         ...,\n",
      "         [-3.2217e-02, -1.9284e-01,  6.6273e-01,  ...,  2.8166e-01,\n",
      "           1.3898e-01, -4.2918e-01],\n",
      "         [-9.9313e-02, -4.0209e-01,  6.5812e-01,  ...,  2.9461e-01,\n",
      "           4.1170e-01, -5.0086e-01],\n",
      "         [ 2.6032e-01, -4.1226e-01,  7.7739e-01,  ...,  3.9881e-01,\n",
      "           4.2493e-01, -3.1808e-01]],\n",
      "\n",
      "        [[ 5.1374e-02, -3.8465e-01,  5.6217e-01,  ...,  4.2719e-01,\n",
      "           3.3103e-01, -4.5804e-01],\n",
      "         [ 1.2376e-01, -3.5190e-01,  5.4413e-01,  ...,  3.4416e-01,\n",
      "           2.4928e-01, -3.2901e-01],\n",
      "         [ 1.3270e-01, -4.3133e-01,  6.2764e-01,  ...,  1.1662e-01,\n",
      "           3.2189e-01, -3.8362e-01],\n",
      "         ...,\n",
      "         [ 6.2181e-02, -4.3538e-01,  4.6834e-01,  ...,  7.0113e-02,\n",
      "           2.8255e-01, -2.7836e-01],\n",
      "         [ 4.4186e-02, -3.1861e-01,  6.0215e-01,  ...,  1.6198e-01,\n",
      "           3.9772e-01, -2.9143e-01],\n",
      "         [ 1.1378e-01, -4.2661e-01,  1.7280e-01,  ..., -1.6209e-02,\n",
      "           4.2973e-01, -3.7907e-01]],\n",
      "\n",
      "        [[-5.7628e-02, -2.1905e-01,  6.4472e-01,  ...,  2.9702e-01,\n",
      "           4.1774e-01, -4.2196e-01],\n",
      "         [ 8.3517e-02, -4.5720e-01,  5.5301e-01,  ..., -2.6334e-02,\n",
      "           4.4814e-01, -3.4193e-01],\n",
      "         [-4.8525e-02, -3.2782e-01,  8.1297e-01,  ...,  4.2120e-01,\n",
      "           4.0913e-01, -3.9038e-01],\n",
      "         ...,\n",
      "         [ 1.5526e-01, -4.0256e-01,  6.1540e-01,  ...,  3.8053e-01,\n",
      "           4.4018e-01, -3.7695e-01],\n",
      "         [ 1.2608e-01, -3.3920e-01,  1.2463e-01,  ...,  3.7013e-01,\n",
      "           3.0703e-01, -4.0446e-01],\n",
      "         [ 1.2378e-01, -3.8992e-01,  4.5465e-01,  ...,  2.4825e-01,\n",
      "           3.5429e-01, -4.5873e-01]]]))}\n"
     ]
    }
   ],
   "source": [
    "# model.transformer.model.add_adapter(\"acu\", \"pfeiffer\")  # Before activated, resul is the same as original mockingjay in original s3prl\n",
    "wavs = [torch.ones(160000, dtype=torch.float) for _ in range(16)]\n",
    "model.train()  # will have random effect\n",
    "# model.eval()\n",
    "with torch.no_grad():\n",
    "    print(model(wavs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'adapters': {}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n",
      "None\n",
      "{'adapters': {}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n",
      "None\n",
      "{'adapters': {}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n"
     ]
    }
   ],
   "source": [
    "from s3prl.adapters.layer import AdapterLayer, AdapterLayerBase\n",
    "from s3prl.adapters.configuration import AdapterConfig\n",
    "for (i, layer) in model.transformer.model.iter_layers():\n",
    "    for module in layer.modules():\n",
    "        if (isinstance(module, AdapterLayerBase)):\n",
    "            print(module.config.adapters.match(\n",
    "                \"acu\",\n",
    "                config_type=AdapterConfig,\n",
    "                layer_idx=i,\n",
    "                location_key=\"self\",\n",
    "            ))\n",
    "            print(module.config.adapters.to_dict())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerConfig {\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hahaha\": \"hahaha\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"transformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pre_layer_norm\": false,\n",
       "  \"share_layer\": false,\n",
       "  \"transformers_version\": \"4.17.0\"\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.config.hahaha = \"hahaha\"\n",
    "model.transformer.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerConfig {\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hahaha\": \"hahaha\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"transformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pre_layer_norm\": false,\n",
       "  \"share_layer\": false,\n",
       "  \"transformers_version\": \"4.17.0\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.encoder.layer[0].output.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerConfig {\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"transformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pre_layer_norm\": false,\n",
       "  \"share_layer\": false,\n",
       "  \"transformers_version\": \"4.17.0\"\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.model.base_model.encoder.layer[0].output.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 19:53:17 | INFO | s3prl.adapters.configuration | Adding adapter 'acu'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adapters': {'acu': 'pfeiffer'}, 'config_map': {}, 'fusions': {}, 'fusion_config_map': {}}\n",
      "UpstreamExpert(\n",
      "  (transformer): PretrainedTransformer(\n",
      "    (extracter): OnlinePreprocessor(\n",
      "      (_melscale): MelScale()\n",
      "      (_mfcc_trans): MFCC(\n",
      "        (amplitude_to_DB): AmplitudeToDB()\n",
      "        (MelSpectrogram): MelSpectrogram(\n",
      "          (spectrogram): Spectrogram()\n",
      "          (mel_scale): MelScale()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (model): TransformerModel(\n",
      "      (shared_parameters): ModuleDict()\n",
      "      (invertible_adapters): ModuleDict()\n",
      "      (input_representations): TransformerInputRepresentations(\n",
      "        (spec_transform): Linear(in_features=80, out_features=768, bias=True)\n",
      "        (LayerNorm): TransformerLayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (1): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (2): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (3): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (4): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (5): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (6): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (7): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (8): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (9): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (10): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (11): TransformerLayer(\n",
      "            (attention): TransformerAttention(\n",
      "              (self): TransformerSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): TransformerSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (LayerNorm): TransformerLayerNorm()\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): TransformerIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): TransformerOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (LayerNorm): TransformerLayerNorm()\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (prefix_tuning): PrefixTuningPool(\n",
      "        (prefix_tunings): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.transformer.model.add_adapter(\"acu\")\n",
    "print(model.transformer.model.config.adapters.to_dict())\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59c55fd4b3978c0def058b870785763f0b3768b417004cdd166463757ca86d4c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Multimodal_Schizo_adapter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
